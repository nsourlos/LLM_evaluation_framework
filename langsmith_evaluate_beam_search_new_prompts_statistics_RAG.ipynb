{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5Kh-zLB9lk"
      },
      "source": [
        "# Evaluate LLM results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no2PHIOWCBdA"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U2Dpuc2xtmmS"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install datasets==2.20.0\n",
        "# !pip install -U langsmith==0.1.99\n",
        "# !pip install langchain_openai==0.1.22\n",
        "# !pip install langchain==0.2.13\n",
        "# !pip install langchain_community==0.2.12                          \n",
        "# !pip install transformers==4.44.0\n",
        "# !pip install termcolor==2.4.0\n",
        "# !pip install accelerate==0.33.0\n",
        "# !pip install pandas==2.2.2\n",
        "# !pip install openpyxl==3.1.5\n",
        "# !pip install python-dotenv==1.0.1\n",
        "# !pip install einops==0.8.0\n",
        "# !pip install wheel==0.44.0\n",
        "# !pip install sentencepiece==0.2.0\n",
        "# !pip install protobuf==5.27.3 #Mistral models needs this\n",
        "# !pip install groq==0.10.0 #Groq models needs this\n",
        "# !pip install matplotlib==3.9.2\n",
        "# !pip install seaborn==0.13.2\n",
        "\n",
        "# !pip install flash-attn==2.6.3 #Install it at the end after wheel has been installed\n",
        "# !pip install anthropic==0.34.1 #Anthropic models needs this\n",
        "\n",
        "# #Only if CPU is used\n",
        "# !pip install torch==2.0.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !jupyter lab --ServerApp.iopub_data_rate_limit=1e10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RunPod specific parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For RunPod change to persistent storage directory\n",
        "import os\n",
        "os.chdir('/workspace')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify Path and Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path ='/workspace/few_questions_only.xlsx' #Dataset generated with the help of GPT-4o - Has to be an excel file with 'input' and 'output' columns\n",
        "#'/Users/nikolaossourlo/Desktop/Example_QA_data_raw.xlsx' #For MacOS\n",
        "#'C:/Users/soyrl/Desktop/Example_QA_data_raw.xlsx' #For Windows\n",
        "#'/content/drive/My Drive/Example_QA_data_raw.xlsx' #For Google Colab\n",
        "#'/home/nikolaossourlo/Example_QA_data_raw.xlsx' #For Delft Blue\n",
        "#'/workspace/Example_QA_data_raw.xlsx' #For RunPod\n",
        "\n",
        "custom_cache_dir=\"/workspace/cache/huggingface\" #Save models here so that we don't have to download them again\n",
        "#\"/scratch/nikolaossourlo/cache\" in Delft Blue\n",
        "\n",
        "# Check if custom_cache_dir is defined, otherwise use default behavior\n",
        "try:\n",
        "    cache_dir=custom_cache_dir\n",
        "except:\n",
        "    cache_dir=None\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import traceback\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
        "\n",
        "# Get the OpenAI API key\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY_DRACO')\n",
        "\n",
        "#Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# Log in with your Hugging Face token\n",
        "login(token=os.getenv('HF_TOKEN'))\n",
        "\n",
        "# print(openai_api_key)\n",
        "# print(langsmith_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select model and name for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model to generate responses to questions - Sometimes we might have to restart session and comment out the models that have already been run\n",
        "models=[ \n",
        "    # \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    # \"meta-llama/Meta-Llama-3.1-8B-Instruct\", #A4500 (20GB VRAM) and in Delft Blue (V100 32GB)\n",
        "    # \"microsoft/Phi-3.5-mini-instruct\", #A40 with 48GB VRAM, A4500 with 20GB VRAM, Delft Blue \n",
        "    # \"mistralai/Mistral-7B-Instruct-v0.3\", #A40 with 48GB VRAM, A4500 with 20GB VRAM and in Delft Blue\n",
        "    # \"Qwen/Qwen2-7B-Instruct\", #A40 with 48GB VRAM, A4500 with 20GB VRAM, Delft Blue\n",
        "    # 'AI-MO/NuminaMath-7B-TIR', #A4500 with 20GB VRAM and in Delft Blue - We can also try 01-ai/Yi-Coder-9B-Chat\n",
        "    # 'microsoft/Phi-3-mini-4k-instruct', #RTX3090\n",
        "    # 'microsoft/phi-4', #14B parameters\n",
        "    # \"google/gemma-2-9b-it\", #More than 20GB of GPU memory needed - Works with A40 with 48GB VRAM, but not with A4500 - 20GB, and V100 - 32GB, Delft Blue\n",
        "    # 'mistralai/Mistral-Nemo-Instruct-2407', #12B parameters, 2 RTX3090, V100 with 32GB VRAM\n",
        "    # \"anthropic/claude-3-5-sonnet-20241022\",\n",
        "    'openai/gpt-4o-mini' #Costs very low ~0.01$ for 9 Q&A pairs.\n",
        "    ] #Takes 7+hours in A40 for the above 13 models with 7Q&A paris and 4 resamples. Cost ±3$ (±180GB)\n",
        "\n",
        "# Groq models are defined as: groq_website/model_name e.g. 'groq_website/llama-3.1-70b-versatile'\n",
        "# OpenAI models are defined as: 'openai/model_name', e.g. 'openai/gpt-4o-mini'\n",
        "# Anthropic models are defined as 'anthropic/model_name', e.g. 'anthropic/claude-3-haiku-20240307' - Couldn't use due to billing issues\n",
        "\n",
        "# I couldn't run 'nvidia/Mistral-NeMo-Minitron-8B-Base', \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" (Conflicting dependencies),\n",
        "# 'google/recurrentgemma-9b-it' # RecurrentGemmaForCausalLM.forward() got an unexpected keyword argument 'position_ids'\n",
        "#Large models take more time (2min/generation for Mistral 12B)\n",
        "\n",
        "#Define model to act as a judge\n",
        "judge_model='openai/gpt-4o-mini' #If used with Llama, only 0.01$ for 9 Q&A pairs for gpt-4o-mini, and 0.22$ for gpt-4o\n",
        "\n",
        "#Define maximum number of tokes in the judge LLM output\n",
        "max_output_tokens=500\n",
        "\n",
        "#Limit of tokens in the generated response from LLM\n",
        "generate_max_tokens=1000\n",
        "\n",
        "#Inference on whole dataset?\n",
        "inference_on_whole_dataset=True\n",
        "\n",
        "#Number of times to resample the dataset\n",
        "n_resamples=4 #4 reduces the variance to 50%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define prompts for custom evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_prompt=\"\"\"\n",
        "You are an autoregressive language model that acts as a judge in comparing a predicted vs an actual answer to a questions.\n",
        "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend \n",
        "a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. \n",
        "Your users are experts in chemical engineering, so they already know you're a language model and your capabilities and limitations, so don't \n",
        "remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. \n",
        "Don't be verbose in your answers, but do provide details and examples where it might help the explanation. \n",
        "\"\"\" #This is common for all prompts below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to completeness compared to the completeness of a given actual, golden standard answer. \n",
        "The completeness metric evaluates the extent to which the user's question is answered in full in the predicted response. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: There is no response.\n",
        "2: No parts of a suitable answer are present.\n",
        "3: Few elements of a complete answer are present.\n",
        "4: Most elements of a complete answer are present.\n",
        "5: The response covers all elements of a complete answer.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "relevance_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to relevance compared to the relevance of a given actual, golden standard answer. \n",
        "The relevance metric evaluates the amount of irrelevant information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response answers something else, not the user's question.\n",
        "2: The response answers the user's question but the information provided is mostly irrelevant.\n",
        "3: The response answers the user's question but contains more irrelevant information than relevant information.\n",
        "4: The response answers the user's question, and shares a bit of irrelevant information.\n",
        "5: The response answers the user's question and contains no irrelevant information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "conciseness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to conciseness compared to the conciseness of a given actual, golden standard answer. \n",
        "The conciseness metric evaluates the amount of unexpected extra information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response is too long and stops before completion or enters an infinite loop.\n",
        "2: The response includes a lot of extra information and uses flowery language.\n",
        "3: The response includes a lot of extra information or uses flowery language.\n",
        "4: The response is short and includes a small amount of extra information.\n",
        "4: The response is as short as possible while still answering the prompt.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "confidence_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to confidence compared to the confidence of a given actual, golden standard answer. \n",
        "The condifence metric evaluates the degree of assurance that is conveyed the response that the predicted answer is correct. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: Complete Rejection. The response makes it clear that the given answer is incorrect or that no correct answer can be provided.\n",
        "2: Doubt and Disagreement. The response suggests that the answer is likely incorrect or raises significant concerns.\n",
        "3: Uncertainty. The response indicates that the answer could be correct, but there is significant doubt or insufficient evidence.\n",
        "4: Moderate Agreement. The response leans towards the answer being correct but acknowledges some uncertainty.\n",
        "5: Full Endorsement. The reponse confidentely asserts that the given answer is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "factuality_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to factuality compared to the factuality of a given actual, golden standard answer.\n",
        " The factuality metric evaluates the degree of hallucination contained in a response or, in other words, how accurate a given response is.\n",
        "You can assign a score from 1 to 5, with the following interpretations:\n",
        "1: The response is a complete hallucination\n",
        "2: The response is mostly a hallucination but does not change key information from the prompt (such as chemical identifiers).\n",
        "3: The response contains large amounts of both hallucinations and factual information.\n",
        "4: The response includes mostly factual information with slight hallucinations.\n",
        "5: The response only includes factual information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "judgement_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to judgement compared to the judgement of a given actual, golden standard answer.\n",
        "The judgment metric assesses how strongly the response implies its correctness, taking into account the actual accuracy of the answer.\n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response confidently claims a hallucination as truth.\n",
        "2: The response misinterprets information received in the prompt.\n",
        "3: The response shows that the model is unsure about the answer or states that information is theoretical.\n",
        "4: The response is wrong but it is made clear that the answer is wrong or that the model is unable to provide a correct answer.\n",
        "5: The response is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#How the dataset will be named in Langsmith\n",
        "def get_dataset_name(model_name, judge_model):\n",
        "    try: #For Hugging Face models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name.split('/')[1]+'_with_judge_'+judge_model+'_beam_search_statistics_RAG'\n",
        "    except: #For OpenAI models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name+'_with_judge_'+judge_model+'_beam_search_statistics_RAG'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX9V2ASWCQG5"
      },
      "source": [
        "Google Drive mount (If run in Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLsdaFvRthOE",
        "outputId": "ee976853-2292-4eee-a380-812283627e56"
      },
      "outputs": [],
      "source": [
        "if 'content/drive/My Drive' in file_path:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lVqBHaT2s6Aq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "qa=pd.read_excel(file_path) #Read Excel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6zdJxKCubI"
      },
      "source": [
        "Create Dataset from df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oUw8Puxfs6Az"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "loaded_dataset=Dataset.from_pandas(qa)\n",
        "\n",
        "if inference_on_whole_dataset==False:\n",
        "    loaded_dataset = loaded_dataset.train_test_split(test_size=0.2, seed=42) #Used if going to fine-tune in part of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vf6thikds6A1"
      },
      "outputs": [],
      "source": [
        "if inference_on_whole_dataset==False:\n",
        "    dataset_train=loaded_dataset['train']\n",
        "    dataset_test=loaded_dataset['test']\n",
        "else:\n",
        "    dataset_test=loaded_dataset #When we use the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXxkzQoHs6A5"
      },
      "source": [
        "Create Langsmith Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtdIrA3Ds6A8",
        "outputId": "90a9b4dd-e91a-4773-934b-2bf58cd8e3a8"
      },
      "outputs": [],
      "source": [
        "#https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets\n",
        "\n",
        "from langsmith import Client\n",
        "\n",
        "example_inputs = [(x['input'],x['output']) for x in dataset_test]\n",
        "print(example_inputs)\n",
        "\n",
        "def create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key):\n",
        "\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    try:\n",
        "        #Load the dataset if already exists\n",
        "        for existing_dataset in client.list_datasets():\n",
        "            if existing_dataset.name==dataset_name:\n",
        "                dataset_langsmith=existing_dataset\n",
        "        for x in dataset_langsmith:\n",
        "            print(\"Dataset Loaded\")\n",
        "            break\n",
        "\n",
        "    except: #Otherwise create it\n",
        "        print(\"Dataset not found. Creating new dataset\")\n",
        "        # Storing inputs in a dataset lets us run chains and LLMs over a shared set of examples.\n",
        "        dataset_langsmith = client.create_dataset(dataset_name=dataset_name,\n",
        "                                                description=\"Q&A chemical engineering.\")\n",
        "\n",
        "        for input_prompt, output_answer in example_inputs:\n",
        "            client.create_example(\n",
        "                inputs={\"question\": input_prompt.replace('\\n', ' ')},\n",
        "                outputs={\"answer\": output_answer.replace('\\n', ' ')},\n",
        "                # metadata={\"source\": \"Wikipedia\"},\n",
        "                dataset_id=dataset_langsmith.id,\n",
        "            )\n",
        "\n",
        "    return dataset_langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/cookbook/introduction\n",
        "# https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators\n",
        "# https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator\n",
        "\n",
        "from langsmith.schemas import Run, Example\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from termcolor import colored\n",
        "\n",
        "list_of_metrics=['completeness_descr','relevance_descr','conciseness_descr','confidence_descr','factuality_descr','judgement_descr']\n",
        "\n",
        "#Function that compares the real answer with the predicted answer of an LLM and returns a score based on the evaluation\n",
        "def factor_evaluator(run: Run, example: Example) -> dict: \n",
        "    # print(\"Run:\",run)\n",
        "\n",
        "    question=run.inputs.get(\"inputs\")['question']\n",
        "    # print(\"Question:\",question)\n",
        "    actual_answer = example.outputs.get(\"answer\")\n",
        "    # print(\"Real answer:\",example.outputs.get(\"answer\"))\n",
        "    predicted_answer = run.outputs.get(\"output\")\n",
        "    # print(\"Predicted Answer:\",answer)\n",
        "    \n",
        "    # Check if there is output from LLM\n",
        "    if not predicted_answer:\n",
        "        print(\"No output from LLM\")\n",
        "        return {\"key\": \"custom_metric\" , \"score\": 0} \n",
        "    \n",
        "    else:\n",
        "        scores={} #Store scores for each metric\n",
        "        descriptions={} #Store descriptions for each metric\n",
        "        \n",
        "        for metric_name in list_of_metrics: #Iterate through all metrics\n",
        "            print(\"Evaluating based on:\",metric_name)\n",
        "            metric_value=common_prompt+eval(metric_name) #Get the actual description of the metric\n",
        "\n",
        "            # Define roles and placeholders\n",
        "            chat_template = ChatPromptTemplate.from_messages(\n",
        "            [(\"system\", metric_value),\n",
        "                (\"user\", \"Question: {question}, Actual answer: {actual_answer}, Predicted answer: {predicted_answer}\"),\n",
        "                # (\"ai\", \"It's sunny and warm outside.\"), #Use this if we want to use few shot prompts\n",
        "            ]\n",
        "            )\n",
        "\n",
        "            messages = chat_template.format_messages(question=question, actual_answer=actual_answer, predicted_answer=predicted_answer)\n",
        "            # print(\"Messages:\",messages)\n",
        "\n",
        "            formatted_messages = [(role, msg.content) for role, msg in zip([\"system\", \"user\"], messages)]\n",
        "            # print(\"Formatted messages:\",formatted_messages) #[('system', 'You are an autoregressive lan....', 'user':.....)]\n",
        "\n",
        "            # Initialize the model and get response\n",
        "            llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0, max_tokens=max_output_tokens, seed=42)\n",
        "            ai_response = llm.invoke(formatted_messages)\n",
        "\n",
        "            # Output\n",
        "            # print(colored(\"System message:\"+ messages[0].content,'blue'))\n",
        "            print(colored(\"User message:\"+ messages[1].content, 'green'))\n",
        "            print(colored(\"AI message:\"+ ai_response.content,'red'))\n",
        "\n",
        "            #Decide what the final score is based on output\n",
        "            if \"FINAL SCORE:\" in ai_response.content: \n",
        "                score = int(ai_response.content.split(\"FINAL SCORE:\")[1])\n",
        "            else:\n",
        "                print(\"Invalid response from LLM:\", ai_response.content)\n",
        "                score = 0 #For cases where the LLM doesn't return a score - Otherwise we are gonna get an error\n",
        "\n",
        "            scores[metric_name]=score\n",
        "            descriptions[metric_name]=ai_response.content\n",
        "            print(\"Scores:\",scores)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"results\":[ #We always need 'key', 'score' pairs\n",
        "            {\"key\": \"completeness\" , \"score\": scores['completeness_descr'],\"value\":descriptions['completeness_descr']},\n",
        "            {\"key\": \"relevance\" , \"score\": scores['relevance_descr'], \"value\":descriptions['relevance_descr']},\n",
        "            {\"key\": \"conciseness\" , \"score\": scores['conciseness_descr'], \"value\":descriptions['conciseness_descr']},\n",
        "            {\"key\": \"confidence\" , \"score\": scores['confidence_descr'], \"value\":descriptions['confidence_descr']},\n",
        "            {\"key\": \"factuality\" , \"score\": scores['factuality_descr'], \"value\":descriptions['factuality_descr']},\n",
        "            {\"key\": \"judgement\" , \"score\": scores['judgement_descr'], \"value\":descriptions['judgement_descr']}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Models that Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.random.manual_seed(0) #Set for reproducibility\n",
        "\n",
        "def initialize_model(model_id):\n",
        "    # # Check if mps acceleration is available (For MacOS)\n",
        "    # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    # print(f\"Using device {device}\")\n",
        "    # model.to(device)\n",
        "\n",
        "    # transformers.set_seed(42) #Tried for reproducibility but didn't work\n",
        "    \n",
        "    pipeline = transformers.pipeline( \n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\":cache_dir},\n",
        "            # trust_remote_code=True,\n",
        "            device_map=\"auto\" #Use 'cuda' if one GPU available (works in Delft Blue with 32GB VRAM) - 'auto' the alternative for distributed over all available GPUs\n",
        "        )\n",
        "    return pipeline\n",
        "\n",
        "def get_model(model_id):\n",
        "    \"\"\"Given a model name, return the loaded model, tokenizer, and pipeline\"\"\"\n",
        "\n",
        "    if 'openai' not in model_id and 'groq_website' not in model_id and 'anthropic' not in model_id: #For Hugging Face models\n",
        "        pipeline=initialize_model(model_id)\n",
        "\n",
        "    #Returns below variables if defined, and returns None for any that are not.\n",
        "    model = locals().get('model', None)\n",
        "    tokenizer = locals().get('tokenizer', None)\n",
        "    pipeline = locals().get('pipeline', None)\n",
        "\n",
        "    return model, tokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RAG and embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS #Better than cosine similarity since it's scales better\n",
        "from langchain.schema import Document\n",
        "\n",
        "def initialize_vectorstore(list_of_questions, list_of_answers):\n",
        "    \"\"\"Initialize the embedding model and FAISS vectorstore for the dataset.\"\"\"\n",
        "    # Initialize embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        cache_folder=cache_dir,\n",
        "        model_kwargs={\"device\": \"cuda\"}\n",
        "    )\n",
        "    \n",
        "    # Create documents from Q&A pairs\n",
        "    documents = [\n",
        "        Document(\n",
        "            page_content=question,\n",
        "            metadata={\"answer\": answer}\n",
        "        ) for question, answer in zip(list_of_questions, list_of_answers)\n",
        "    ]\n",
        "    \n",
        "    # Create and save FAISS index\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    \n",
        "    # Optionally save the index for later use\n",
        "    # vectorstore.save_local(\"faiss_index\")\n",
        "    \n",
        "    return vectorstore\n",
        "\n",
        "def get_similar_qa_pairs(query, vectorstore, top_k=5):\n",
        "    \"\"\"Get the most similar Q&A pairs using FAISS.\"\"\"\n",
        "    # Search for similar documents\n",
        "    similar_docs = vectorstore.similarity_search_with_score(query, k=top_k)\n",
        "    \n",
        "    # Format results\n",
        "    similar_pairs = []\n",
        "    for doc, score in similar_docs:\n",
        "        similar_pairs.append({\n",
        "            'question': doc.page_content,\n",
        "            'answer': doc.metadata['answer'],\n",
        "            'similarity': 1 - score  # Convert distance to similarity score\n",
        "        })\n",
        "    \n",
        "    return similar_pairs\n",
        "\n",
        "def check_context_relevance(query, similar_pairs, judge_model, openai_api_key):\n",
        "    \"\"\"Check if the retrieved context is relevant enough to use for RAG.\"\"\"\n",
        "\n",
        "    # query=\"What is the weather in Rotterdam now?\" #With this query the context is not relevant\n",
        "\n",
        "    # Construct prompt for the judge\n",
        "    prompt = f\"\"\"Given a user question and retrieved similar Q&A pairs, determine if the context is relevant enough to be used for answering the question.\n",
        "    Consider:\n",
        "    1. Semantic similarity between the question and retrieved pairs\n",
        "    2. Whether the context provides useful information for answering\n",
        "    3. If using no context might be better than using potentially misleading context\n",
        "\n",
        "    User Question: {query}\n",
        "\n",
        "    Retrieved Q&A Pairs:\n",
        "    {similar_pairs}\n",
        "\n",
        "    Should this context be used for answering the question? Respond with only 'Yes' or 'No'.\n",
        "    \"\"\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that determines context relevance.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    # Use OpenAI to judge relevance\n",
        "    import openai\n",
        "    from langsmith.wrappers import wrap_openai\n",
        "    client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        model=judge_model.split('/')[1],\n",
        "        seed=42\n",
        "    )\n",
        " \n",
        "    print(\"Use context/RAG:\",response.choices[0].message.content.strip().lower()) #Rotterdam query above returns 'no'\n",
        "    \n",
        "    return response.choices[0].message.content.strip().lower() == 'yes'\n",
        "\n",
        "def format_context(similar_pairs):\n",
        "    \"\"\"Format the similar Q&A pairs into a context string.\"\"\"\n",
        "    context = \"Here are some relevant previous Q&A pairs:\\n\\n\"\n",
        "    for pair in similar_pairs:\n",
        "        context += f\"Question: {pair['question']}\\n\"\n",
        "        context += f\"Answer: {pair['answer']}\\n\\n\"\n",
        "    return context.strip()\n",
        "\n",
        "\n",
        "# # Get unique questions/answers\n",
        "# client = Client(api_key=langsmith_api_key)\n",
        "# questions_answers=[x for x in client.list_examples(dataset_id=dataset_langsmith.id)]\n",
        "# list_of_questions=[x.inputs['question'] for x in questions_answers]\n",
        "# list_of_answers=[x.outputs['answer'] for x in questions_answers]\n",
        "\n",
        "# # Initialize vectorstore once\n",
        "# vectorstore = initialize_vectorstore(list_of_questions, list_of_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def predict(inputs: dict) -> dict:\n",
        "    \"\"\"Given a question, check if context should be used and return the answer from the model\"\"\"\n",
        "    \n",
        "    #Get these variables from the global scope\n",
        "    global model_name, vectorstore\n",
        "    \n",
        "    question=inputs['question']\n",
        "\n",
        "    # Get similar Q&A pairs\n",
        "    similar_pairs = get_similar_qa_pairs(\n",
        "        question,\n",
        "        vectorstore\n",
        "    )\n",
        "    \n",
        "    # Check if context should be used\n",
        "    use_context = check_context_relevance(question, similar_pairs, judge_model, openai_api_key)\n",
        "    \n",
        "    # Prepare messages\n",
        "    if use_context:\n",
        "        context = format_context(similar_pairs)\n",
        "        user_content = f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
        "    else:\n",
        "        user_content = question\n",
        "  \n",
        "    messages = [ #Only use the questions to ask the model to generate the response\n",
        "      {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "\n",
        "\n",
        "    if 'gemma' not in model_name and 'anthropic' not in model_name: #Gemma doesn't support system message\n",
        "      messages.insert(0, {\"role\": \"system\", \"content\": \"You are a language model specialized in chemical engineering. Answer the following question:\"})\n",
        "    else: #For gemma add system prompt in user message\n",
        "      messages[0]['content']=\"You are a language model specialized in chemical engineering. Answer the following question: \" + messages[0]['content']\n",
        "    # print(\"Prompt:\",messages)\n",
        "\n",
        "    generation_args = { \n",
        "        \"max_new_tokens\": max_output_tokens, \n",
        "        \"return_full_text\": False, \n",
        "        \"temperature\": 0.05, #Has to be positive number - not considered from model when do_sample is False (reproducible results)\n",
        "        \"do_sample\": True, #Selects highest probability token if sets to False\n",
        "        \"num_beams\" : 5, #3 can also work if computationally intensive - more info on https://huggingface.co/blog/how-to-generate\n",
        "        #Warnings will be raised by some models\n",
        "\n",
        "        #If we only set temp!=0 or if we also set do_sample=False then warning: `do_sample` is set to `False`. However, `temperature` is set to `1e-08` \n",
        "        # -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
        "        # That means that the temperature is probably ignored\n",
        "        # Sometimes, results not reproducible if only temp is set\n",
        "        # A temparature of 0.01 or lower results in: \"Error running target function: probability tensor contains either `inf`, `nan` or element < 0\"\n",
        "      } \n",
        "    \n",
        "    if 'openai' not in model_name and 'groq_website' not in model_name and 'anthropic' not in model_name: #For Hugging Face models\n",
        "      response=pipeline(messages, **generation_args)[0]['generated_text']\n",
        "      print(model_name,':',response)\n",
        "\n",
        "    else: \n",
        "      if 'openai' in model_name:\n",
        "        try:\n",
        "          import openai\n",
        "          from langsmith.wrappers import wrap_openai\n",
        "                  \n",
        "          # Define OpenAI client\n",
        "          openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "          \n",
        "          response = openai_client.chat.completions.create(messages=messages, temperature=0, model=model_name.split('/')[1],  seed=42) \n",
        "          # print(\"Response:\",response.choices[0].message.content)\n",
        "          response=response.choices[0].message.content #That's the response without formatting\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"OpenAI Model ID:\",model_name)\n",
        "\n",
        "      elif 'groq_website' in model_name:\n",
        "        try:\n",
        "          from groq import Groq\n",
        "          client = Groq()\n",
        "          actual_model_name=model_name.split('/')[1]\n",
        "          response = client.chat.completions.create(\n",
        "              model=actual_model_name,\n",
        "              max_tokens=generate_max_tokens,\n",
        "              temperature=0,\n",
        "              messages=messages)\n",
        "          # print(\"Response from Groq:\",response.choices[0].message.content)\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"Groq Model ID:\",model_name)\n",
        "\n",
        "      elif 'anthropic' in model_name:\n",
        "        try:\n",
        "          import anthropic\n",
        "          client = anthropic.Anthropic()\n",
        "          response = client.messages.create(\n",
        "              model=model_name.split('/')[1],\n",
        "              messages=messages,\n",
        "              temperature=0,\n",
        "              max_tokens=max_output_tokens,\n",
        "          )\n",
        "          print(\"Response from Anthropic:\",response.content[0].text)\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"Anthropic Model ID:\",model_name)\n",
        "\n",
        "    return {\"output\": response}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_statistics(values):\n",
        "    \"\"\"Calculate mean, standard error, and confidence intervals.\"\"\"\n",
        "    mean_value = np.mean(values)  # Mean of the metric over single run and over single metric (but over all questions)\n",
        "    std_error = np.std(values, ddof=1) / np.sqrt(len(values))  # ddof=1 to divide by n-1 to calculate the sample sd\n",
        "    \n",
        "    assert np.std(values, ddof=1) == np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), \"Standard deviation calculation mismatch\"\n",
        "    \n",
        "    margin_of_error = 1.96 * std_error  # didn't use t_critical=t.ppf(0.975, df=len(values)-1) since we're using sample standard deviation\n",
        "\n",
        "    return {\n",
        "        'mean': mean_value,\n",
        "        'std_error': std_error,\n",
        "        'ci_low': mean_value - margin_of_error,\n",
        "        'ci_high': mean_value + margin_of_error\n",
        "    }\n",
        "\n",
        "def plot_metric_distributions(metric_values, axes, colors, bin_edges, metric_names):\n",
        "    \"\"\"Plot individual metric distributions with error bars.\"\"\"\n",
        "    error_bars = []\n",
        "    run_stats = {}\n",
        "    \n",
        "    for metric_idx, (metric_name, values) in enumerate(metric_values.items()):  # Loop over runs' metric names and values\n",
        "        clean_metric_name = metric_name.replace('_descr', '')  # This is over one run and over one metric (but over all questions)\n",
        "        metric_name = metric_names[metric_idx]\n",
        "        assert clean_metric_name == metric_name, \"Metric name mismatch\"\n",
        "        \n",
        "        stats = calculate_statistics(values)\n",
        "        sns.histplot(values, bins=bin_edges, color=colors[metric_idx], ax=axes[metric_idx], kde=False)\n",
        "        \n",
        "        #Store error bars\n",
        "        if metric_idx == 0:\n",
        "            error_bars = []\n",
        "        error_bars.append((stats['mean'], axes[metric_idx].get_ylim()[1]/2, stats['ci_high'] - stats['mean']))\n",
        "        \n",
        "        run_stats[metric_name] = stats\n",
        "\n",
        "        axes[metric_idx].set_title(f'{metric_name} (Mean: {stats[\"mean\"]:.2f} ± {stats[\"std_error\"]:.2f} SE, CI: {stats[\"ci_low\"]:.2f}-{stats[\"ci_high\"]:.2f})')\n",
        "        axes[metric_idx].set_xlim(0, 5.5)  # Keep 0 in case of errors\n",
        "        axes[metric_idx].set_ylabel('Frequency')\n",
        "        axes[metric_idx].set_xlabel('Values' if metric_idx == len(metric_values)-1 else '')\n",
        "        \n",
        "    return error_bars, run_stats\n",
        "\n",
        "def plot_question_scores(metric_names, grouped_values, colors):\n",
        "    \"\"\"Plot scores for each question across metrics.\"\"\"\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Define colors for each metric\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "    # First count all frequencies per score (1-5) per metric for one run over all questions\n",
        "    question_scores_by_metric = {metric: [] for metric in metric_names}\n",
        "    score_metric_counts = {}\n",
        "\n",
        "    #Plot each metric's values and store question scores\n",
        "    for i, (metric, question_scores) in enumerate(zip(metric_names, grouped_values)):\n",
        "        width = 0.8 / len(question_scores)  # Width of each metric's bar\n",
        "        \n",
        "        for j, val in enumerate(question_scores): #Create a bar for each question's score\n",
        "            plt.bar(i + j * width, val, width=width, color=colors[i], alpha=0.5, \n",
        "                    label=metric if j == 0 else \"\")\n",
        "                    # i is the index of metric and determines the base position of a group of bars corresponding to that metric.\n",
        "                    # j*width adds an offset to the base position to separate individual bars within the same group (metric). \n",
        "                    # Each j corresponds to a different value in question_scores, creating distinct bars for the values of question_scores for the same metric.\n",
        "                    # By combining the above two, we get the exact x-position of a specific bar     \n",
        "            question_scores_by_metric[metric].append((j, val))\n",
        "\n",
        "        counts = Counter(question_scores)  # Count frequency of each score in question_scores (e.g. {4: 1, 3: 2, 2: 2, 1: 1, 0: 1}, where key is score)\n",
        "        for score, freq in counts.items():\n",
        "            if score not in score_metric_counts:\n",
        "                score_metric_counts[score] = {}\n",
        "            score_metric_counts[score][metric] = freq  #Keeps track of how many times each metric gets a specific score over all questions (for one run)\n",
        "            # {4: {'completeness': 1, 'confidence': 1, 'factuality': 1, 'judgement': 1}, 3: {'completeness': 1, 'relevance': 2, 'conciseness': 2, ....}\n",
        "\n",
        "    return question_scores_by_metric, score_metric_counts\n",
        "\n",
        "def plot_ordered_scores(metric_names, question_scores_by_metric, colors):\n",
        "    \"\"\"Plot metrics ordered by score values.\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    for i, metric in enumerate(metric_names):\n",
        "        plt.subplot(len(metric_names), 1, i+1)\n",
        "        sorted_questions = sorted(question_scores_by_metric[metric], key=lambda x: x[1]) #Sort questions by score\n",
        "        \n",
        "        #Plot bars\n",
        "        x_pos = range(len(sorted_questions))\n",
        "        scores = [q[1] for q in sorted_questions] #q[1] is the score, q[0] is the index\n",
        "        plt.bar(x_pos, scores, color=colors[i], alpha=0.5)\n",
        "\n",
        "        #Add question indices as x-axis labels\n",
        "        plt.xticks(x_pos, [str(q[0]) for q in sorted_questions])\n",
        "        \n",
        "        plt.ylabel(metric)\n",
        "        plt.ylim(0, 5.5)\n",
        "        plt.yticks(range(6))  # Set y-axis ticks from 0 to 5\n",
        "\n",
        "        if i == len(metric_names)-1:\n",
        "            plt.xlabel('Question number (ordered by score)')\n",
        "\n",
        "def plot_accumulated_distributions(score_metric_counts, metric_names, colors):\n",
        "    \"\"\"Plot accumulated distribution of scores by metric.\"\"\"\n",
        "    legend_added = set()\n",
        "\n",
        "    #For each score, plot metrics in order of frequency (highest frequency at bottom)\n",
        "    for score in sorted(score_metric_counts.keys()):\n",
        "        #Sort metrics by frequency for this score\n",
        "        sorted_metrics = sorted(score_metric_counts[score].items(),\n",
        "                            key=lambda x: x[1], #Use the frequency (second element of each tuple) as the sorting key\n",
        "                            reverse=True)  # highest frequency first\n",
        "        bottom = 0\n",
        "        for metric, freq in sorted_metrics:\n",
        "            i = metric_names.index(metric) #get index for color\n",
        "            plt.bar(score, freq,\n",
        "                    width=0.4,\n",
        "                    color=colors[i],\n",
        "                    alpha=0.5,\n",
        "                    label=metric if metric not in legend_added else \"\",\n",
        "                    bottom=bottom)\n",
        "            bottom += freq\n",
        "            legend_added.add(metric)\n",
        "\n",
        "           \n",
        "def plot_figures_metrics(all_runs_model_metrics, metric_names, model_name, judge_model):\n",
        "    \"\"\"\n",
        "    Creates visualizations and calculates statistics for evaluation metrics across multiple runs.\n",
        "\n",
        "    Args:\n",
        "        all_runs_model_metrics (dict): Nested dictionary containing evaluation metrics for each model and run.\n",
        "            Structure: {model_id: [{metric1_descr_run1: [q1_score, q2_score, ...], \n",
        "                                  metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "                                 {metric1_descr_run2: [q1_score, q2_score, ...],\n",
        "                                  metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "                                 ...num_runs]}\n",
        "            Example: {'model1': [{'completeness_descr_run1': [4.5, 3.0, 4.0], \n",
        "                                'relevance_descr_run1': [3.5, 4.0, 3.0]}, ...,\n",
        "                               {'completeness_descr_run2': [4.0, 3.5, 4.5],\n",
        "                                'relevance_descr_run2': [3.0, 4.5, 3.5], ...},\n",
        "                               ...num_runs]}\n",
        "            Where each inner dictionary represents one run containing scores for each metric across all questions\n",
        "        metric_names (list): Names of metrics to analyze and plot (e.g. ['completeness', 'relevance'])\n",
        "        model_name (str): Name/identifier of the model being evaluated\n",
        "        judge_model (str): Name/identifier of the model used for judging the evaluations\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary statistics for each model, run and metric.\n",
        "            Structure: {model_name: {run_idx: {metric_name: {\n",
        "                'mean': float,\n",
        "                'std_error': float, \n",
        "                'ci_low': float,\n",
        "                'ci_high': float\n",
        "            }}}}\n",
        "            Example: {'anthropic/claude-3-5-sonnet': {\n",
        "                '0': {'completeness': {'mean': 4.5, 'std_error': 0.5, \n",
        "                                     'ci_low': 3.52, 'ci_high': 5.48},\n",
        "                      'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "                                  'ci_low': 2.52, 'ci_high': 4.48} , ...},\n",
        "                '1': {'completeness': {'mean': 4.5, 'std_error': 0.5,\n",
        "                                     'ci_low': 3.52, 'ci_high': 5.48},\n",
        "                      'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "                                  'ci_low': 2.52, 'ci_high': 4.48}, ...},\n",
        "                ...num_runs}}\n",
        "\n",
        "    The function generates several visualization types:\n",
        "    - Individual histograms for each metric showing score distributions\n",
        "    - Error bars indicating means and confidence intervals\n",
        "    - Overlapping bar plots comparing metrics\n",
        "    - Stacked distribution plots showing relative frequencies of scores\n",
        "\n",
        "    All plots are saved as PNG files with names indicating the judge model,\n",
        "    evaluated model, run index, and plot type.\n",
        "    \"\"\"\n",
        "\n",
        "    summary_stats_all_runs = {}  # Keep track of summary statistics over all runs\n",
        "\n",
        "    for run_idx, metric_values_run in enumerate(all_runs_model_metrics[model_name]): #Loop over runs\n",
        "\n",
        "        colors = sns.color_palette(\"Set3\", len(metric_names))\n",
        "        \n",
        "        # Create two figures - one with separate subplots and one overlaid\n",
        "        fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 18))\n",
        "        plt.subplots_adjust(hspace=0.6, top=0.94)\n",
        "        fig.suptitle(f'Metric Distributions for {model_name} (Run {run_idx})', fontsize=16)\n",
        "        \n",
        "        bin_edges = np.arange(0.0, 5.6, 0.2)  # Bins for range 0-5\n",
        "        metric_names = [name.replace('_descr', '') for name in metric_values_run]\n",
        "        \n",
        "        error_bars, run_stats = plot_metric_distributions(metric_values_run, axes, colors, bin_edges, metric_names)\n",
        "        \n",
        "        # Save version without error bars\n",
        "        plt.figure(fig.number)\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions_no_error_bars.png\")\n",
        "        \n",
        "        # Add error bars and save updated version\n",
        "        for i, (mean, ylim, margin) in enumerate(error_bars):\n",
        "            axes[i].errorbar(mean, ylim, xerr=margin, color='black', capsize=5, \n",
        "                           capthick=1, elinewidth=2, marker='o')\n",
        "        \n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Print summary statistics - Can also be seen in txt file. \n",
        "        print(f\"\\nSummary Statistics over run {run_idx}:\")\n",
        "        print(\"-\" * 50)\n",
        "        for metric, stats in run_stats.items():\n",
        "            print(f\"{metric}:\")\n",
        "            for key, value in stats.items():\n",
        "                print(f\"  {key}: {value:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        summary_stats_all_runs[run_idx] = run_stats #For one run\n",
        "\n",
        "        grouped_values=list(metric_values_run.values()) #Values of all metrics for one run over all questions. There are num_metrics lists in that list. \n",
        "        values = [val for sublist in grouped_values for val in sublist] #Flatten the list - Size is num_questions*num_metrics (1st metric questions, 2nd metric questions, etc)\n",
        "        \n",
        "        question_scores_by_metric, score_metric_counts = plot_question_scores(metric_names, grouped_values, colors)\n",
        "        plt.xlabel('Metrics')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Per-Metric Question Scores Distribution')\n",
        "        plt.xticks(np.arange(len(metric_names)) + 0.1, metric_names)\n",
        "        plt.yticks(range(6)) #Set y-ticks to 0-5\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_per_metric_question_scores.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Plot ordered scores\n",
        "        plot_ordered_scores(metric_names, question_scores_by_metric, colors)\n",
        "        plt.suptitle('Question indices ordered by metric value')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_question_indices_ordered_by_metric_value.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Plot accumulated distributions\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plot_accumulated_distributions(score_metric_counts, metric_names, colors)\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Score Distribution Histogram by Metric') \n",
        "        plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.xticks(np.arange(0, 6))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_score_distribution_histogram_by_metric.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "    return summary_stats_all_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform the Evaluation over all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "def load_model_stats(judge_model): #In case we had to restart the loop - some models didn't run - Keep track of all model stats\n",
        "    \"\"\"Load existing stats from files or initialize empty dictionaries.\"\"\"\n",
        "    try:\n",
        "        with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "            all_models_stats = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_models_stats = {}  # Used in comparison between models\n",
        "\n",
        "    try:\n",
        "        with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "            all_runs_model_metrics = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_runs_model_metrics = {}  # Used in plotting metrics\n",
        "        \n",
        "    return all_models_stats, all_runs_model_metrics\n",
        "\n",
        "def perform_evaluation(model_id, judge_model, n_resamples, example_inputs, factor_evaluator, langsmith_api_key):\n",
        "    \"\"\"Perform evaluation runs and collect results.\"\"\"\n",
        "    global vectorstore\n",
        "    dataset_name = get_dataset_name(model_id, judge_model) #How the dataset will be named in Langsmith\n",
        "    dataset_langsmith = create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key)\n",
        "    results_df, list_of_questions, vectorstore = process_evaluation_results(langsmith_api_key, dataset_langsmith)\n",
        "    print(\"Vectorstore:\",vectorstore)\n",
        "    print(\"List of questions:\",list_of_questions)\n",
        "\n",
        "    evaluation_all_resamples = [] #Used below to obtain the unique questions/answers and also the results of each resample\n",
        "    \n",
        "    begin = time.time()\n",
        "    for resample_idx in range(n_resamples):\n",
        "        print(f\"\\nPerforming evaluation of resample {resample_idx+1}/{n_resamples} of {model_id}\")\n",
        "\n",
        "        evaluation_results = evaluate(\n",
        "            predict, #Function that call our LLM and returns its output\n",
        "            data=dataset_langsmith.name, #Just using dataset_langsmith doesn't work \n",
        "            evaluators=[factor_evaluator], #Evaluators to use\n",
        "            max_concurrency=1, #Run one question through langsmith each time - Other values will give errors in resulting excels\n",
        "            # metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
        "            experiment_prefix=str(judge_model)+'_judge_with_'+str(model_id)+'_resample_'+str(resample_idx) # A prefix for your experiment names to easily identify them\n",
        "        )\n",
        "        evaluation_all_resamples.extend(evaluation_results) #Used below to get unique questions/answers and to select the predicted answers\n",
        "        #This has n_resamples*num_questions elements, for just one model\n",
        "\n",
        "    with open('evaluation_all_resamples_'+str(model_id.split('/')[1])+'_'+str(judge_model.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(evaluation_all_resamples))\n",
        "\n",
        "    assert len(evaluation_all_resamples)==n_resamples*len(example_inputs), f\"Number of evaluation results not matching num_resamples*num_questions. \\\n",
        "        Got {len(evaluation_all_resamples)} evaluation results but expected {n_resamples*len(example_inputs)}\"\n",
        "    \n",
        "    print(f\"Total time for evaluation: {time.time() - begin}\")\n",
        "\n",
        "    return evaluation_all_resamples, dataset_langsmith, results_df, list_of_questions, vectorstore\n",
        "\n",
        "def process_evaluation_results(langsmith_api_key, dataset_langsmith):\n",
        "    \"\"\"Extract questions and answers from evaluation results.\"\"\"\n",
        "    #https://docs.smith.langchain.com/tutorials/Developers/evaluation\n",
        "\n",
        "    # Get unique questions/answers\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "    questions_answers=[x for x in client.list_examples(dataset_id=dataset_langsmith.id)]\n",
        "    list_of_questions=[x.inputs['question'] for x in questions_answers]\n",
        "    list_of_answers=[x.outputs['answer'] for x in questions_answers]\n",
        "        \n",
        "    # with open('list_of_questions.txt', 'w') as f:\n",
        "    #     f.write(str(list_of_questions))\n",
        "        \n",
        "    # with open('list_of_answers.txt', 'w') as f:\n",
        "    #     f.write(str(list_of_answers))\n",
        "    \n",
        "    # Initialize vectorstore\n",
        "    vectorstore = initialize_vectorstore(list_of_questions, list_of_answers)\n",
        "        \n",
        "    results_df = pd.DataFrame({\n",
        "        'questions': list_of_questions,\n",
        "        'answers': list_of_answers\n",
        "    })\n",
        "    return results_df, list_of_questions, vectorstore\n",
        "\n",
        "def process_metrics(resample_results, list_of_metrics, list_of_questions, resample_idx, results_df, model_name):\n",
        "    \"\"\"\n",
        "    Process metrics for a single resample and update results DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        resample_results: Results from current resample\n",
        "        list_of_metrics: List of metrics to process\n",
        "        resample_idx: Current resample index\n",
        "        results_df: DataFrame to update with metrics\n",
        "        model_name: Name of the model being evaluated\n",
        "        \n",
        "    Returns:\n",
        "        individual_run_metric_scores, metrics, results_df\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = [] #This should be the same as resample_results (list) except when there are 'traceback' errors where it will be 0.\n",
        "    # metrics format will be:[[EvaluationResult(key='completeness', score=4, value='To evaluate the .... - It has num_questions sublists, each with num_metrics values\n",
        "\n",
        "    for result in resample_results:\n",
        "        if result['run'].outputs['output'] is None or not result['evaluation_results']['results']: #or result['run'].error is not None - Same as first condition\n",
        "            metrics.append(0)  # Use 0 to indicate failed evaluation - We might even get in here when LangSmith API connection issues\n",
        "            print(\"Error: No metric value found!\")\n",
        "            #Also print which condition is true\n",
        "            print(\"result['run'].outputs['output'] is None\",result['run'].outputs['output'] is None)\n",
        "            print(\"not result['evaluation_results']['results']\",not result['evaluation_results']['results'])\n",
        "            # print(\"result['run'].error is not None\",result['run'].error is not None)\n",
        "        else:\n",
        "            metrics.append(result['evaluation_results']['results'])\n",
        "\n",
        "    with open('resample_results_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(resample_results))\n",
        "\n",
        "    assert len(resample_results)==len(list_of_questions), f\"Number of resample results not matching num_questions. Got {len(resample_results)} resample \\\n",
        "        results but expected {len(list_of_questions)}\"\n",
        "    \n",
        "    with open('metrics_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(metrics))\n",
        "\n",
        "    assert len(metrics)==len(list_of_questions), f\"Number of metrics not matching num_questions. Got {len(metrics)} metrics but expected {len(list_of_questions)}\"\n",
        "    \n",
        "    #This is at the end a dict with num_metrics keys and each key has num_questions values.\n",
        "    #Example: {'completeness_descr': [4, 3, 3, 5, 5, 4, 3], 'relevance_descr': [4, 3, 3, 3, 4, 3, 1], ....} assuming 7 questions\n",
        "    individual_run_metric_scores = {} #Keep track of scores of all metrics over all questions for one resample\n",
        "\n",
        "    for metric_idx, metric_name in enumerate(list_of_metrics): #Get specific metric name and values over all questions for the current resample\n",
        "\n",
        "        clean_metric_names, metric_scores, metric_prompts = [], [], [] #Metric scores and prompts for all questions for a given resample - Should be num_questions elements each time\n",
        "        \n",
        "        #Get all metric keys for the current resample over all questions, handling potential missing keys (values set to 0 for those - they are errors)\n",
        "        for m in metrics:\n",
        "            if m == 0: #If there is an error\n",
        "                key = metric_name.replace('_descr','')\n",
        "                score = 0\n",
        "                prompt=\"\"\n",
        "            else:\n",
        "                try:\n",
        "                    key = m[metric_idx].key #Metric name\n",
        "                    score = m[metric_idx].score ##Scores of a given metric over all questions for a given resample\n",
        "                    prompt = m[metric_idx].value #Prompt used for the evaluation\n",
        "                except:\n",
        "                    print(\"Error: Metric not found - Shouldn't get here\")\n",
        "                    key = metric_name.replace('_descr','')\n",
        "                    score = 0\n",
        "                    prompt = \"\"\n",
        "                \n",
        "            clean_metric_names.append(key)\n",
        "            metric_scores.append(score)\n",
        "            metric_prompts.append(prompt)\n",
        "            \n",
        "        assert all(name == metric_name.replace('_descr','') for name in clean_metric_names), \"Metric keys not matching\"\n",
        "            \n",
        "        # with open('metric_scores_'+str(resample_idx)+'_'+str(metric_name)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        #     f.write(str(metric_scores))\n",
        "\n",
        "        assert len(metric_scores)==len(list_of_questions), f\"Number of metric scores not matching num_questions. Got {len(metric_scores)} metric scores \\\n",
        "            but expected {len(list_of_questions)}\"\n",
        "            \n",
        "        # with open('metric_prompts_'+str(resample_idx)+'_'+str(metric_name)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        #     f.write(str(metric_prompts))\n",
        "\n",
        "        assert len(metric_prompts)==len(list_of_questions), f\"Number of metric prompts not matching num_questions. Got {len(metric_prompts)} metric prompts \\\n",
        "            but expected {len(list_of_questions)}\"\n",
        "\n",
        "        # Update results DataFrame\n",
        "        clean_metric_name = clean_metric_names[0] #Just one metric name without the _descr\n",
        "        results_df[f'metric_{clean_metric_name}_{resample_idx+1}'] = metric_scores\n",
        "        results_df[f'prompt_{clean_metric_name}_{resample_idx+1}'] = metric_prompts\n",
        "        \n",
        "        # Store scores for return\n",
        "        individual_run_metric_scores[metric_name] = metric_scores #len is num_metrics\n",
        "    \n",
        "    \n",
        "    # with open('individual_run_metric_scores_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "    #     f.write(str(individual_run_metric_scores))\n",
        "\n",
        "    return individual_run_metric_scores, metrics, results_df\n",
        "\n",
        "def calculate_metric_statistics(all_runs_metric_scores, list_of_metrics, num_questions, model_name):\n",
        "    \"\"\"Calculate statistical metrics across resamples (reduce variance - step 3.1).\"\"\"\n",
        "    metric_stats_resampling = {} # Calculate mean and standard error for each metric and question across K resamples\n",
        "    #The above dict will have num_metrics elements, each with metric keys (e.g. mean, std, etc), that will have num_questions values\n",
        "    #Example: {'completeness': {'mean':[4, 3, 3, 5, 5, 4, 3]}, #here for a dataset with 7 questions\n",
        "    #          'relevance': {'mean':[4, 3, 3, 3, 4, 3, 2]}, ...}\n",
        "    \n",
        "    for metric in list_of_metrics:\n",
        "        metric_stats_resampling[metric] = {\n",
        "            'means': [],  # Mean score across K resamples for each question\n",
        "            'standard_errors': [],  # Standard error of the mean for each question\n",
        "            'conditional_vars': []  # Conditional variance reduced by factor of K\n",
        "        }\n",
        "        \n",
        "        # For each question\n",
        "        for q in range(num_questions):\n",
        "            # Get K scores for this metric/question across all resamples (num_resamples elements each time in that list)\n",
        "            scores = [run[metric][q] for run in all_runs_metric_scores]\n",
        "            K = len(scores)  # Number of resamples\n",
        "            assert len(scores)==n_resamples, f\"Number of scores not matching num_resamples. Got {len(scores)} scores but expected {n_resamples}\"\n",
        "            \n",
        "            # Calculate statistics\n",
        "            mean = np.mean(scores) #Average score of each question for a given metric over all resamples\n",
        "            var = np.var(scores) #Variance of the scores of each question for a given metric over all resamples\n",
        "            # Calculate conditional variance reduced by factor of K. Var(mean) = σ²/K where σ² is the variance of individual scores\n",
        "            conditional_var = var / K if K > 0 else 0\n",
        "            standard_error = np.sqrt(conditional_var)\n",
        "            \n",
        "            # Store results\n",
        "            metric_stats_resampling[metric]['means'].append(mean)\n",
        "            metric_stats_resampling[metric]['standard_errors'].append(standard_error)\n",
        "            metric_stats_resampling[metric]['conditional_vars'].append(conditional_var)\n",
        "    \n",
        "    with open('metric_stats_resampling_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(metric_stats_resampling))\n",
        "\n",
        "    assert len(metric_stats_resampling)==len(list_of_metrics), f\"Number of metric_stats_resampling not matching num_metrics. \\\n",
        "        Got {len(metric_stats_resampling)} metric_stats_resampling but expected {len(list_of_metrics)}\"\n",
        "    \n",
        "    for metric in list_of_metrics:\n",
        "        assert len(metric_stats_resampling[metric]['means']) == num_questions, f\"Number of values for metric '{metric}' ({len(metric_stats_resampling[metric]['means'])}) \\\n",
        "            not matching expected number of questions ({num_questions})\"\n",
        "\n",
        "    return metric_stats_resampling\n",
        "\n",
        "def handle_zero_values(results_df, n_resamples, list_of_metrics):\n",
        "    \"\"\"\n",
        "    Handle zero values in results.\n",
        "    \n",
        "    Args:\n",
        "        results_df (pd.DataFrame): DataFrame containing results\n",
        "        n_resamples (int): Number of resamples\n",
        "        list_of_metrics (list): List of metrics to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Indices of rows containing zero values for each metric\n",
        "    \"\"\"\n",
        "    zero_rows_columns = {}\n",
        "    \n",
        "    try:\n",
        "        # Handle 0 values across all resamples - These are errors\n",
        "        for resample_idx in range(n_resamples):\n",
        "            for metric in list_of_metrics:\n",
        "                try:\n",
        "                    simple_metric_name = metric.replace('_descr','')\n",
        "                    metric_col = f'metric_{simple_metric_name}_{resample_idx+1}'\n",
        "                    \n",
        "                    # Check if column exists\n",
        "                    if metric_col not in results_df.columns:\n",
        "                        print(colored(f\"Warning: Column {metric_col} not found in DataFrame\", 'yellow'))\n",
        "                        continue\n",
        "                    \n",
        "                    zero_indices = results_df[metric_col] == 0 #series with True/False\n",
        "                    \n",
        "                    if zero_indices.any(): #If any of the values of that column are 0\n",
        "                        zero_rows_columns[metric_col] = []\n",
        "                        for idx in zero_indices[zero_indices].index: #Loop over True indices (rows with 0s)\n",
        "                            try:\n",
        "                                print(colored(f\"Missing value for metric '{simple_metric_name}' in resample {resample_idx+1}\", 'red'))\n",
        "                                print(colored(f\"Question: {results_df.loc[idx, 'questions']}\", 'green'))\n",
        "                                zero_rows_columns[metric_col].append(idx) #Keep track of columns and rows with zero values\n",
        "                            except Exception as e:\n",
        "                                print(colored(f\"Unexpected error processing zero value at row {idx}: {e}\", 'red'))\n",
        "                \n",
        "                except Exception as e:\n",
        "                    print(colored(f\"Error processing metric {metric} in resample {resample_idx}: {e}\", 'red'))\n",
        "        \n",
        "        return zero_rows_columns # Return column names and rows with zero values\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(colored(f\"Critical error in handle_zero_values: {e}\", 'red'))\n",
        "        traceback.print_exc()\n",
        "        return {}  # Return empty dict in case of critical error\n",
        "\n",
        "def process_zero_values(results_df, zero_rows_columns, list_of_metrics, model_name): #TO BE ACTIVATED\n",
        "    \"\"\"Process and optionally replace zero values in results.\"\"\"\n",
        "    for column_name, row_indices in zero_rows_columns.items():\n",
        "        for row_idx in row_indices:\n",
        "                \n",
        "            # Get values for this metric for this row and column (one resample per time)\n",
        "            values = results_df.loc[row_idx, column_name]\n",
        "\n",
        "            assert values==0, \"Values should be 0\"\n",
        "            \n",
        "            if values != 0: #We should never get here\n",
        "                with open('values_'+str(model_name.split('/')[1])+'_'+str(column_name)+'_'+str(row_idx)+'.txt', 'w') as f:\n",
        "                    f.write(str(values))\n",
        "                \n",
        "            #Given that values are 0, replace with mean of non-zero values\n",
        "            df_values=results_df.loc[:, column_name].values\n",
        "            non_zero_values = [x for x in df_values if x != 0]\n",
        "\n",
        "            if len(non_zero_values) > 0:\n",
        "                mean_value = np.mean(non_zero_values)\n",
        "\n",
        "                if results_df.loc[row_idx, column_name] == 0 and mean_value != 0:\n",
        "                    print(colored(f\"0 value in row {row_idx}, column {column_name} should be replaced with mean {mean_value:.2f}\", 'yellow'))\n",
        "                    # Uncomment to actually replace values:\n",
        "                    # results_df.at[row_idx, col] = round(mean_value, 1)\n",
        "\n",
        "def reorganize_evaluation_metrics(all_resamples_metrics, list_of_metrics, model_name, list_of_questions, n_resamples):\n",
        "    \"\"\"    \n",
        "    This function takes evaluation metrics from multiple resampling runs and reorganizes them into\n",
        "    a structured dictionary where each metric's scores are grouped together. It handles cases where\n",
        "    some evaluations may have failed (represented by 0s).\n",
        "    \n",
        "    Args:\n",
        "        all_resamples_metrics (list): List of evaluation results for each resample. Each resample contains\n",
        "                                     scores for multiple questions and metrics.\n",
        "        list_of_metrics (list): List of metric names to process (e.g., ['completeness_descr', 'relevance_descr']).\n",
        "        model_name (str): Name of the model being evaluated, used for logging.\n",
        "        list_of_questions (list): List of questions that were evaluated.\n",
        "        n_resamples (int): Number of resampling iterations performed.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary where keys are metric names (without '_descr' suffix) and values are lists\n",
        "              containing all scores for that metric across all resamples and questions.\n",
        "              \n",
        "    Note:\n",
        "        The function assumes each resample has scores for all questions and metrics.\n",
        "    \"\"\"\n",
        "    metric_scores_all_resamples = {metric.replace('_descr', ''): [] for metric in list_of_metrics}\n",
        "    #The above dict will have num_metrics elements, with their value for each question, over each run (first num_questions for first run, then next num_questions for next run, etc)\n",
        "    #Example: {'completeness': {'mean':[4, 3, 3, 5, 5, 4, 3, 4, 3, 3, 5, 5, 0, 3, 5, 3, 3, 5, 5, 4, 3]},  #assuming 3 runs and 7 questions\n",
        "    #          'relevance': {'mean':[4, 3, 3, 3, 4, 3, 2, 4, 3, 3, 3, 3, 0, 2, 4, 3, 3, 3, 3, 3, 2]}, ...}\n",
        "    #In case of error, there will be num_questions less elements in the sublist for which there was an error\n",
        "    \n",
        "    for metric_name in list_of_metrics:\n",
        "        clean_name = metric_name.replace('_descr', '')\n",
        "        \n",
        "        #Each resample_metrics (num_resamples in total) has a list of num_questions lists, each having num_metrics values\n",
        "        #format of each sublist: [EvaluationResult(key='completeness', score=4, value='To evaluate the ...\n",
        "        #If error, instead of the above list we have just a 0.\n",
        "        for resample_idx, resample_metrics in enumerate(all_resamples_metrics):\n",
        "\n",
        "            with open('resample_metrics_'+str(resample_idx)+'_'+str(metric_name)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "                f.write(str(resample_metrics))\n",
        "\n",
        "            metric_idx = list_of_metrics.index(metric_name) #0-num_metrics the range of values of this. \n",
        "\n",
        "            scores = [m[metric_idx].score if m!=0 and m!=[] else 0 \n",
        "                     for m in resample_metrics] #num_questions elements each time\n",
        "            assert len(scores)==len(list_of_questions), \"Scores length not matching num_questions\"\n",
        "\n",
        "            with open('scores_'+str(resample_idx)+'_'+str(metric_name)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "                f.write(str(scores))\n",
        "\n",
        "            metric_scores_all_resamples[clean_name].extend(scores) #Every time we add one metric for one resample (num_questions elements)\n",
        "\n",
        "            with open('metric_stats_reorganized_'+str(resample_idx)+'_'+str(metric_name)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "                f.write(str(metric_scores_all_resamples))\n",
        "\n",
        "    assert [len(x) for x in metric_scores_all_resamples.values()]==[len(list_of_questions)*n_resamples]*len(list_of_metrics), \"Metric stats length not matching\"\n",
        "\n",
        "    return metric_scores_all_resamples\n",
        "\n",
        "def save_results(results_df, judge_model, model_id, stage=\"before\"):\n",
        "    \"\"\"Save results DataFrame to Excel.\"\"\"\n",
        "    filename = (f\"results_{judge_model.split('/')[1]}_judge_with_\"\n",
        "               f\"{model_id.replace('/','_')}_{stage}_nan_replacement.xlsx\")\n",
        "    results_df.to_excel(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution loop (move code below inside this function)\n",
        "# def main():\n",
        "all_models_stats, all_runs_model_metrics = load_model_stats(judge_model) #Try to load already saved data (if some models have already been evaluated), otherwise initialize empty dicts\n",
        "\n",
        "for model_id in models:\n",
        "    global model_name, model, tokenizer, pipeline, vectorstore\n",
        "    model, tokenizer, pipeline = get_model(model_id)\n",
        "    model_name = model_id #Since model_name defined as global variable\n",
        "    \n",
        "    try: #Sometimes some errors with the evaluation\n",
        "        evaluation_all_resamples, dataset_langsmith, results_df, list_of_questions, vectorstore = perform_evaluation(model_id, judge_model, n_resamples, example_inputs, factor_evaluator, langsmith_api_key)\n",
        "        chunk_size = len(example_inputs) #Number of questions\n",
        "        # results_df, list_of_questions = process_evaluation_results(langsmith_api_key, dataset_langsmith)\n",
        "        \n",
        "        all_resamples_metrics = [] #Keep track of all metrics over all resamples and all questions\n",
        "        #There will be n_resamples lists, each with num_questions sublists (each having num_metrics sublists) (so num_questions*num_metrics elements in those in total)\n",
        "        #Each question will have 6 metric values like this: [EvaluationResult(key='completeness', score=4, value='To evaluate the ....\n",
        "        all_runs_metric_scores = [] #This will be appended to the input that plots metrics at the end. \n",
        "        #The format of it is [{metric1_descr_run1: [q1_score, q2_score, ...], metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "        #                     {metric1_descr_run2: [q1_score, q2_score, ...], metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "        #                     ...num_runs]\n",
        "        \n",
        "        # Process each resample\n",
        "        for resample_idx in range(n_resamples):\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            #Resample_results saved above in the process_metrics function\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            assert len(resample_results)==chunk_size, f\"Number of resample results not matching num_questions. Got {len(resample_results)} resample results but expected {chunk_size}\"\n",
        "\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results] #None if error\n",
        "            assert len(predicted_answers)==chunk_size, f\"Number of predicted answers not matching num_questions. Got {len(predicted_answers)} predicted answers but expected {chunk_size}\"\n",
        "\n",
        "            # with open('predicted_answers_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            #     f.write(str(predicted_answers))\n",
        "\n",
        "            #Add predicted answers to df\n",
        "            results_df[f'predicted_answer_{resample_idx+1}'] = predicted_answers\n",
        "\n",
        "            individual_run_metric_scores, metrics, results_df = process_metrics(\n",
        "                    resample_results, \n",
        "                    list_of_metrics, \n",
        "                    list_of_questions,\n",
        "                    resample_idx,\n",
        "                    results_df,\n",
        "                    model_name\n",
        "                )                \n",
        "            \n",
        "            #In each iteration we append the metrics (6 in total) of one resample for all questions - n at the end, one for each resample\n",
        "            #If there is an error, the metrics will be 0 (there will be n_errors*num_metrics less 'EvaluationResult' objects in that case)\n",
        "            all_resamples_metrics.append(metrics)\n",
        "\n",
        "            #Has n_resamples lists, each with num_metrics sublists (each sublist has scores over all questions of one metric) \n",
        "            all_runs_metric_scores.append(individual_run_metric_scores)\n",
        "        \n",
        "        assert len(all_runs_metric_scores)==n_resamples, f\"Number of all_runs_metric_scores not matching num_resamples. \\\n",
        "            Got {len(all_runs_metric_scores)} all_runs_metric_scores but expected {n_resamples}\"\n",
        "        \n",
        "        for i in range(n_resamples):\n",
        "            assert len(all_runs_metric_scores[i])==len(list_of_metrics), f\"Number of all_runs_metric_scores[{i}] not matching num_metrics. \\\n",
        "                Got {len(all_runs_metric_scores[i])} all_runs_metric_scores[{i}] but expected {len(list_of_metrics)}\"\n",
        "\n",
        "        with open('all_runs_metric_scores_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(all_runs_metric_scores))\n",
        "\n",
        "        with open('all_resamples_metrics_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(all_resamples_metrics))\n",
        "\n",
        "        assert len(all_resamples_metrics)==n_resamples, f\"Number of all_resamples_metrics not matching num_resamples. \\\n",
        "            Got {len(all_resamples_metrics)} all_resamples_metrics but expected {n_resamples}\"\n",
        "        \n",
        "        for i in range(n_resamples): #Each one will have num_questions elements, each with num_metrics sublists (or 0 if error)\n",
        "            assert len(all_resamples_metrics[i])==len(list_of_questions), f\"Number of all_resamples_metrics[{i}] not matching num_questions. \\\n",
        "                Got {len(all_resamples_metrics[i])} all_resamples_metrics[{i}] but expected {len(list_of_questions)}\" #Each all_ressamples_metrics[i] should have num_questions elements\n",
        "\n",
        "        # Calculate statistics\n",
        "        metric_stats_resampling = calculate_metric_statistics(\n",
        "            all_runs_metric_scores, \n",
        "            list_of_metrics, \n",
        "            len(list_of_questions),\n",
        "            model_name\n",
        "        )\n",
        "        \n",
        "        # Save initial results\n",
        "        save_results(results_df, judge_model, model_id, \"before\")\n",
        "        \n",
        "        # Handle zero values\n",
        "        zero_rows_columns = handle_zero_values(results_df, n_resamples, list_of_metrics)\n",
        "        if zero_rows_columns:\n",
        "            unique_zero_rows_columns = len(set([x for sublist in list(zero_rows_columns.values()) for x in sublist]))\n",
        "            print(colored(f\"ERROR: Found missing values in {unique_zero_rows_columns} rows out of {len(results_df)}\", 'red'))\n",
        "            process_zero_values(results_df, zero_rows_columns, list_of_metrics, model_name) #Replace 0s with mean of non-zero values\n",
        "        \n",
        "        # Reorganize metrics - Has num_metrics keys, each with num_questions*num_resamples values (as a list)\n",
        "        metric_scores_all_resamples = reorganize_evaluation_metrics(all_resamples_metrics, list_of_metrics, model_name, list_of_questions, n_resamples)\n",
        "\n",
        "        with open('metric_scores_all_resamples_final_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(metric_scores_all_resamples))\n",
        "\n",
        "        assert len(metric_scores_all_resamples)==len(list_of_metrics), f\"Number of metric_scores_all_resamples not matching num_metrics. \\\n",
        "            Got {len(metric_scores_all_resamples)} metric_scores_all_resamples but expected {len(list_of_metrics)}\"\n",
        "        \n",
        "        for i in range(len(list_of_metrics)):\n",
        "            name_of_metric=list_of_metrics[i].replace('_descr','')\n",
        "            assert len(metric_scores_all_resamples[name_of_metric])==len(list_of_questions)*n_resamples, f\"Number of metric_scores_all_resamples[{name_of_metric}] not matching \\\n",
        "                num_questions*num_resamples. Got {len(metric_scores_all_resamples[name_of_metric])} metric_scores_all_resamples[{name_of_metric}] but \\\n",
        "                expected {len(list_of_questions)*n_resamples}\"\n",
        "\n",
        "        metric_names = list(metric_scores_all_resamples.keys()) #Final list of metrics for plotting\n",
        "        \n",
        "        # Verify metric names\n",
        "        metrics_names_loop = [metric.replace('_descr','') for metric in list_of_metrics]\n",
        "        assert metrics_names_loop == metric_names, \"Metric names mismatch\"\n",
        "        \n",
        "        # Save results\n",
        "        all_runs_model_metrics[model_id] = all_runs_metric_scores #Used in plotting metrics\n",
        "        #Dictionary in format {model_id:[{metric_1_run_1:[values], metric_2_run_1:[values], ...}, {metric_1_run_2:[values]....}]\n",
        "\n",
        "        all_models_stats[model_id] = plot_figures_metrics(\n",
        "            all_runs_model_metrics,\n",
        "            metric_names,\n",
        "            model_id,\n",
        "            judge_model\n",
        "        ) #Stats like mean, std, etc. per metric and per run over all questions\n",
        "        \n",
        "        # Save to files\n",
        "        with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "            json.dump(all_models_stats, f, indent=4)\n",
        "        with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "            json.dump(all_runs_model_metrics, f, indent=4)\n",
        "\n",
        "        print(\"Model\",model_id,\"saved\")\n",
        "        print(\"Models saved so far:\",list(all_models_stats.keys()))\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in evaluating model\",model_id)\n",
        "        print(\"Error Details:\", e)\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    finally:\n",
        "        # Clear VRAM\n",
        "        del model, tokenizer, pipeline\n",
        "        torch.cuda.empty_cache()\n",
        "        print('-'*100)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistical comparison between models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_model_performances(all_models_stats, all_runs_model_metrics):\n",
        "    \"\"\"\n",
        "    Performs statistical comparison between models using paired differences, standard errors,\n",
        "    and Pearson correlation coefficients following section 4.2 methodology.\n",
        "    \n",
        "    Args:\n",
        "        all_models_stats (dict): Dictionary containing statistics for each model\n",
        "        all_runs_model_metrics (dict): Dictionary containing raw metrics for each model/run/question\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary containing pairwise comparison results\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy import stats\n",
        "    import itertools\n",
        "    \n",
        "    # Get all model pairs for comparison\n",
        "    models = list(all_runs_model_metrics.keys())\n",
        "    model_pairs = list(itertools.combinations(models, 2))\n",
        "    \n",
        "    # Store results\n",
        "    comparison_results = {}\n",
        "    \n",
        "    for model1, model2 in model_pairs:\n",
        "        comparison_key = f\"{model1.split('/')[-1]}_vs_{model2.split('/')[-1]}\"\n",
        "        comparison_results[comparison_key] = {}\n",
        "        \n",
        "        # Get metrics (removing '_descr' suffix)\n",
        "        metrics = [metric.replace('_descr', '') for metric in list(all_runs_model_metrics[model1][0].keys())]\n",
        "        \n",
        "        # Create file for this model comparison\n",
        "        variance_results_text = f\"\\n=== Variance Analysis Results for {comparison_key} ===\\n\"\n",
        "        \n",
        "        for metric in metrics:\n",
        "            # Calculate differences and correlations for each resample\n",
        "            resample_differences = []\n",
        "            resample_ses = []\n",
        "            correlations = []\n",
        "            model1_variances = []  # Initialize list\n",
        "            model2_variances = []  # Initialize list\n",
        "            \n",
        "            # Iterate through resamples - Same number for both models\n",
        "            for resample_idx in range(len(all_runs_model_metrics[model1])):\n",
        "                # Get scores for both models for this resample\n",
        "                scores1 = all_runs_model_metrics[model1][resample_idx][f'{metric}_descr']\n",
        "                scores2 = all_runs_model_metrics[model2][resample_idx][f'{metric}_descr']\n",
        "                \n",
        "                # Calculate differences for each question\n",
        "                question_differences = np.array(scores1) - np.array(scores2)\n",
        "                \n",
        "                # Calculate mean difference for this resample\n",
        "                mean_diff = np.mean(question_differences) #Same as the formula in the paper since mean(a-b)=mean(a)-mean(b)\n",
        "                \n",
        "                # Calculate standard error for this resample - Paired analysis (section 4.2)\n",
        "                n = len(question_differences)\n",
        "                se = np.sqrt(np.sum((question_differences - mean_diff)**2) / (n * (n-1))) if n > 1 else np.nan\n",
        "\n",
        "                # # Calculate standard errors for each model - Unpaired analysis (section 4.1)\n",
        "                # n = len(scores1)\n",
        "                # sea = np.sqrt(np.sum((scores1 - np.mean(scores1))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "                # seb = np.sqrt(np.sum((scores2 - np.mean(scores2))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "\n",
        "                # # Calculate the combined standard error as sqrt(sea^2 + seb^2)\n",
        "                # se = np.sqrt(sea**2 + seb**2)\n",
        "\n",
        "                # Calculate variances for each model\n",
        "                var1 = np.var(scores1, ddof=1)  # Using ddof=1 for sample variance\n",
        "                var2 = np.var(scores2, ddof=1)\n",
        "                model1_variances.append(var1)\n",
        "                model2_variances.append(var2)\n",
        "                \n",
        "                # Calculate Pearson correlation\n",
        "                correlation, _ = stats.pearsonr(scores1, scores2)\n",
        "                \n",
        "                resample_differences.append(mean_diff)\n",
        "                resample_ses.append(se)\n",
        "                correlations.append(correlation)\n",
        "            \n",
        "            # Convert to numpy arrays\n",
        "            resample_differences = np.array(resample_differences)\n",
        "            resample_ses = np.array(resample_ses)\n",
        "            correlations = np.array(correlations)\n",
        "            model1_variances = np.array(model1_variances)\n",
        "            model2_variances = np.array(model2_variances)\n",
        "            print(\"resample_differences\",resample_differences)\n",
        "            print(\"resample_ses\",resample_ses)\n",
        "            print(\"correlations\",correlations)\n",
        "            print(f\"Model 1 variances: {model1_variances}\")\n",
        "            print(f\"Model 2 variances: {model2_variances}\")\n",
        "          \n",
        "            # Calculate overall mean difference over all resamples\n",
        "            overall_mean_diff = np.mean(resample_differences)\n",
        "            print(\"overall_mean_diff\",overall_mean_diff)\n",
        "            \n",
        "            #We want an aggregated SE across all resamples for the same questions (same paired differences)\n",
        "            #This approach accounts for the fact that each resampling provides a different estimate of the variance of the same underlying distribution, \n",
        "            # and averaging these estimates gives a better representation of the overall uncertainty.\n",
        "\n",
        "            # Calculate pooled standard error across resamples\n",
        "            R = len(resample_differences)\n",
        "            pooled_se = np.sqrt(np.sum(resample_ses**2) / (R**2))\n",
        "            print(\"pooled_se\",pooled_se)\n",
        "            \n",
        "            # # If the resampling results are independent estimates of variance (i.e., combining uncertainty estimates from independent sources), the combined variance is\n",
        "            # # the sum of all individual variances, and the combined standard error is given below (goal to capture total variability)\n",
        "            # # Calculate the overall combined SE across all resamples\n",
        "            # combined_se = np.sqrt(np.nansum(np.array(resample_ses)**2))\n",
        "\n",
        "            # Calculate overall variance reduction across all resamples\n",
        "            n = len(scores1)\n",
        "            \n",
        "            # Calculate mean variances across resamples\n",
        "            mean_var1 = np.mean(model1_variances)  # Var(sA)\n",
        "            mean_var2 = np.mean(model2_variances)  # Var(sB)\n",
        "            \n",
        "            # Calculate mean correlation across resamples\n",
        "            mean_correlation = np.mean(correlations)\n",
        "            \n",
        "            # Calculate covariance between model scores\n",
        "            mean_cov = mean_correlation * np.sqrt(mean_var1 * mean_var2)  # Cov(sA, sB)\n",
        "            \n",
        "            # Calculate variance for unpaired case: Var(μA-B,unpaired) = (Var(sA) + Var(sB))/n\n",
        "            var_unpaired = (mean_var1 + mean_var2) / n\n",
        "            \n",
        "            # Calculate variance for paired case: Var(μA-B,paired) = (Var(sA) + Var(sB) - 2Cov(sA,sB))/n\n",
        "            var_paired = (mean_var1 + mean_var2 - 2 * mean_cov) / n\n",
        "            \n",
        "            # The reduction in variance is: Var(μA-B,unpaired) - Var(μA-B,paired) = 2Cov(xA,xB)/n\n",
        "            variance_reduction = 2 * mean_cov / n  # This should equal var_unpaired - var_paired\n",
        "            \n",
        "            # Calculate percentage reduction in variance\n",
        "            percent_reduction = (variance_reduction / var_unpaired) * 100 if var_unpaired != 0 else 0\n",
        "\n",
        "            # Add results for this metric to the text\n",
        "            variance_results_text += f\"\\nMetric: {metric}\\n\"\n",
        "            variance_results_text += f\"Mean Model 1 variance (Var(sA)): {mean_var1:.6f}\\n\"\n",
        "            variance_results_text += f\"Mean Model 2 variance (Var(sB)): {mean_var2:.6f}\\n\"\n",
        "            variance_results_text += f\"Mean covariance (Cov(sA,sB)): {mean_cov:.6f}\\n\"\n",
        "            variance_results_text += f\"Unpaired variance: {var_unpaired:.6f}\\n\"\n",
        "            variance_results_text += f\"Paired variance: {var_paired:.6f}\\n\"\n",
        "            variance_results_text += f\"Variance reduction (2Cov(xA,xB)/n): {variance_reduction:.6f}\\n\"\n",
        "            variance_results_text += f\"Percent reduction: {percent_reduction:.1f}%\\n\"\n",
        "\n",
        "            # # Calculate t-statistic and p-value\n",
        "            # t_stat = overall_mean_diff / pooled_se if pooled_se != 0 else np.nan\n",
        "            # df = R - 1  # degrees of freedom\n",
        "            # p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if not np.isnan(t_stat) else np.nan\n",
        "            \n",
        "            # # Calculate confidence interval\n",
        "            # t_crit = stats.t.ppf(0.975, df)  # 95% CI\n",
        "            # ci_margin = t_crit * pooled_se\n",
        "\n",
        "            # Calculate z-statistic and CI using standard normal distribution\n",
        "            z_stat = overall_mean_diff / pooled_se if pooled_se != 0 else np.nan\n",
        "            \n",
        "            # Calculate confidence interval using 1.96 for 95% CI\n",
        "            ci_margin = 1.96 * pooled_se\n",
        "            \n",
        "            # Calculate p-value using standard normal distribution\n",
        "            #For a two-tailed test p = 2 × (1 − Φ(|z|)), where Φ(z) is the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat))) if not np.isnan(z_stat) else np.nan\n",
        "            \n",
        "            # # Calculate average Pearson correlation - not accurate when correlations close to 1 or -1, variances differences across resamples, sample size is small.\n",
        "            # avg_correlation = np.mean(correlations)\n",
        "\n",
        "            #Apply Fisher z-transformation\n",
        "            z_values = [0.5 * np.log((1 + r) / (1 - r)) for r in correlations]\n",
        "\n",
        "            # Compute the mean Fisher z-value\n",
        "            z_mean = np.mean(z_values)\n",
        "\n",
        "            #Back-transform to Pearson correlation scale\n",
        "            overall_correlation = (np.exp(2 * z_mean) - 1) / (np.exp(2 * z_mean) + 1)\n",
        "            \n",
        "            # Store results\n",
        "            comparison_results[comparison_key][metric] = {\n",
        "                \"mean_difference\": overall_mean_diff,\n",
        "                \"pooled_standard_error\": pooled_se,\n",
        "                \"ci_low\": overall_mean_diff - ci_margin,\n",
        "                \"ci_high\": overall_mean_diff + ci_margin,\n",
        "                # \"t_statistic\": t_stat,\n",
        "                \"z_statistic\": z_stat,\n",
        "                \"p_value\": p_value,\n",
        "                \"significant\": p_value < 0.05 if not np.isnan(p_value) else None,\n",
        "                \"better_model\": model1.split('/')[-1] if overall_mean_diff > 0 else model2.split('/')[-1],\n",
        "                \"pearson_correlation\": overall_correlation\n",
        "            }\n",
        "        \n",
        "        # Write all metrics results for this model comparison to a single file\n",
        "        with open(f'variance_results_{comparison_key}.txt', 'w') as f:\n",
        "            variance_results_text += f\"Overall Variance Reduction Analysis:\\n\"\n",
        "            f.write(variance_results_text)\n",
        "    \n",
        "    return comparison_results\n",
        "\n",
        "comparison_results = compare_model_performances(all_models_stats, all_runs_model_metrics)\n",
        "\n",
        "# Save results to file\n",
        "with open('comparison_results.json', 'w') as f:\n",
        "    # Convert numpy types to native Python types for JSON serialization\n",
        "    def convert_to_serializable(obj):\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
        "            np.int16, np.int32, np.int64, np.uint8,\n",
        "            np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.bool_)):\n",
        "            return bool(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        elif obj is None:\n",
        "            return None\n",
        "        return obj\n",
        "    \n",
        "    serializable_results = json.loads(\n",
        "        json.dumps(comparison_results, default=convert_to_serializable)\n",
        "    )\n",
        "    json.dump(serializable_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comparison results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract metrics and models from comparison_results\n",
        "metrics = [metric.replace('_descr', '') for metric in list_of_metrics]\n",
        "model_pairs = list(comparison_results.keys())\n",
        "\n",
        "# Create figure with subplots for each metric\n",
        "fig, axes = plt.subplots(2, 3, figsize=(25, 20), dpi=600)  # Increased DPI for higher resolution\n",
        "fig.suptitle('Model Comparison Results by Metric', fontsize=16, y=1.05)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Extract data for this metric\n",
        "    means = []\n",
        "    cis = []\n",
        "    labels = []\n",
        "    \n",
        "    for pair in model_pairs:\n",
        "        metric_data = comparison_results[pair][metric]\n",
        "        means.append(metric_data['mean_difference'])\n",
        "        # ci_margin = metric_data['ci_margin']\n",
        "        cis.append([metric_data['ci_low'], \n",
        "                   metric_data['ci_high']])\n",
        "        labels.append(pair)\n",
        "\n",
        "    # Create bar plot\n",
        "    bars = ax.bar(range(len(means)), means)\n",
        "    \n",
        "    # Add error bars for confidence intervals\n",
        "    ax.errorbar(range(len(means)), means, \n",
        "               yerr=[[m - ci[0] for m, ci in zip(means, cis)],\n",
        "                     [ci[1] - m for m, ci in zip(means, cis)]],\n",
        "               fmt='none', color='black', capsize=5)\n",
        "    \n",
        "    # Add horizontal line at y=0\n",
        "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_title(f'{metric.capitalize()}')\n",
        "    ax.set_xticks(range(len(means)))\n",
        "    ax.set_xticklabels(labels, rotation=90) # Changed to vertical labels\n",
        "    ax.set_ylabel('Mean Difference')\n",
        "    \n",
        "    # Color bars based on statistical significance\n",
        "    for j, bar in enumerate(bars):\n",
        "        if comparison_results[model_pairs[j]][metric]['p_value'] < 0.05:\n",
        "            bar.set_color('darkred')\n",
        "        else:\n",
        "            bar.set_color('lightgray')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot before showing with high resolution\n",
        "plt.savefig('model_comparisons.png', bbox_inches='tight', dpi=600)  # Increased DPI for higher resolution\n",
        "\n",
        "# Show plot after saving\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_table(comparison_results, metrics):\n",
        "    \"\"\"\n",
        "    Creates a formatted table from comparison results.\n",
        "    \n",
        "    Args:\n",
        "        comparison_results (dict): The comparison results dictionary\n",
        "        metrics (list): List of metrics to include\n",
        "        \n",
        "    Returns:\n",
        "        str: Formatted markdown table\n",
        "    \"\"\"\n",
        "    # Table header\n",
        "    table = \"| Metric | Model | Baseline | Model - Baseline | 95% Conf. Interval | Correlation |\\n\"\n",
        "    table += \"|--------|--------|-----------|-----------------|-------------------|-------------|\\n\"\n",
        "    \n",
        "    # Add rows for each comparison and metric\n",
        "    for pair in comparison_results:\n",
        "        model1, model2 = pair.split('_vs_')\n",
        "        for metric in metrics:\n",
        "            results = comparison_results[pair][metric]\n",
        "            \n",
        "            row = f\"| {metric} | {model1} | {model2} | \"\n",
        "            row += f\"{results['mean_difference']:.1%} | \"\n",
        "            row += f\"({results['ci_low']:.1%}, {results['ci_high']:.1%}) | \"\n",
        "            row += f\"{results['pearson_correlation']:.2f} |\\n\"\n",
        "            \n",
        "            table += row\n",
        "            \n",
        "    return table\n",
        "\n",
        "# Create and print the table\n",
        "metrics = [m.replace('_descr', '') for m in list_of_metrics]\n",
        "comparison_table = create_comparison_table(comparison_results, metrics)\n",
        "print(comparison_table)\n",
        "\n",
        "# Save table to file\n",
        "with open('comparison_table.txt', 'w') as f:\n",
        "    f.write(comparison_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Power Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "def perform_power_analysis(effect_size=0.5, alpha=0.05, power=0.8):\n",
        "    \"\"\"\n",
        "    Perform power analysis to determine required sample size.\n",
        "    \n",
        "    Args:\n",
        "        effect_size (float): Expected effect size (Cohen's d)\n",
        "        alpha (float): Significance level\n",
        "        power (float): Desired statistical power\n",
        "        \n",
        "    Returns:\n",
        "        int: Required sample size per group\n",
        "    \"\"\"\n",
        "    analysis = TTestIndPower()\n",
        "    sample_size = analysis.solve_power(\n",
        "        effect_size=effect_size,\n",
        "        alpha=alpha,\n",
        "        power=power,\n",
        "        alternative='two-sided'\n",
        "    )\n",
        "    return int(np.ceil(sample_size))\n",
        "\n",
        "# First, determine required sample size\n",
        "required_samples = perform_power_analysis(effect_size=0.1254, alpha=0.05, power=0.8)  #These parameters result in a sample size of 1000\n",
        "print(f\"Required samples per model for statistical power: {required_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For real-time inference (below implementation only for meta-llama/Meta-Llama-3.1-8B-Instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# # del pipeline #Otherwise too much memory is used\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto')\n",
        "\n",
        "# #Example of real-time response generation\n",
        "# messages=[{\"role\": \"user\", \"content\": \"What is the chemical formula of water?\"}]\n",
        "\n",
        "# inputs_tokenized = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#     return_dict=True,\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# input_ids = inputs_tokenized['input_ids']\n",
        "\n",
        "# # Generate tokens one by one\n",
        "# max_length = 256\n",
        "# output_ids = input_ids\n",
        "# for _ in range(256):\n",
        "#     outputs = model.generate(\n",
        "#         output_ids,\n",
        "#         max_new_tokens=1,\n",
        "#         do_sample=True,\n",
        "#         top_k=50,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "#     new_token_id = outputs[0, -1].item()\n",
        "#     if new_token_id == tokenizer.eos_token_id:\n",
        "#         break\n",
        "#     output_ids = torch.cat([output_ids, outputs[:, -1:]], dim=1)\n",
        "#     new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
        "#     print(new_token, end=\"\", flush=True)\n",
        "\n",
        "# print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other evaluators from Langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations\n",
        "# https://docs.smith.langchain.com/old/evaluation/quickstart\n",
        "\n",
        "# from langsmith.evaluation import LangChainStringEvaluator\n",
        "\n",
        "# eval_llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0.0, seed=42)\n",
        "\n",
        "# #Evaluators\n",
        "# qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm}) #LLM just gives 'correct' or 'incorrect' based on reference answer\n",
        "# context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm}) #Also uses reference context of example outputs to do the above\n",
        "# cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm}) #Same as above but with chain of thought 'reasoning'\n",
        "\n",
        "#Prompts Used internally:\n",
        "\n",
        "# 1) context_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. \n",
        "# It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 2) cot_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "# Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# EXPLANATION: step by step reasoning here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 3) qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# TRUE ANSWER: true answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, use custom prompts as shown below (and set {\"prompt\": PROMPT} as additional argument inside the config above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "# _PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in chemical engineering answers to questions.\n",
        "# You are grading the following question:\n",
        "# {query}\n",
        "# Here is the real answer:\n",
        "# {answer}\n",
        "# You are grading the following predicted answer:\n",
        "# {result}\n",
        "# Respond with CORRECT or INCORRECT:\n",
        "# \"\"\"\n",
        "\n",
        "# PROMPT = PromptTemplate(\n",
        "#     input_variables=[\"query\", \"result\", \"answer\"], template=_PROMPT_TEMPLATE\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes: Non-reproducible results, even when seed set (https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed), temperature=0 (top_p should not change when we changed temperature - smaller values result in more constrained and focused response - https://medium.com/@rasithbm/chatopenai-parameters-83bef49f6384)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b9c5f781e34806b9756d9e3e553a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cacd8a8bd03b4d0d83d266fe85e8ee65",
              "IPY_MODEL_abf8eb8102384433a59628820355d272",
              "IPY_MODEL_be7aa2993d57460c9d4f23c090e42c36"
            ],
            "layout": "IPY_MODEL_75aa3a3eb1b8420f9f26d404505e46cc"
          }
        },
        "0adc7382479c412f9c9230a17b56ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c13fc64f2e143b29105ec10e444b779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ace1e934716404a9340bce56cb1a3cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e3db7de6fcc04e6ba738f7ae78cef24d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1bd3eb0157a3477f907dae0c8fdbbec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24713d9c124b41488af127cfd3d1321e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331036e81d104ab49c44bcbde2d873f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec4c77240854f3ebab46e0b7d307f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ace1e934716404a9340bce56cb1a3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9db2c468cf4dc598a80da6a548d034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9d371e98fc45329cf381bc36a290ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331036e81d104ab49c44bcbde2d873f7",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd3eb0157a3477f907dae0c8fdbbec4",
            "value": ""
          }
        },
        "6e84ac6346d8450d9814b9f1a647164c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75aa3a3eb1b8420f9f26d404505e46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87311a42fde0441fb2e88a0655a95f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c13fc64f2e143b29105ec10e444b779",
              "IPY_MODEL_a2ac8fac33444da794be1f25a9c0d702",
              "IPY_MODEL_c0e527b08dd942a684246e2db22ed22d"
            ],
            "layout": "IPY_MODEL_cf7de095d0514bbf937d0632c777a232"
          }
        },
        "9253c9636a6c4e20b215ff11c928be07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "981e94324ba64b548259b1f1d297a564": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24713d9c124b41488af127cfd3d1321e",
            "placeholder": "​",
            "style": "IPY_MODEL_0adc7382479c412f9c9230a17b56ea42",
            "value": " 9/? [00:06&lt;00:00,  6.70s/it]"
          }
        },
        "98971ab8fe5c411f9c8c4c77753a745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bc61359ce44954bd235566d9ada2d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f628c84495494694a62ca9c181ec63ba",
            "value": 1
          }
        },
        "9b527616d5a647d88a33b14ee2712211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9d371e98fc45329cf381bc36a290ae",
              "IPY_MODEL_98971ab8fe5c411f9c8c4c77753a745f",
              "IPY_MODEL_981e94324ba64b548259b1f1d297a564"
            ],
            "layout": "IPY_MODEL_c1ffb867972444579481d5408abcdba9"
          }
        },
        "a2ac8fac33444da794be1f25a9c0d702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec4c77240854f3ebab46e0b7d307f74",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcfaa70f8c14dfcb0528b0cc0573db3",
            "value": 2
          }
        },
        "abf8eb8102384433a59628820355d272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6d3c439b254aa793c5d39304170849",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fac87e8a3a044de994d726896d479de3",
            "value": 1
          }
        },
        "b68644c249c04e059c924e1165a01370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7aa2993d57460c9d4f23c090e42c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6defd5c41f94c0ebda4080bb09c19c3",
            "placeholder": "​",
            "style": "IPY_MODEL_6e84ac6346d8450d9814b9f1a647164c",
            "value": " 9/? [01:10&lt;00:00,  5.27s/it]"
          }
        },
        "c0e527b08dd942a684246e2db22ed22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed1e9eb6e6bc4452b3d12aecffb6cc2a",
            "placeholder": "​",
            "style": "IPY_MODEL_9253c9636a6c4e20b215ff11c928be07",
            "value": " 2/2 [00:18&lt;00:00,  8.66s/it]"
          }
        },
        "c1ffb867972444579481d5408abcdba9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6d3c439b254aa793c5d39304170849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cacd8a8bd03b4d0d83d266fe85e8ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68644c249c04e059c924e1165a01370",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9db2c468cf4dc598a80da6a548d034",
            "value": ""
          }
        },
        "cf7de095d0514bbf937d0632c777a232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3db7de6fcc04e6ba738f7ae78cef24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8bc61359ce44954bd235566d9ada2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ed1e9eb6e6bc4452b3d12aecffb6cc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f628c84495494694a62ca9c181ec63ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6defd5c41f94c0ebda4080bb09c19c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac87e8a3a044de994d726896d479de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdcfaa70f8c14dfcb0528b0cc0573db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
