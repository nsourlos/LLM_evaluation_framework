{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5Kh-zLB9lk"
      },
      "source": [
        "# Evaluate LLM results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no2PHIOWCBdA"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U2Dpuc2xtmmS"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install datasets==2.20.0\n",
        "# !pip install -U langsmith==0.1.99\n",
        "# !pip install langchain_openai==0.1.22\n",
        "# !pip install langchain==0.2.13\n",
        "# !pip install langchain_community==0.2.12                          \n",
        "# !pip install transformers==4.44.0\n",
        "# !pip install termcolor==2.4.0\n",
        "# !pip install accelerate==0.33.0\n",
        "# !pip install pandas==2.2.2\n",
        "# !pip install openpyxl==3.1.5\n",
        "# !pip install python-dotenv==1.0.1\n",
        "# !pip install einops==0.8.0\n",
        "# !pip install wheel==0.44.0\n",
        "# !pip install sentencepiece==0.2.0\n",
        "# !pip install protobuf==5.27.3 #Mistral models needs this\n",
        "# !pip install groq==0.10.0 #Groq models needs this\n",
        "# !pip install matplotlib==3.9.2\n",
        "# !pip install seaborn==0.13.2\n",
        "\n",
        "# !pip install flash-attn==2.6.3 #Install it at the end after wheel has been installed\n",
        "# !pip install anthropic==0.34.1 #Anthropic models needs this\n",
        "\n",
        "# #Only if CPU is used\n",
        "# !pip install torch==2.0.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!jupyter lab --ServerApp.iopub_data_rate_limit=1e10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RunPod specific parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For RunPod change to persistent storage directory\n",
        "import os\n",
        "os.chdir('/workspace')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify Path and Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path ='/workspace/few_questions_only.xlsx' #Dataset generated with the help of GPT-4o - Has to be an excel file with 'input' and 'output' columns\n",
        "#'/Users/nikolaossourlo/Desktop/Example_QA_data_raw.xlsx' #For MacOS\n",
        "#'C:/Users/soyrl/Desktop/Example_QA_data_raw.xlsx' #For Windows\n",
        "#'/content/drive/My Drive/Example_QA_data_raw.xlsx' #For Google Colab\n",
        "#'/home/nikolaossourlo/Example_QA_data_raw.xlsx' #For Delft Blue\n",
        "#'/workspace/Example_QA_data_raw.xlsx' #For RunPod\n",
        "\n",
        "custom_cache_dir=\"/workspace/cache/huggingface\" #Save models here so that we don't have to download them again\n",
        "#\"/scratch/nikolaossourlo/cache\" in Delft Blue\n",
        "\n",
        "# Check if custom_cache_dir is defined, otherwise use default behavior\n",
        "try:\n",
        "    cache_dir=custom_cache_dir\n",
        "except:\n",
        "    cache_dir=None\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import traceback\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
        "\n",
        "# Get the OpenAI API key\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY_DRACO')\n",
        "\n",
        "#Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# Log in with your Hugging Face token\n",
        "login(token=os.getenv('HF_TOKEN'))\n",
        "\n",
        "# print(openai_api_key)\n",
        "# print(langsmith_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select model and name for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model to generate responses to questions - Sometimes we might have to restart session and comment out the models that have already been run\n",
        "models=[ \n",
        "    # \"anthropic/claude-3-5-sonnet-20241022\",\n",
        "    # \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", #Takes 2.5-3mins in A4500 (20GB VRAM) and in Delft Blue (35min for 44Q&A pairs in V100 32GB)\n",
        "    # \"microsoft/Phi-3.5-mini-instruct\", #Took 5mins in A40 with 48GB VRAM, 2mins in A4500 with 20GB VRAM, 3mins in Delft Blue (50min for 44Q&A pairs in V100 32GB)\n",
        "    # \"mistralai/Mistral-7B-Instruct-v0.3\", #4mins in A40 with 48GB VRAM, 2.5mins in A4500 with 20GB VRAM and in Delft Blue\n",
        "    # \"Qwen/Qwen2-7B-Instruct\", #4mins in A40 with 48GB VRAM, 2 mins in A4500 with 20GB VRAM, 2.5mins in Delft Blue\n",
        "    # 'AI-MO/NuminaMath-7B-TIR', #2.5 in A4500 with 20GB VRAM and in Delft Blue - We can also try 01-ai/Yi-Coder-9B-Chat\n",
        "    # 'microsoft/Phi-3-mini-4k-instruct', #6 mins in RTX3090\n",
        "    # 'microsoft/phi-4', #14B parameters\n",
        "    # \"google/gemma-2-9b-it\", #More than 20GB of GPU memory needed - Works with A40 with 48GB VRAM (8mins), but not with A4500 - 20GB, and V100 - 32GB, 4.5mins in Delft Blue\n",
        "    # 'mistralai/Mistral-Nemo-Instruct-2407', #12B parameters, 11mins in 2 RTX3090, 16mins in V100 with 32GB VRAM (48mins to run over all 44 Q&A pairs)\n",
        "    # 'openai/gpt-4o-mini' #Costs very low ~0.01$ for 9 Q&A pairs.\n",
        "    ] #All above models need ~200GB space. For 44 Q&A pairs it takes ~50min/model\n",
        "\n",
        "# Groq models are defined as: groq_website/model_name e.g. 'groq_website/llama-3.1-70b-versatile'\n",
        "# OpenAI models are defined as: 'openai/model_name', e.g. 'openai/gpt-4o-mini'\n",
        "# Anthropic models are defined as 'anthropic/model_name', e.g. 'anthropic/claude-3-haiku-20240307' - Couldn't use due to billing issues\n",
        "\n",
        "# I couldn't run 'nvidia/Mistral-NeMo-Minitron-8B-Base', \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" (Conflicting dependencies),\n",
        "# 'google/recurrentgemma-9b-it' # RecurrentGemmaForCausalLM.forward() got an unexpected keyword argument 'position_ids'\n",
        "#Large models take more time (2min/generation for Mistral 12B)\n",
        "\n",
        "#Define model to act as a judge\n",
        "judge_model='openai/gpt-4o-mini' #If used with Llama, only 0.01$ for 9 Q&A pairs for gpt-4o-mini, and 0.22$ for gpt-4o\n",
        "\n",
        "#Define maximum number of tokes in the judge LLM output\n",
        "max_output_tokens=500\n",
        "\n",
        "#Limit of tokens in the generated response from LLM\n",
        "generate_max_tokens=1000\n",
        "\n",
        "#Inference on whole dataset?\n",
        "inference_on_whole_dataset=True\n",
        "\n",
        "#Number of times to resample the dataset\n",
        "n_resamples=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define prompts for custom evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_prompt=\"\"\"\n",
        "You are an autoregressive language model that acts as a judge in comparing a predicted vs an actual answer to a questions.\n",
        "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend \n",
        "a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. \n",
        "Your users are experts in chemical engineering, so they already know you're a language model and your capabilities and limitations, so don't \n",
        "remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. \n",
        "Don't be verbose in your answers, but do provide details and examples where it might help the explanation. \n",
        "\"\"\" #This is common for all prompts below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to completeness compared to the completeness of a given actual, golden standard answer. \n",
        "The completeness metric evaluates the extent to which the user's question is answered in full in the predicted response. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: There is no response.\n",
        "2: No parts of a suitable answer are present.\n",
        "3: Few elements of a complete answer are present.\n",
        "4: Most elements of a complete answer are present.\n",
        "5: The response covers all elements of a complete answer.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "relevance_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to relevance compared to the relevance of a given actual, golden standard answer. \n",
        "The relevance metric evaluates the amount of irrelevant information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response answers something else, not the user's question.\n",
        "2: The response answers the user's question but the information provided is mostly irrelevant.\n",
        "3: The response answers the user's question but contains more irrelevant information than relevant information.\n",
        "4: The response answers the user's question, and shares a bit of irrelevant information.\n",
        "5: The response answers the user's question and contains no irrelevant information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "conciseness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to conciseness compared to the conciseness of a given actual, golden standard answer. \n",
        "The conciseness metric evaluates the amount of unexpected extra information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response is too long and stops before completion or enters an infinite loop.\n",
        "2: The response includes a lot of extra information and uses flowery language.\n",
        "3: The response includes a lot of extra information or uses flowery language.\n",
        "4: The response is short and includes a small amount of extra information.\n",
        "4: The response is as short as possible while still answering the prompt.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "confidence_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to confidence compared to the confidence of a given actual, golden standard answer. \n",
        "The condifence metric evaluates the degree of assurance that is conveyed the response that the predicted answer is correct. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: Complete Rejection. The response makes it clear that the given answer is incorrect or that no correct answer can be provided.\n",
        "2: Doubt and Disagreement. The response suggests that the answer is likely incorrect or raises significant concerns.\n",
        "3: Uncertainty. The response indicates that the answer could be correct, but there is significant doubt or insufficient evidence.\n",
        "4: Moderate Agreement. The response leans towards the answer being correct but acknowledges some uncertainty.\n",
        "5: Full Endorsement. The reponse confidentely asserts that the given answer is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "factuality_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to factuality compared to the factuality of a given actual, golden standard answer.\n",
        " The factuality metric evaluates the degree of hallucination contained in a response or, in other words, how accurate a given response is.\n",
        "You can assign a score from 1 to 5, with the following interpretations:\n",
        "1: The response is a complete hallucination\n",
        "2: The response is mostly a hallucination but does not change key information from the prompt (such as chemical identifiers).\n",
        "3: The response contains large amounts of both hallucinations and factual information.\n",
        "4: The response includes mostly factual information with slight hallucinations.\n",
        "5: The response only includes factual information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "judgement_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to judgement compared to the judgement of a given actual, golden standard answer.\n",
        "The judgment metric assesses how strongly the response implies its correctness, taking into account the actual accuracy of the answer.\n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response confidently claims a hallucination as truth.\n",
        "2: The response misinterprets information received in the prompt.\n",
        "3: The response shows that the model is unsure about the answer or states that information is theoretical.\n",
        "4: The response is wrong but it is made clear that the answer is wrong or that the model is unable to provide a correct answer.\n",
        "5: The response is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#How the dataset will be named in Langsmith\n",
        "def get_dataset_name(model_name, judge_model):\n",
        "    try: #For Hugging Face models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name.split('/')[1]+'_with_judge_'+judge_model+'_beam_search_statistics'\n",
        "    except: #For OpenAI models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name+'_with_judge_'+judge_model+'_beam_search_statistics'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX9V2ASWCQG5"
      },
      "source": [
        "Google Drive mount (If run in Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLsdaFvRthOE",
        "outputId": "ee976853-2292-4eee-a380-812283627e56"
      },
      "outputs": [],
      "source": [
        "if 'content/drive/My Drive' in file_path:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVqBHaT2s6Aq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "qa=pd.read_excel(file_path) #Read Excel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6zdJxKCubI"
      },
      "source": [
        "Create Dataset from df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUw8Puxfs6Az"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "loaded_dataset=Dataset.from_pandas(qa)\n",
        "\n",
        "if inference_on_whole_dataset==False:\n",
        "    loaded_dataset = loaded_dataset.train_test_split(test_size=0.2, seed=42) #Used if going to fine-tune in part of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf6thikds6A1"
      },
      "outputs": [],
      "source": [
        "if inference_on_whole_dataset==False:\n",
        "    dataset_train=loaded_dataset['train']\n",
        "    dataset_test=loaded_dataset['test']\n",
        "else:\n",
        "    dataset_test=loaded_dataset #When we use the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXxkzQoHs6A5"
      },
      "source": [
        "Create Langsmith Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtdIrA3Ds6A8",
        "outputId": "90a9b4dd-e91a-4773-934b-2bf58cd8e3a8"
      },
      "outputs": [],
      "source": [
        "#https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets\n",
        "\n",
        "from langsmith import Client\n",
        "\n",
        "example_inputs = [(x['input'],x['output']) for x in dataset_test]\n",
        "print(example_inputs)\n",
        "\n",
        "def create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key):\n",
        "\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    try:\n",
        "        #Load the dataset if already exists\n",
        "        for existing_dataset in client.list_datasets():\n",
        "            if existing_dataset.name==dataset_name:\n",
        "                dataset_langsmith=existing_dataset\n",
        "        for x in dataset_langsmith:\n",
        "            print(\"Dataset Loaded\")\n",
        "            break\n",
        "\n",
        "    except: #Otherwise create it\n",
        "        print(\"Dataset not found. Creating new dataset\")\n",
        "        # Storing inputs in a dataset lets us run chains and LLMs over a shared set of examples.\n",
        "        dataset_langsmith = client.create_dataset(dataset_name=dataset_name,\n",
        "                                                description=\"Q&A chemical engineering.\")\n",
        "\n",
        "        for input_prompt, output_answer in example_inputs:\n",
        "            client.create_example(\n",
        "                inputs={\"question\": input_prompt.replace('\\n', ' ')},\n",
        "                outputs={\"answer\": output_answer.replace('\\n', ' ')},\n",
        "                # metadata={\"source\": \"Wikipedia\"},\n",
        "                dataset_id=dataset_langsmith.id,\n",
        "            )\n",
        "\n",
        "    return dataset_langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/cookbook/introduction\n",
        "# https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators\n",
        "# https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator\n",
        "\n",
        "from langsmith.schemas import Run, Example\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from termcolor import colored\n",
        "\n",
        "list_of_metrics=['completeness_descr','relevance_descr','conciseness_descr','confidence_descr','factuality_descr','judgement_descr']\n",
        "\n",
        "#Function that compares the real answer with the predicted answer of an LLM and returns a score based on the evaluation\n",
        "def factor_evaluator(run: Run, example: Example) -> dict: \n",
        "    # print(\"Run:\",run)\n",
        "\n",
        "    question=run.inputs.get(\"inputs\")['question']\n",
        "    # print(\"Question:\",question)\n",
        "    actual_answer = example.outputs.get(\"answer\")\n",
        "    # print(\"Real answer:\",example.outputs.get(\"answer\"))\n",
        "    predicted_answer = run.outputs.get(\"output\")\n",
        "    # print(\"Predicted Answer:\",answer)\n",
        "    \n",
        "    # Check if there is output from LLM\n",
        "    if not predicted_answer:\n",
        "        print(\"No output from LLM\")\n",
        "        return {\"key\": \"custom_metric\" , \"score\": 0} \n",
        "    \n",
        "    else:\n",
        "        scores={} #Store scores for each metric\n",
        "        descriptions={} #Store descriptions for each metric\n",
        "        \n",
        "        for metric_name in list_of_metrics: #Iterate through all metrics\n",
        "            print(\"Evaluating based on:\",metric_name)\n",
        "            metric_value=common_prompt+eval(metric_name) #Get the actual description of the metric\n",
        "\n",
        "            # Define roles and placeholders\n",
        "            chat_template = ChatPromptTemplate.from_messages(\n",
        "            [(\"system\", metric_value),\n",
        "                (\"user\", \"Question: {question}, Actual answer: {actual_answer}, Predicted answer: {predicted_answer}\"),\n",
        "                # (\"ai\", \"It's sunny and warm outside.\"), #Use this if we want to use few shot prompts\n",
        "            ]\n",
        "            )\n",
        "\n",
        "            messages = chat_template.format_messages(question=question, actual_answer=actual_answer, predicted_answer=predicted_answer)\n",
        "            # print(\"Messages:\",messages)\n",
        "\n",
        "            formatted_messages = [(role, msg.content) for role, msg in zip([\"system\", \"user\"], messages)]\n",
        "            # print(\"Formatted messages:\",formatted_messages) #[('system', 'You are an autoregressive lan....', 'user':.....)]\n",
        "\n",
        "            # Initialize the model and get response\n",
        "            llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0, max_tokens=max_output_tokens, seed=42)\n",
        "            ai_response = llm.invoke(formatted_messages)\n",
        "\n",
        "            # Output\n",
        "            # print(colored(\"System message:\"+ messages[0].content,'blue'))\n",
        "            print(colored(\"User message:\"+ messages[1].content, 'green'))\n",
        "            print(colored(\"AI message:\"+ ai_response.content,'red'))\n",
        "\n",
        "            #Decide what the final score is based on output\n",
        "            if \"FINAL SCORE:\" in ai_response.content: \n",
        "                score = int(ai_response.content.split(\"FINAL SCORE:\")[1])\n",
        "            else:\n",
        "                print(\"Invalid response from LLM:\", ai_response.content)\n",
        "                score = 0\n",
        "\n",
        "            scores[metric_name]=score\n",
        "            descriptions[metric_name]=ai_response.content\n",
        "            print(\"Scores:\",scores)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"results\":[ #We always need 'key', 'score' pairs\n",
        "            {\"key\": \"completeness\" , \"score\": scores['completeness_descr'],\"value\":descriptions['completeness_descr']},\n",
        "            {\"key\": \"relevance\" , \"score\": scores['relevance_descr'], \"value\":descriptions['relevance_descr']},\n",
        "            {\"key\": \"conciseness\" , \"score\": scores['conciseness_descr'], \"value\":descriptions['conciseness_descr']},\n",
        "            {\"key\": \"confidence\" , \"score\": scores['confidence_descr'], \"value\":descriptions['confidence_descr']},\n",
        "            {\"key\": \"factuality\" , \"score\": scores['factuality_descr'], \"value\":descriptions['factuality_descr']},\n",
        "            {\"key\": \"judgement\" , \"score\": scores['judgement_descr'], \"value\":descriptions['judgement_descr']}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Models that Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.random.manual_seed(0) #Set for reproducibility\n",
        "\n",
        "def initialize_model(model_id):\n",
        "    # # Check if mps acceleration is available (For MacOS)\n",
        "    # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    # print(f\"Using device {device}\")\n",
        "    # model.to(device)\n",
        "\n",
        "    # transformers.set_seed(42) #Tried for reproducibility but didn't work\n",
        "    \n",
        "    pipeline = transformers.pipeline( \n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\":cache_dir},\n",
        "            # trust_remote_code=True,\n",
        "            device_map=\"auto\" #Use 'cuda' if one GPU available (works in Delft Blue with 32GB VRAM) - 'auto' the alternative for distributed over all available GPUs\n",
        "        )\n",
        "    return pipeline\n",
        "\n",
        "def get_model(model_id):\n",
        "    \"\"\"Given a model name, return the loaded model, tokenizer, and pipeline\"\"\"\n",
        "\n",
        "    if 'openai' not in model_id and 'groq_website' not in model_id and 'anthropic' not in model_id: #For Hugging Face models\n",
        "        pipeline=initialize_model(model_id)\n",
        "\n",
        "    #Returns below variables if defined, and returns None for any that are not.\n",
        "    model = locals().get('model', None)\n",
        "    tokenizer = locals().get('tokenizer', None)\n",
        "    pipeline = locals().get('pipeline', None)\n",
        "\n",
        "    return model, tokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def predict(inputs: dict) -> dict:\n",
        "    \"\"\"Given a question, return the answer from the model\"\"\"\n",
        "    \n",
        "    #Get these variables from the global scope\n",
        "    global model_name\n",
        "    \n",
        "    messages = [ #Only use the questions to ask the model to generate the response\n",
        "      {\"role\": \"user\", \"content\": inputs['question']},\n",
        "    ]\n",
        "\n",
        "    if 'gemma' not in model_name and 'anthropic' not in model_name: #Gemma doesn't support system message\n",
        "      messages.insert(0, {\"role\": \"system\", \"content\": \"You are a language model specialized in chemical engineering. Answer the following question:\"})\n",
        "    else: #For gemma add system prompt in user message\n",
        "      messages[0]['content']=\"You are a language model specialized in chemical engineering. Answer the following question: \" + messages[0]['content']\n",
        "    # print(\"Prompt:\",messages)\n",
        "\n",
        "    generation_args = { \n",
        "        \"max_new_tokens\": max_output_tokens, \n",
        "        \"return_full_text\": False, \n",
        "        \"temperature\": 0.05, #Has to be positive number - not considered from model when do_sample is False (reproducible results)\n",
        "        \"do_sample\": True, #Selects highest probability token if sets to False\n",
        "        \"num_beams\" : 5, #3 can also work if computationally intensive - more info on https://huggingface.co/blog/how-to-generate\n",
        "        #Warnings will be raised by some models\n",
        "\n",
        "        #If we only set temp!=0 or if we also set do_sample=False then warning: `do_sample` is set to `False`. However, `temperature` is set to `1e-08` \n",
        "        # -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
        "        # That means that the temperature is probably ignored\n",
        "        # Sometimes, results not reproducible if only temp is set\n",
        "        # A temparature of 0.01 or lower results in: \"Error running target function: probability tensor contains either `inf`, `nan` or element < 0\"\n",
        "      } \n",
        "    \n",
        "    if 'openai' not in model_name and 'groq_website' not in model_name and 'anthropic' not in model_name: #For Hugging Face models\n",
        "      response=pipeline(messages, **generation_args)[0]['generated_text']\n",
        "      print(model_name,':',response)\n",
        "\n",
        "    else: \n",
        "      if 'openai' in model_name:\n",
        "        try:\n",
        "          import openai\n",
        "          from langsmith.wrappers import wrap_openai\n",
        "                  \n",
        "          # Define OpenAI client\n",
        "          openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "          \n",
        "          response = openai_client.chat.completions.create(messages=messages, temperature=0, model=model_name.split('/')[1],  seed=42) \n",
        "          # print(\"Response:\",response.choices[0].message.content)\n",
        "          response=response.choices[0].message.content #That's the response without formatting\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"OpenAI Model ID:\",model_name)\n",
        "\n",
        "      elif 'groq_website' in model_name:\n",
        "        try:\n",
        "          from groq import Groq\n",
        "          client = Groq()\n",
        "          actual_model_name=model_name.split('/')[1]\n",
        "          response = client.chat.completions.create(\n",
        "              model=actual_model_name,\n",
        "              max_tokens=generate_max_tokens,\n",
        "              temperature=0,\n",
        "              messages=messages)\n",
        "          # print(\"Response from Groq:\",response.choices[0].message.content)\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"Groq Model ID:\",model_name)\n",
        "\n",
        "      elif 'anthropic' in model_name:\n",
        "        try:\n",
        "          import anthropic\n",
        "          client = anthropic.Anthropic()\n",
        "          response = client.messages.create(\n",
        "              model=model_name.split('/')[1],\n",
        "              messages=messages,\n",
        "              temperature=0,\n",
        "              max_tokens=max_output_tokens,\n",
        "          )\n",
        "          print(\"Response from Anthropic:\",response.content[0].text)\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"Anthropic Model ID:\",model_name)\n",
        "\n",
        "    return {\"output\": response}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_figures_metrics(all_runs_model_metrics, metric_names, model_name, judge_model):\n",
        "#     \"\"\"\n",
        "#     Creates visualizations and calculates statistics for evaluation metrics across multiple runs.\n",
        "\n",
        "#     Args:\n",
        "#         all_runs_model_metrics (dict): Nested dictionary containing evaluation metrics for each model and run.\n",
        "#             Structure: {model_id: [{metric1_descr_run1: [q1_score, q2_score, ...], \n",
        "#                                   metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "#                                  {metric1_descr_run2: [q1_score, q2_score, ...],\n",
        "#                                   metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "#                                  ...num_runs]}\n",
        "#             Example: {'model1': [{'completeness_descr_run1': [4.5, 3.0, 4.0], \n",
        "#                                 'relevance_descr_run1': [3.5, 4.0, 3.0]}, ...,\n",
        "#                                {'completeness_descr_run2': [4.0, 3.5, 4.5],\n",
        "#                                 'relevance_descr_run2': [3.0, 4.5, 3.5], ...},\n",
        "#                                ...num_runs]}\n",
        "#             Where each inner dictionary represents one run containing scores for each metric across all questions\n",
        "#         metric_names (list): Names of metrics to analyze and plot (e.g. ['completeness', 'relevance'])\n",
        "#         model_name (str): Name/identifier of the model being evaluated\n",
        "#         judge_model (str): Name/identifier of the model used for judging the evaluations\n",
        "\n",
        "#     Returns:\n",
        "#         dict: Summary statistics for each model, run and metric.\n",
        "#             Structure: {model_name: {run_idx: {metric_name: {\n",
        "#                 'mean': float,\n",
        "#                 'std_error': float, \n",
        "#                 'ci_low': float,\n",
        "#                 'ci_high': float\n",
        "#             }}}}\n",
        "#             Example: {'anthropic/claude-3-5-sonnet': {\n",
        "#                 '0': {'completeness': {'mean': 4.5, 'std_error': 0.5, \n",
        "#                                      'ci_low': 3.52, 'ci_high': 5.48},\n",
        "#                       'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "#                                   'ci_low': 2.52, 'ci_high': 4.48} , ...},\n",
        "#                 '1': {'completeness': {'mean': 4.5, 'std_error': 0.5,\n",
        "#                                      'ci_low': 3.52, 'ci_high': 5.48},\n",
        "#                       'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "#                                   'ci_low': 2.52, 'ci_high': 4.48}, ...},\n",
        "#                 ...num_runs}}\n",
        "\n",
        "#     The function generates several visualization types:\n",
        "#     - Individual histograms for each metric showing score distributions\n",
        "#     - Error bars indicating means and confidence intervals\n",
        "#     - Overlapping bar plots comparing metrics\n",
        "#     - Stacked distribution plots showing relative frequencies of scores\n",
        "\n",
        "#     All plots are saved as PNG files with names indicating the judge model,\n",
        "#     evaluated model, run index, and plot type.\n",
        "#     \"\"\"\n",
        "\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import seaborn as sns\n",
        "#     import numpy as np\n",
        "#     from scipy.stats import t\n",
        "#     from collections import Counter\n",
        "\n",
        "#     summary_stats_all_runs={} #Keep track of summary statistics over all runs\n",
        "\n",
        "#     for run_idx, metric_values_run in enumerate(all_runs_model_metrics[model_name]): #Loop over runs\n",
        "\n",
        "#         # Colors for separate plots\n",
        "#         colors = sns.color_palette(\"Set3\", len(metric_names))\n",
        "\n",
        "#         # Create two figures - one with separate subplots and one overlaid\n",
        "#         fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 18))        \n",
        "#         plt.subplots_adjust(hspace=0.6, top=0.94)\n",
        "\n",
        "#         # Set titles\n",
        "#         fig.suptitle(f'Metric Distributions for {model_name} (Run {run_idx})', fontsize=16)\n",
        "\n",
        "#         # Define the bin edges explicitly to ensure consistency\n",
        "#         bin_edges = np.arange(0.0, 5.6, 0.2)  # Adjust to cover the range 1-5 with bins of width 1\n",
        "\n",
        "#         # Extract all values from the metric_values_run dictionary\n",
        "#         metric_names = [name.replace('_descr', '') for name in metric_values_run]\n",
        "\n",
        "#         #Summary statistics over one run\n",
        "#         run_stats={}\n",
        "\n",
        "#         for metric_idx, (metric_name, values) in enumerate(metric_values_run.items()): #Loop over runs' metric names and values (num_questions values)\n",
        "\n",
        "#             # Remove _descr from metric name if present\n",
        "#             clean_metric_name = metric_name.replace('_descr', '') #This is over one run and over one metric (but over all questions)\n",
        "#             metric_name=metric_names[metric_idx]\n",
        "#             assert clean_metric_name==metric_name, \"Metric name mismatch\"\n",
        "\n",
        "#             mean_value=np.mean(values) #Mean of the metric over single run and over single metric (but over all questions)\n",
        "#             std_error=np.std(values, ddof=1)/np.sqrt(len(values)) # ddof=1 to divide by n-1 to calculate the sample sd, default (ddof=0) calculates the population sd\n",
        "\n",
        "#             assert np.std(values, ddof=1)==np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), \"Standard deviation calculation mismatch\"\n",
        "#             # print(\"Standard deviation calculation confirmed\", np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), np.std(values, ddof=1)) #works\n",
        "\n",
        "#             # Plot on individual subplot\n",
        "#             sns.histplot(values, bins=bin_edges, color=colors[metric_idx], ax=axes[metric_idx], kde=False)\n",
        "            \n",
        "#             # Calculate confidence intervals - didn't use t_critical=t.ppf(0.975, df=len(values)-1) since we're using sample standard deviation\n",
        "#             margin_of_error = 1.96 * std_error\n",
        "#             ci_low = mean_value - margin_of_error\n",
        "#             ci_high = mean_value + margin_of_error\n",
        "\n",
        "#             # Add error bar to show confidence interval on individual plots\n",
        "#             # axes[metric_idx].errorbar(mean_value, axes[metric_idx].get_ylim()[1]/2, \n",
        "#             #                 xerr=margin_of_error,\n",
        "#             #                 color='black',\n",
        "#             #                 capsize=5,\n",
        "#             #                 capthick=1,\n",
        "#             #                 elinewidth=2,\n",
        "#             #                 marker='o')\n",
        "#             # Create error bar object but don't add it yet\n",
        "\n",
        "#             # Store error bar parameters for this metric\n",
        "#             if metric_idx == 0:\n",
        "#                 error_bars = []\n",
        "#             error_bars.append((mean_value, axes[metric_idx].get_ylim()[1]/2, margin_of_error))\n",
        "\n",
        "#             run_stats[metric_name]={\"mean\":mean_value,\"std_error\":std_error,\"ci_low\":ci_low,\"ci_high\":ci_high}\n",
        "#             axes[metric_idx].set_title(f'{metric_name} (Mean: {np.mean(mean_value):.2f} ± {np.mean(std_error):.2f} SE, CI: {ci_low:.2f}-{ci_high:.2f})')\n",
        "#             axes[metric_idx].set_xlim(0, 5.5) #Keep 0 in case of errors\n",
        "#             axes[metric_idx].set_ylabel('Frequency')\n",
        "#             # axes[metric_idx].set_yticks(range(0, 11, 5)) #Set y-ticks at intervals of 5\n",
        "\n",
        "#             # Hide x-axis labels and ticks for all but the last subplot\n",
        "#             if metric_idx < len(metric_names) - 1:\n",
        "#                 axes[metric_idx].set_xlabel('')\n",
        "#             else:\n",
        "#                 axes[metric_idx].set_xlabel('Values')\n",
        "\n",
        "#         # Save version without error bars\n",
        "#         plt.figure(fig.number)\n",
        "#         plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions_no_error_bars.png')\n",
        "\n",
        "#         # Add error bars and save version with them\n",
        "#         for i, error_bar in enumerate(error_bars):\n",
        "#             mean, ylim, margin = error_bar\n",
        "#             axes[i].errorbar(mean, ylim,\n",
        "#                            xerr=margin,\n",
        "#                            color='black',\n",
        "#                            capsize=5,\n",
        "#                            capthick=1,\n",
        "#                            elinewidth=2,\n",
        "#                            marker='o')          \n",
        "\n",
        "#         plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions.png')\n",
        "#         plt.close('all')\n",
        "\n",
        "#             # Print summary statistics\n",
        "#         print(f\"\\nSummary Statistics over run {run_idx}:\")\n",
        "#         print(\"-\" * 50)\n",
        "#         for metric, stats in run_stats.items():\n",
        "#             print(f\"{metric}:\")\n",
        "#             print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "#             print(f\"  Standard Error: {stats['std_error']:.2f}\")\n",
        "#             print(f\"  CI: {stats['ci_low']:.2f}-{stats['ci_high']:.2f}\")\n",
        "#             print(\"-\" * 50)\n",
        "\n",
        "#         summary_stats_all_runs[run_idx]=run_stats #Add summary stats for one run to the dictionary\n",
        "\n",
        "#         grouped_values=list(metric_values_run.values()) #Values of all metrics for one run over all questions. There are num_metrics lists in that list. \n",
        "#         values = [val for sublist in grouped_values for val in sublist] #Flatten the list - Size is num_questions*num_metrics (1st metric questions, 2nd metric questions, etc)\n",
        "\n",
        "#         #  Split values into groups for each metric\n",
        "#         # grouped_values = [values[i:i+2] for i in range(0, len(values), 2)] #List of lists, each sublist is a metric's values for one run over all questions\n",
        "\n",
        "#         # Create a figure and axis\n",
        "#         plt.figure(figsize=(10, 6))\n",
        "\n",
        "#         # Define colors for each metric\n",
        "#         colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "#         # First count all frequencies per score (1-5) per metric for one run over all questions\n",
        "#         score_metric_counts = {}\n",
        "#         question_scores_by_metric = {metric: [] for metric in metric_names}\n",
        "\n",
        "#         # Plot each metric's values and store question scores\n",
        "#         for i, (metric, question_scores) in enumerate(zip(metric_names, grouped_values)): \n",
        "\n",
        "#             width = 0.8 / len(question_scores) #Width of each metric's bar\n",
        "#             # Create a bar for each question's score\n",
        "#             for j, val in enumerate(question_scores):\n",
        "#                 plt.bar(i + j * width, val, width=width, color=colors[i], alpha=0.5, label=metric if j == 0 else \"\")\n",
        "#                 # i is the index of metric and determines the base position of a group of bars corresponding to that metric.\n",
        "#                 # j*width adds an offset to the base position to separate individual bars within the same group (metric). \n",
        "#                 # Each j corresponds to a different value in question_scores, creating distinct bars for the values of question_scores for the same metric.\n",
        "#                 # By combining the above two, we get the exact x-position of a specific bar\n",
        "#                 question_scores_by_metric[metric].append((j, val)) # Store question index and score\n",
        "\n",
        "#             #Below for the distribution overlay plot\n",
        "#             counts = Counter(question_scores) #Count the frequency of each score in question_scores (e.g. {4: 1, 3: 2, 2: 2, 1: 1, 0: 1}, where key is score)\n",
        "#             for score, freq in counts.items():\n",
        "#                 if score not in score_metric_counts:\n",
        "#                     score_metric_counts[score] = {}\n",
        "#                 score_metric_counts[score][metric] = freq #Keeps track of how many times each metric gets a specific score over all questions (for one run)\n",
        "#                 # {4: {'completeness': 1, 'confidence': 1, 'factuality': 1, 'judgement': 1}, 3: {'completeness': 1, 'relevance': 2, 'conciseness': 2, ....}\n",
        "\n",
        "#         # Add labels and title\n",
        "#         plt.xlabel('Metrics')\n",
        "#         plt.ylabel('Score')\n",
        "#         plt.title('Scores of each metric for all questions over a single run')\n",
        "#         plt.xticks(np.arange(len(metric_names)) + 0.1, metric_names)\n",
        "#         plt.yticks(range(6))  # Set y-axis ticks to integers from 0 to 5\n",
        "#         plt.figure(fig.number)\n",
        "#         plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_values_all_questions.png')\n",
        "#         plt.close('all')\n",
        "\n",
        "\n",
        "\n",
        "#         # Create new figure for ordered plot\n",
        "#         plt.figure(figsize=(15, 10))\n",
        "        \n",
        "#         # For each metric, plot scores with questions ordered by score\n",
        "#         for i, metric in enumerate(metric_names):\n",
        "#             plt.subplot(len(metric_names), 1, i+1)\n",
        "            \n",
        "#             # Sort questions by score\n",
        "#             sorted_questions = sorted(question_scores_by_metric[metric], key=lambda x: x[1])\n",
        "            \n",
        "#             # Plot bars\n",
        "#             x_pos = range(len(sorted_questions))\n",
        "#             scores = [q[1] for q in sorted_questions]\n",
        "#             question_nums = [q[0] for q in sorted_questions]\n",
        "            \n",
        "#             plt.bar(x_pos, scores, color=colors[i], alpha=0.5)\n",
        "            \n",
        "#             # Add question numbers as labels\n",
        "#             for j, (x, score, q_num) in enumerate(zip(x_pos, scores, question_nums)):\n",
        "#                 if score <= 1:  # Low scores on left\n",
        "#                     plt.text(x, -0.2, f'Q{q_num+1}', rotation=90, ha='right')\n",
        "#                 elif score >= 4:  # High scores on right  \n",
        "#                     plt.text(x, -0.2, f'Q{q_num+1}', rotation=90, ha='right')\n",
        "                    \n",
        "#             plt.ylabel(metric)\n",
        "#             plt.ylim(-0.5, 5.5)\n",
        "#             if i == len(metric_names)-1:\n",
        "#                 plt.xlabel('Questions (ordered by score)')\n",
        "            \n",
        "#         plt.suptitle('Metric scores ordered by value, with question numbers shown for extreme scores')\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_values_ordered.png')\n",
        "#         plt.close('all')\n",
        "\n",
        "\n",
        "\n",
        "#         # Keep track of which metrics we've added to the legend\n",
        "#         legend_added = set()\n",
        "\n",
        "#         # For each score, plot metrics in order of frequency (highest frequency at bottom)\n",
        "#         for score in sorted(score_metric_counts.keys()):\n",
        "#             # Sort metrics by frequency for this score\n",
        "#             sorted_metrics = sorted(score_metric_counts[score].items(), #For a given score, sort by frequency of each metric\n",
        "#                                 key=lambda x: x[1], #Use the frequency (second element of each tuple) as the sorting key\n",
        "#                                 reverse=True)  # highest frequency first\n",
        "#             bottom = 0\n",
        "#             for metric, freq in sorted_metrics:\n",
        "#                 i = metric_names.index(metric)  # get index for color\n",
        "#                 plt.bar(score, freq,\n",
        "#                         width=0.4,\n",
        "#                         color=colors[i],\n",
        "#                         alpha=0.5,\n",
        "#                         label=metric if metric not in legend_added else \"\",\n",
        "#                         bottom=bottom)\n",
        "#                 bottom += freq\n",
        "#                 legend_added.add(metric)\n",
        "\n",
        "#         plt.xlabel('Score')\n",
        "#         plt.ylabel('Frequency')\n",
        "#         plt.title('Accumulated distribution of scores by metric.')# In each score the number of times each metric got that score over all questions is shown\n",
        "#         plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#         plt.xticks(np.arange(1, 6))\n",
        "#         plt.tight_layout()\n",
        "#         plt.figure(fig.number)\n",
        "#         plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions_accumulated.png')\n",
        "#         plt.close('all')\n",
        "\n",
        "#     return summary_stats_all_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_figures_metrics(all_runs_model_metrics, metric_names, model_name, judge_model):\n",
        "    \"\"\"\n",
        "    Creates visualizations and calculates statistics for evaluation metrics across multiple runs.\n",
        "\n",
        "    Args:\n",
        "        all_runs_model_metrics (dict): Nested dictionary containing evaluation metrics for each model and run.\n",
        "            Structure: {model_id: [{metric1_descr_run1: [q1_score, q2_score, ...], \n",
        "                                  metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "                                 {metric1_descr_run2: [q1_score, q2_score, ...],\n",
        "                                  metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "                                 ...num_runs]}\n",
        "            Example: {'model1': [{'completeness_descr_run1': [4.5, 3.0, 4.0], \n",
        "                                'relevance_descr_run1': [3.5, 4.0, 3.0]}, ...,\n",
        "                               {'completeness_descr_run2': [4.0, 3.5, 4.5],\n",
        "                                'relevance_descr_run2': [3.0, 4.5, 3.5], ...},\n",
        "                               ...num_runs]}\n",
        "            Where each inner dictionary represents one run containing scores for each metric across all questions\n",
        "        metric_names (list): Names of metrics to analyze and plot (e.g. ['completeness', 'relevance'])\n",
        "        model_name (str): Name/identifier of the model being evaluated\n",
        "        judge_model (str): Name/identifier of the model used for judging the evaluations\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary statistics for each model, run and metric.\n",
        "            Structure: {model_name: {run_idx: {metric_name: {\n",
        "                'mean': float,\n",
        "                'std_error': float, \n",
        "                'ci_low': float,\n",
        "                'ci_high': float\n",
        "            }}}}\n",
        "            Example: {'anthropic/claude-3-5-sonnet': {\n",
        "                '0': {'completeness': {'mean': 4.5, 'std_error': 0.5, \n",
        "                                     'ci_low': 3.52, 'ci_high': 5.48},\n",
        "                      'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "                                  'ci_low': 2.52, 'ci_high': 4.48} , ...},\n",
        "                '1': {'completeness': {'mean': 4.5, 'std_error': 0.5,\n",
        "                                     'ci_low': 3.52, 'ci_high': 5.48},\n",
        "                      'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "                                  'ci_low': 2.52, 'ci_high': 4.48}, ...},\n",
        "                ...num_runs}}\n",
        "\n",
        "    The function generates several visualization types:\n",
        "    - Individual histograms for each metric showing score distributions\n",
        "    - Error bars indicating means and confidence intervals\n",
        "    - Overlapping bar plots comparing metrics\n",
        "    - Stacked distribution plots showing relative frequencies of scores\n",
        "\n",
        "    All plots are saved as PNG files with names indicating the judge model,\n",
        "    evaluated model, run index, and plot type.\n",
        "    \"\"\"\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    from collections import Counter\n",
        "\n",
        "    def calculate_statistics(values):\n",
        "        \"\"\"Calculate mean, standard error, and confidence intervals.\"\"\"\n",
        "        mean_value = np.mean(values)  # Mean of the metric over single run and over single metric (but over all questions)\n",
        "        std_error = np.std(values, ddof=1) / np.sqrt(len(values))  # ddof=1 to divide by n-1 to calculate the sample sd\n",
        "        \n",
        "        assert np.std(values, ddof=1) == np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), \"Standard deviation calculation mismatch\"\n",
        "        \n",
        "        margin_of_error = 1.96 * std_error  # didn't use t_critical=t.ppf(0.975, df=len(values)-1) since we're using sample standard deviation\n",
        "        return {\n",
        "            'mean': mean_value,\n",
        "            'std_error': std_error,\n",
        "            'ci_low': mean_value - margin_of_error,\n",
        "            'ci_high': mean_value + margin_of_error\n",
        "        }\n",
        "\n",
        "    def plot_metric_distributions(metric_values, axes, colors, bin_edges):\n",
        "        \"\"\"Plot individual metric distributions with error bars.\"\"\"\n",
        "        error_bars = []\n",
        "        run_stats = {}\n",
        "        \n",
        "        for metric_idx, (metric_name, values) in enumerate(metric_values.items()):  # Loop over runs' metric names and values\n",
        "            clean_metric_name = metric_name.replace('_descr', '')  # This is over one run and over one metric (but over all questions)\n",
        "            metric_name = metric_names[metric_idx]\n",
        "            assert clean_metric_name == metric_name, \"Metric name mismatch\"\n",
        "            \n",
        "            stats = calculate_statistics(values)\n",
        "            sns.histplot(values, bins=bin_edges, color=colors[metric_idx], ax=axes[metric_idx], kde=False)\n",
        "            \n",
        "            #Store error bars\n",
        "            if metric_idx == 0:\n",
        "                error_bars = []\n",
        "            error_bars.append((stats['mean'], axes[metric_idx].get_ylim()[1]/2, stats['ci_high'] - stats['mean']))\n",
        "            \n",
        "            run_stats[metric_name] = stats\n",
        "            axes[metric_idx].set_title(f'{metric_name} (Mean: {stats[\"mean\"]:.2f} ± {stats[\"std_error\"]:.2f} SE, CI: {stats[\"ci_low\"]:.2f}-{stats[\"ci_high\"]:.2f})')\n",
        "            axes[metric_idx].set_xlim(0, 5.5)  # Keep 0 in case of errors\n",
        "            axes[metric_idx].set_ylabel('Frequency')\n",
        "            axes[metric_idx].set_xlabel('Values' if metric_idx == len(metric_values)-1 else '')\n",
        "            \n",
        "        return error_bars, run_stats\n",
        "\n",
        "    def plot_question_scores(metric_names, grouped_values, colors):\n",
        "        \"\"\"Plot scores for each question across metrics.\"\"\"\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Define colors for each metric\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "        # First count all frequencies per score (1-5) per metric for one run over all questions\n",
        "        question_scores_by_metric = {metric: [] for metric in metric_names}\n",
        "        score_metric_counts = {}\n",
        "\n",
        "        #Plot each metric's values and store question scores\n",
        "        for i, (metric, question_scores) in enumerate(zip(metric_names, grouped_values)):\n",
        "            width = 0.8 / len(question_scores)  # Width of each metric's bar\n",
        "            \n",
        "            for j, val in enumerate(question_scores): #Create a bar for each question's score\n",
        "                plt.bar(i + j * width, val, width=width, color=colors[i], alpha=0.5, \n",
        "                       label=metric if j == 0 else \"\")\n",
        "                # i is the index of metric and determines the base position of a group of bars corresponding to that metric.\n",
        "                # j*width adds an offset to the base position to separate individual bars within the same group (metric). \n",
        "                # Each j corresponds to a different value in question_scores, creating distinct bars for the values of question_scores for the same metric.\n",
        "                # By combining the above two, we get the exact x-position of a specific bar     \n",
        "                question_scores_by_metric[metric].append((j, val))\n",
        "\n",
        "            counts = Counter(question_scores)  # Count frequency of each score in question_scores (e.g. {4: 1, 3: 2, 2: 2, 1: 1, 0: 1}, where key is score)\n",
        "            for score, freq in counts.items():\n",
        "                if score not in score_metric_counts:\n",
        "                    score_metric_counts[score] = {}\n",
        "                score_metric_counts[score][metric] = freq  #Keeps track of how many times each metric gets a specific score over all questions (for one run)\n",
        "                # {4: {'completeness': 1, 'confidence': 1, 'factuality': 1, 'judgement': 1}, 3: {'completeness': 1, 'relevance': 2, 'conciseness': 2, ....}\n",
        "\n",
        "        return question_scores_by_metric, score_metric_counts\n",
        "\n",
        "    def plot_ordered_scores(metric_names, question_scores_by_metric, colors):\n",
        "        \"\"\"Plot metrics ordered by score values.\"\"\"\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        \n",
        "        for i, metric in enumerate(metric_names):\n",
        "            plt.subplot(len(metric_names), 1, i+1)\n",
        "            sorted_questions = sorted(question_scores_by_metric[metric], key=lambda x: x[1]) #Sort questions by score\n",
        "            \n",
        "            #Plot bars\n",
        "            x_pos = range(len(sorted_questions))\n",
        "            scores = [q[1] for q in sorted_questions]\n",
        "            question_nums = [q[0] for q in sorted_questions]\n",
        "            \n",
        "            plt.bar(x_pos, scores, color=colors[i], alpha=0.5)\n",
        "            \n",
        "            #Add question numbers as labels\n",
        "            for j, (x, score, q_num) in enumerate(zip(x_pos, scores, question_nums)):\n",
        "                if score <= 1 or score >= 4:  # Label extreme scores\n",
        "                    plt.text(x, -0.2, f'Q{q_num+1}', rotation=90, ha='right')\n",
        "                    \n",
        "            plt.ylabel(metric)\n",
        "            plt.ylim(-0.5, 5.5)\n",
        "            if i == len(metric_names)-1:\n",
        "                plt.xlabel('Questions (ordered by score)')\n",
        "\n",
        "    def plot_accumulated_distributions(score_metric_counts, metric_names, colors):\n",
        "        \"\"\"Plot accumulated distribution of scores by metric.\"\"\"\n",
        "        legend_added = set()\n",
        "\n",
        "        #For each score, plot metrics in order of frequency (highest frequency at bottom)\n",
        "        for score in sorted(score_metric_counts.keys()):\n",
        "            #Sort metrics by frequency for this score\n",
        "            sorted_metrics = sorted(score_metric_counts[score].items(),\n",
        "                                key=lambda x: x[1], #Use the frequency (second element of each tuple) as the sorting key\n",
        "                                reverse=True)  # highest frequency first\n",
        "            bottom = 0\n",
        "            for metric, freq in sorted_metrics:\n",
        "                i = metric_names.index(metric) #get index for color\n",
        "                plt.bar(score, freq,\n",
        "                       width=0.4,\n",
        "                       color=colors[i],\n",
        "                       alpha=0.5,\n",
        "                       label=metric if metric not in legend_added else \"\",\n",
        "                       bottom=bottom)\n",
        "                bottom += freq\n",
        "                legend_added.add(metric)\n",
        "\n",
        "    summary_stats_all_runs = {}  # Keep track of summary statistics over all runs\n",
        "\n",
        "    for run_idx, metric_values_run in enumerate(all_runs_model_metrics[model_name]): #Loop over runs\n",
        "\n",
        "        colors = sns.color_palette(\"Set3\", len(metric_names))\n",
        "        \n",
        "        # Create two figures - one with separate subplots and one overlaid\n",
        "        fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 18))\n",
        "        plt.subplots_adjust(hspace=0.6, top=0.94)\n",
        "        fig.suptitle(f'Metric Distributions for {model_name} (Run {run_idx})', fontsize=16)\n",
        "        \n",
        "        bin_edges = np.arange(0.0, 5.6, 0.2)  # Bins for range 1-5\n",
        "        metric_names = [name.replace('_descr', '') for name in metric_values_run]\n",
        "        \n",
        "        error_bars, run_stats = plot_metric_distributions(metric_values_run, axes, colors, bin_edges)\n",
        "        \n",
        "        # Save version without error bars\n",
        "        plt.figure(fig.number)\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions_no_error_bars.png\")\n",
        "        \n",
        "        # Add error bars and save updated version\n",
        "        for i, (mean, ylim, margin) in enumerate(error_bars):\n",
        "            axes[i].errorbar(mean, ylim, xerr=margin, color='black', capsize=5, \n",
        "                           capthick=1, elinewidth=2, marker='o')\n",
        "        \n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(f\"\\nSummary Statistics over run {run_idx}:\")\n",
        "        print(\"-\" * 50)\n",
        "        for metric, stats in run_stats.items():\n",
        "            print(f\"{metric}:\")\n",
        "            for key, value in stats.items():\n",
        "                print(f\"  {key}: {value:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        summary_stats_all_runs[run_idx] = run_stats #For one run\n",
        "\n",
        "        grouped_values=list(metric_values_run.values()) #Values of all metrics for one run over all questions. There are num_metrics lists in that list. \n",
        "        values = [val for sublist in grouped_values for val in sublist] #Flatten the list - Size is num_questions*num_metrics (1st metric questions, 2nd metric questions, etc)\n",
        "        \n",
        "        question_scores_by_metric, score_metric_counts = plot_question_scores(metric_names, grouped_values, colors)\n",
        "        \n",
        "        plt.xlabel('Metrics')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Scores of each metric for all questions over a single run')\n",
        "        plt.xticks(np.arange(len(metric_names)) + 0.1, metric_names)\n",
        "        plt.yticks(range(6)) #Set y-ticks to 0-5\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_values_all_questions.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Plot ordered scores\n",
        "        plot_ordered_scores(metric_names, question_scores_by_metric, colors)\n",
        "        plt.suptitle('Metric scores ordered by value, with question numbers shown for extreme scores')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_values_ordered.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Plot accumulated distributions\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plot_accumulated_distributions(score_metric_counts, metric_names, colors)\n",
        "        \n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Accumulated distribution of scores by metric.') #In each score the number of times each metric got that score over all questions\n",
        "        plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.xticks(np.arange(1, 6))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{judge_model.split('/')[1]}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions_accumulated.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "    return summary_stats_all_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform the Evaluation over all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "def load_model_stats(judge_model): #In case we had to restart the loop - some models didn't run - Keep track of all model stats\n",
        "    \"\"\"Load existing stats from files or initialize empty dictionaries.\"\"\"\n",
        "    try:\n",
        "        with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "            all_models_stats = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_models_stats = {}  # Used in comparison between models\n",
        "\n",
        "    try:\n",
        "        with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "            all_runs_model_metrics = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_runs_model_metrics = {}  # Used in plotting metrics\n",
        "        \n",
        "    return all_models_stats, all_runs_model_metrics\n",
        "\n",
        "def perform_evaluation(model_id, judge_model, n_resamples, example_inputs, factor_evaluator):\n",
        "    \"\"\"Perform evaluation runs and collect results.\"\"\"\n",
        "    dataset_name = get_dataset_name(model_id, judge_model) #How the dataset will be named in Langsmith\n",
        "    dataset_langsmith = create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key)\n",
        "    # print(\"dataset_langsmith\",dataset_langsmith)\n",
        "    # print(\"dataset_langsmith_id\",dataset_langsmith.id)\n",
        "    # # Initialize client with your API key\n",
        "    # client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    # # Get all examples from the dataset\n",
        "    # examples_test = client.list_examples(dataset_id=dataset_langsmith.id)\n",
        "    # # print(\"examples_test\",[x for x in examples_test])\n",
        "    # # print(\"len examples_test\",len(examples_test)) #error of type generator no attribute len\n",
        "    # examples_tests=[x for x in examples_test]\n",
        "    # print(\"examples_test\",examples_tests)\n",
        "    # print(\"len examples_tests\",len(examples_tests))\n",
        "    # print(type(examples_tests))\n",
        "    # print(\"examples_tests[0]\",examples_tests[0])\n",
        "    # print(type(examples_tests[0]))\n",
        "    # print(\"examples_tests[0].inputs\",examples_tests[0].inputs)\n",
        "    # print(\"examples_tests[0].outputs\",examples_tests[0].outputs)\n",
        "    # print(\"examples_tests[0].inputs['question']\",examples_tests[0].inputs['question'])\n",
        "    # print(\"examples_tests[0].outputs['answer']\",examples_tests[0].outputs['answer'])\n",
        "\n",
        "    # # try:\n",
        "    # print(\"[x.inputs for x in examples_tests])\",[x.inputs for x in examples_tests])\n",
        "    # print(\"[x.inputs['question'] for x in examples_tests])\",[x.inputs['question'] for x in examples_tests])\n",
        "    # print(\"[x.outputs['answer'] for x in examples_tests])\",[x.outputs['answer'] for x in examples_tests])\n",
        "    # print(\"len([x.inputs['question'] for x in examples_tests])\",len([x.inputs['question'] for x in examples_tests]))\n",
        "    # print(\"len([x.outputs['answer'] for x in examples_tests])\",len([x.outputs['answer'] for x in examples_tests]))\n",
        "\n",
        "    # # except:\n",
        "    # #     print(\"examples_tests.example\",examples_tests.example)\n",
        "    # # print(\"dataset_langsmith_examples\",dataset_langsmith.list_examples())\n",
        "    # # client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    # # Get all examples from the dataset\n",
        "    # # examples_test = client.list_examples(dataset_id=dataset_langsmith.id)\n",
        "    # # print(\"examples_test\",[x for x in examples_test])\n",
        "    # # print(\"list_of_questions\",list_of_questions)\n",
        "    # print(\"List of question new\", [x['example'].inputs['question'] for x in examples_test])\n",
        "    # # print(\"list_of_answers\",list_of_answers)\n",
        "    # print(\"List of answers new\", [x['example'].outputs['answer'] for x in examples_test])\n",
        "    # # with open('dataset_langsmith.txt', 'w') as f:\n",
        "    # #     f.write(str(dataset_langsmith))\n",
        "    # # print(\"Example inputs\",example_inputs)\n",
        "    # # with open('example_inputs.txt', 'w') as f:\n",
        "    # #     f.write(str(example_inputs))\n",
        "\n",
        "    evaluation_all_resamples = [] #Used below to obtain the unique questions/answers and also the results of each resample\n",
        "    \n",
        "    begin = time.time()\n",
        "    for resample_idx in range(n_resamples):\n",
        "        print(f\"\\nPerforming evaluation of resample {resample_idx+1}/{n_resamples} of {model_id}\")\n",
        "        evaluation_results = evaluate(\n",
        "            predict, #Function that call our LLM and returns its output\n",
        "            data=dataset_langsmith.name, #Just using dataset_langsmith doesn't work \n",
        "            evaluators=[factor_evaluator], #Evaluators to use\n",
        "            max_concurrency=1, #Run questions in parallel (if not 1)\n",
        "            # metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
        "            experiment_prefix=str(judge_model)+'_judge_with_'+str(model_id)+'_resample_'+str(resample_idx) # A prefix for your experiment names to easily identify them\n",
        "        )\n",
        "        evaluation_all_resamples.extend(evaluation_results) #Used below to get unique questions/answers and to select the predicted answers\n",
        "        #This was n_resamples*num_questions elements, for just one model\n",
        "        #We didn't use dataset_langsmith to extract q and a pairs since not the same order during inference\n",
        "    # print(\"evaluation_all_resamples\",evaluation_all_resamples)\n",
        "    with open('evaluation_all_resamples_'+str(model_id.split('/')[1])+'_'+str(judge_model.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(evaluation_all_resamples))\n",
        "    \n",
        "    print(f\"Total time taken: {time.time() - begin}\")\n",
        "    return evaluation_all_resamples, dataset_langsmith\n",
        "\n",
        "def process_evaluation_results(langsmith_api_key, dataset_langsmith): #evaluation_all_resamples, chunk_size, \n",
        "    \"\"\"Extract questions and answers from evaluation results.\"\"\"\n",
        "    #https://docs.smith.langchain.com/tutorials/Developers/evaluation\n",
        "    \n",
        "    # # Get unique questions/answers (take only first resample since they're repeated)\n",
        "    # unique_results = evaluation_all_resamples[:chunk_size] #Includes the questions and actual answers of one resample only (same for the others)\n",
        "    # list_of_questions = [x['example'].inputs['question'] for x in unique_results]\n",
        "    # list_of_answers = [x['example'].outputs['answer'] for x in unique_results]\n",
        "\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "    # Get all examples from the dataset\n",
        "    # examples_test = client.list_examples(dataset_id=dataset_langsmith.id)\n",
        "\n",
        "    questions_answers=[x for x in client.list_examples(dataset_id=dataset_langsmith.id)]\n",
        "    list_of_questions=[x.inputs['question'] for x in questions_answers]\n",
        "    list_of_answers=[x.outputs['answer'] for x in questions_answers]\n",
        "\n",
        "    # # print(\"examples_test\",[x for x in examples_test])\n",
        "    print(\"list_of_questions\",list_of_questions)\n",
        "    # print(\"List of question new\", new_list_of_questions)\n",
        "    print(\"list_of_answers\",list_of_answers)\n",
        "    # print(\"List of answers new\", new_list_of_answers)\n",
        "    \n",
        "    # print(\"unique_results\",unique_results)\n",
        "    # assert list_of_questions == new_list_of_questions, \"list_of_questions and new_list_of_questions are not the same\"\n",
        "    # assert list_of_answers == new_list_of_answers, \"list_of_answers and new_list_of_answers are not the same\"\n",
        "    # # print(\"list_of_questions\",list_of_questions)\n",
        "    # # print(\"list_of_answers\",list_of_answers)\n",
        "\n",
        "\n",
        "    # # Save lists to text files\n",
        "    # with open('unique_results.txt', 'w') as f:\n",
        "    #     f.write(str(unique_results))\n",
        "        \n",
        "    with open('list_of_questions.txt', 'w') as f:\n",
        "        f.write(str(list_of_questions))\n",
        "        \n",
        "    with open('list_of_answers.txt', 'w') as f:\n",
        "        f.write(str(list_of_answers))\n",
        "    \n",
        "    results_df = pd.DataFrame({\n",
        "        'questions': list_of_questions,\n",
        "        'answers': list_of_answers\n",
        "    })\n",
        "    return results_df, list_of_questions\n",
        "\n",
        "def process_metrics(resample_results, list_of_metrics, resample_idx, results_df, model_name):\n",
        "    \"\"\"\n",
        "    Process metrics for a single resample and update results DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        resample_results: Results from current resample\n",
        "        list_of_metrics: List of metrics to process\n",
        "        resample_idx: Current resample index\n",
        "        results_df: DataFrame to update with metrics\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (individual_run_metric_scores, metrics)\n",
        "    \"\"\"\n",
        "    # A list with in the format: [EvaluationResult(key='completeness', score=4, value='To evaluate the .... - It has num_questions lists, each with num_metrics values\n",
        "    \n",
        "    # metrics = ([0] if not resample_results else \n",
        "    #           [x['evaluation_results']['results'] if x['run'].outputs['output'] is not None else 0 \n",
        "    #            for x in resample_results])\n",
        "    metrics = []\n",
        "    for result in resample_results:\n",
        "        if result['run'].outputs['output'] is None or not result['evaluation_results']['results']: #JUST ADDED SECOND CONDITION\n",
        "            metrics.append(0)  # Use 0 to indicate failed evaluation\n",
        "        else:\n",
        "            metrics.append(result['evaluation_results']['results'])\n",
        "\n",
        "    print(\"resample_results\",resample_results)\n",
        "    print(\"type(resample_results)\",type(resample_results))\n",
        "    with open('resample_results_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(resample_results))\n",
        "    \n",
        "    print(\"metrics\"+str(resample_idx)+\"_\"+str(model_name.split('/')[1]),metrics)\n",
        "    with open('metrics_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(metrics))\n",
        "    \n",
        "    individual_run_metric_scores = {} #Keep track of scores of all metrics over all questions for one resample\n",
        "    for metric_idx, metric_name in enumerate(list_of_metrics): #Get specific metric name and values over all questions for the current resample\n",
        "        #Get all metric keys for the current resample over all questions, handling potential missing keys (values set to 0 above due to errors)\n",
        "        clean_metric_names, metric_scores, metric_prompts = [], [], []\n",
        "        \n",
        "        for m in metrics:\n",
        "            if m == 0:\n",
        "                key = metric_name.replace('_descr','')\n",
        "                score = 0\n",
        "                prompt=\"\"\n",
        "            else:\n",
        "                try:\n",
        "                    key = m[metric_idx].key\n",
        "                    score = m[metric_idx].score ##Scores of a given metric over all questions for a given resample\n",
        "                    prompt = m[metric_idx].value\n",
        "                except:\n",
        "                    key = metric_name.replace('_descr','')\n",
        "                    score = 0\n",
        "                    prompt = \" \"\n",
        "                \n",
        "            clean_metric_names.append(key)\n",
        "            metric_scores.append(score)\n",
        "            metric_prompts.append(prompt)\n",
        "            \n",
        "        assert all(name == metric_name.replace('_descr','') for name in clean_metric_names), \"Metric keys mismatch\"\n",
        "        # print(\"clean_metric_names\",clean_metric_names)\n",
        "        # print(\"metric_scores\",metric_scores)\n",
        "        # print(\"metric_prompts\",metric_prompts)\n",
        "        \n",
        "        with open('clean_metric_names_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(clean_metric_names))\n",
        "            \n",
        "        with open('metric_scores_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(metric_scores))\n",
        "            \n",
        "        with open('metric_prompts_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(metric_prompts))\n",
        "\n",
        "        # Update results DataFrame\n",
        "        clean_metric_name = clean_metric_names[0]\n",
        "        results_df[f'metric_{clean_metric_name}_{resample_idx+1}'] = metric_scores\n",
        "        results_df[f'prompt_{clean_metric_name}_{resample_idx+1}'] = metric_prompts\n",
        "        \n",
        "        # Store scores for return\n",
        "        individual_run_metric_scores[metric_name] = metric_scores #len is num_metrics\n",
        "    # print(\"individual_run_metric_scores\",individual_run_metric_scores)\n",
        "    print(\"resample_idx\",resample_idx)\n",
        "    \n",
        "    with open('individual_run_metric_scores_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(individual_run_metric_scores))\n",
        "\n",
        "        \n",
        "    return individual_run_metric_scores, metrics\n",
        "\n",
        "def calculate_metric_statistics(all_runs_metric_scores, list_of_metrics, num_questions, model_name): #Reduce variance (step 3.1)\n",
        "    \"\"\"Calculate statistical metrics across resamples.\"\"\"\n",
        "    metric_stats_resampling = {} # Calculate mean and standard error for each metric and question across K resamples\n",
        "    \n",
        "    for metric in list_of_metrics:\n",
        "        metric_stats_resampling[metric] = {\n",
        "            'means': [],  # Mean score across K resamples for each question\n",
        "            'standard_errors': [],  # Standard error of the mean for each question\n",
        "            'conditional_vars': []  # Conditional variance reduced by factor of K\n",
        "        }\n",
        "        \n",
        "        # For each question\n",
        "        for q in range(num_questions):\n",
        "            # Get K scores for this metric/question across all resamples\n",
        "            scores = [run[metric][q] for run in all_runs_metric_scores]\n",
        "            K = len(scores)  # Number of resamples\n",
        "            \n",
        "            # Calculate statistics\n",
        "            mean = np.mean(scores)\n",
        "            var = np.var(scores)\n",
        "            # Calculate conditional variance reduced by factor of K\n",
        "            # Var(mean) = σ²/K where σ² is the variance of individual scores\n",
        "            conditional_var = var / K if K > 0 else 0\n",
        "            standard_error = np.sqrt(conditional_var)\n",
        "            \n",
        "            # Store results\n",
        "            metric_stats_resampling[metric]['means'].append(mean)\n",
        "            metric_stats_resampling[metric]['standard_errors'].append(standard_error)\n",
        "            metric_stats_resampling[metric]['conditional_vars'].append(conditional_var)\n",
        "    \n",
        "    # print(\"metric_stats_resampling\",metric_stats_resampling)\n",
        "\n",
        "    with open('metric_stats_resampling_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "        f.write(str(metric_stats_resampling))\n",
        "        \n",
        "    return metric_stats_resampling\n",
        "\n",
        "def handle_zero_values(results_df, n_resamples, list_of_metrics):\n",
        "    \"\"\"\n",
        "    Handle zero values in results.\n",
        "    \n",
        "    Args:\n",
        "        results_df (pd.DataFrame): DataFrame containing results\n",
        "        n_resamples (int): Number of resamples\n",
        "        list_of_metrics (list): List of metrics to check\n",
        "        \n",
        "    Returns:\n",
        "        list: Indices of rows containing zero values\n",
        "    \"\"\"\n",
        "    zero_rows = []\n",
        "    \n",
        "    try:\n",
        "        # Handle 0 values across all resamples - These are errors\n",
        "        for resample_idx in range(n_resamples):\n",
        "            for metric in list_of_metrics:\n",
        "                try:\n",
        "                    simple_metric_name = metric.replace('_descr','')\n",
        "                    metric_col = f'metric_{simple_metric_name}_{resample_idx+1}'\n",
        "                    \n",
        "                    # Check if column exists\n",
        "                    if metric_col not in results_df.columns:\n",
        "                        print(colored(f\"Warning: Column {metric_col} not found in DataFrame\", 'yellow'))\n",
        "                        continue\n",
        "                    \n",
        "                    zero_indices = results_df[metric_col] == 0 #series with True/False\n",
        "                    \n",
        "                    if zero_indices.any():\n",
        "                        for idx in zero_indices[zero_indices].index: #Loop over True indices\n",
        "                            try:\n",
        "                                print(colored(\n",
        "                                    f\"Missing value for metric '{simple_metric_name}' \"\n",
        "                                    f\"in resample {resample_idx+1}\", 'red'))\n",
        "                                print(colored(\n",
        "                                    f\"Question: {results_df.loc[idx, 'questions']}\", 'green'))\n",
        "                                zero_rows.append(idx) #Keep track of rows with a zero value anywhere (any metric, resample)\n",
        "                            except KeyError as ke:\n",
        "                                print(colored(\n",
        "                                    f\"Error accessing row {idx} or 'questions' column: {ke}\", \n",
        "                                    'red'))\n",
        "                            except Exception as e:\n",
        "                                print(colored(\n",
        "                                    f\"Unexpected error processing zero value at row {idx}: {e}\", \n",
        "                                    'red'))\n",
        "                \n",
        "                except Exception as e:\n",
        "                    print(colored(\n",
        "                        f\"Error processing metric {metric} in resample {resample_idx}: {e}\", \n",
        "                        'red'))\n",
        "        \n",
        "        return list(set(zero_rows))  # Return unique indices\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(colored(f\"Critical error in handle_zero_values: {e}\", 'red'))\n",
        "        traceback.print_exc()\n",
        "        return []  # Return empty list in case of critical error\n",
        "\n",
        "def process_zero_values(results_df, zero_rows, list_of_metrics):\n",
        "    \"\"\"Process and optionally replace zero values in results.\"\"\"\n",
        "    #Do not drop the above rows since only some are missing and not all (we have n_resamples answers for each row). \n",
        "    metric_cols = [col for col in results_df.columns if col.startswith('metric_')]\n",
        "    print(\"metric_cols\",metric_cols)\n",
        "    for row_idx in zero_rows:\n",
        "        for metric_name in list_of_metrics:\n",
        "            # Get all columns for this metric across resamples\n",
        "            metric_resample_cols = [col for col in metric_cols \n",
        "                                  if metric_name.replace('_descr','') in col]\n",
        "            # print(\"metric_resample_cols\",metric_resample_cols)\n",
        "            \n",
        "            with open('metric_resample_cols.txt', 'w') as f:\n",
        "                f.write(str(metric_resample_cols))\n",
        "            \n",
        "            # Get values for this metric across all resamples for this row\n",
        "            values = results_df.loc[row_idx, metric_resample_cols]\n",
        "            # print(\"values\",values)\n",
        "            \n",
        "            with open('values.txt', 'w') as f:\n",
        "                f.write(str(values))\n",
        "            \n",
        "            try:\n",
        "                values = values.values\n",
        "                # print(\"values_values\",values)\n",
        "\n",
        "                with open('values_values.txt', 'w') as f:\n",
        "                    f.write(str(values))\n",
        "            except:\n",
        "                print(f\"Warning: Unexpected values type: {type(values)}\")\n",
        "\n",
        "                with open('values_type.txt', 'w') as f:\n",
        "                    f.write(str(type(values)))\n",
        "\n",
        "                continue\n",
        "                \n",
        "            # Handle zero values\n",
        "            # If any values are 0, replace with mean of non-zero values -DIDN'T DO IT YET - HAS TO BE DONE IN ALL VARIABLES, NOT ONLY IN DF\n",
        "            if (values == 0).any():\n",
        "                non_zero_values = values[values != 0]\n",
        "                # print(\"non_zero_values\",non_zero_values)\n",
        "\n",
        "                with open('non_zero_values.txt', 'w') as f:\n",
        "                    f.write(str(non_zero_values))\n",
        "\n",
        "                if len(non_zero_values) > 0:\n",
        "                    mean_value = non_zero_values.mean()\n",
        "                    for col in metric_resample_cols:\n",
        "                        if results_df.loc[row_idx, col] == 0:\n",
        "                            print(colored(\n",
        "                                f\"0 value in row {row_idx}, column {col} \"\n",
        "                                f\"with mean {mean_value:.2f}\", 'yellow'))\n",
        "                            # Uncomment to actually replace values:\n",
        "                            # results_df.at[row_idx, col] = round(mean_value, 1)\n",
        "\n",
        "def reorganize_metrics(all_resamples_metrics, list_of_metrics):\n",
        "    \"\"\"Reorganize metrics by type and calculate statistics.\"\"\"\n",
        "    metric_stats = {metric.replace('_descr', ''): [] for metric in list_of_metrics}\n",
        "    \n",
        "    for metric_name in list_of_metrics: #If calculating statistics works, this works too\n",
        "        clean_name = metric_name.replace('_descr', '')\n",
        "        \n",
        "        #Each resample_metrics (num_resamples in total) has a list of num_questions lists, each having num_metrics values\n",
        "        #format of each sublist: [EvaluationResult(key='completeness', score=4, value='To evaluate the ...\n",
        "        for resample_idx, resample_metrics in enumerate(all_resamples_metrics):\n",
        "            # print(\"resample_metrics\",resample_metrics) #CONFIRM THAT ORDER IS CORRECT HERE\n",
        "\n",
        "            with open('resample_metrics_'+str(resample_idx)+'_'+str(metric_name)+'.txt', 'w') as f:\n",
        "                f.write(str(resample_metrics))\n",
        "\n",
        "            metric_idx = list_of_metrics.index(metric_name)\n",
        "            print(\"metric_idx\",metric_idx)\n",
        "\n",
        "            scores = [m[metric_idx].score if m!=0 and m!=[] else 0 \n",
        "                     for m in resample_metrics]\n",
        "            # print(\"scores\",scores)\n",
        "\n",
        "            with open('scores_'+str(resample_idx)+'_'+str(metric_name)+'.txt', 'w') as f:\n",
        "                f.write(str(scores))\n",
        "\n",
        "            metric_stats[clean_name].extend(scores)\n",
        "            # print(\"metric_stats\",metric_stats)\n",
        "\n",
        "            with open('metric_stats_reorganized_'+str(resample_idx)+'_'+str(metric_name)+'.txt', 'w') as f:\n",
        "                f.write(str(metric_stats))\n",
        "\n",
        "    return metric_stats\n",
        "\n",
        "def save_results(results_df, judge_model, model_id, stage=\"before\"):\n",
        "    \"\"\"Save results DataFrame to Excel.\"\"\"\n",
        "    filename = (f\"results_{judge_model.split('/')[1]}_judge_with_\"\n",
        "               f\"{model_id.replace('/','_')}_{stage}_nan_replacement.xlsx\")\n",
        "    results_df.to_excel(filename, index=False)\n",
        "\n",
        "# Main execution loop\n",
        "# def main():\n",
        "all_models_stats, all_runs_model_metrics = load_model_stats(judge_model)\n",
        "\n",
        "for model_id in models:\n",
        "    global model_name, model, tokenizer, pipeline\n",
        "    model, tokenizer, pipeline = get_model(model_id)\n",
        "    model_name = model_id #Since model_name defined as global variable\n",
        "    \n",
        "    try: #Sometimes some errors with the evaluation (missing questions/answers)\n",
        "        evaluation_all_resamples, dataset_langsmith = perform_evaluation(model_id, judge_model, n_resamples, example_inputs, factor_evaluator)\n",
        "        chunk_size = len(example_inputs) #Number of questions\n",
        "        results_df, list_of_questions = process_evaluation_results(langsmith_api_key, dataset_langsmith) #evaluation_all_resamples, chunk_size, \n",
        "        \n",
        "        all_resamples_metrics = [] #Keep track of all metrics over all resamples and all questions\n",
        "        #There will be n_resamples lists, each with num_questions*num_metrics sublists \n",
        "        #Each question will have 6 metric values like this: [EvaluationResult(key='completeness', score=4, value='To evaluate the ....\n",
        "        all_runs_metric_scores = [] #This will be appended to the input that plots metrics at the end. \n",
        "        #The format of it is [{metric1_descr_run1: [q1_score, q2_score, ...], metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "        #                     {metric1_descr_run2: [q1_score, q2_score, ...], metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "        #                     ...num_runs]\n",
        "        \n",
        "        # Process each resample\n",
        "        for resample_idx in range(n_resamples):\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            print(\"resample_results\",resample_results)\n",
        "\n",
        "            with open('resample_results_main_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "                f.write(str(resample_results))\n",
        "\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results]\n",
        "            print(\"predicted_answers\",predicted_answers)\n",
        "            print(\"len(predicted_answers)\",len(predicted_answers))\n",
        "            assert len(predicted_answers)==chunk_size, \"Number of predicted answers does not match the number of questions\"\n",
        "\n",
        "            with open('predicted_answers_main_'+str(resample_idx)+'_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "                f.write(str(predicted_answers))\n",
        "\n",
        "            #Add predicted answers to df\n",
        "            results_df[f'predicted_answer_{resample_idx+1}'] = predicted_answers\n",
        "#WEA ARE HERE!! COMPARE RESAMPLE RESULTS OUTPUT WITH METRICS IN FUNCTION BELOW!            \n",
        "            individual_run_metric_scores, metrics = process_metrics(\n",
        "                    resample_results, \n",
        "                    list_of_metrics, \n",
        "                    resample_idx,\n",
        "                    results_df,  # Pass the DataFrame to update\n",
        "                    model_name\n",
        "                )                \n",
        "            \n",
        "            #In each iteration we append the metrics (6 in total) of one resample for all questions - n at the end, one for each resample\n",
        "            all_resamples_metrics.append(metrics)\n",
        "\n",
        "            #Has n_resamples lists, each with num_metrics sublists (each sublist has scores over all questions of one metric) \n",
        "            all_runs_metric_scores.append(individual_run_metric_scores)\n",
        "        \n",
        "        \n",
        "\n",
        "        # print(\"all_runs_metric_scores\",all_runs_metric_scores)\n",
        "        # print(\"all_resamples_metrics\",all_resamples_metrics)\n",
        "\n",
        "        with open('all_runs_metric_scores_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(all_runs_metric_scores))\n",
        "\n",
        "        with open('all_resamples_metrics_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(all_resamples_metrics))\n",
        "\n",
        "        # Calculate statistics\n",
        "        metric_stats_resampling = calculate_metric_statistics(\n",
        "            all_runs_metric_scores, \n",
        "            list_of_metrics, \n",
        "            len(list_of_questions),\n",
        "            model_name\n",
        "        )\n",
        "        \n",
        "        # Save initial results\n",
        "        save_results(results_df, judge_model, model_id, \"before\")\n",
        "        \n",
        "        # Handle zero values\n",
        "        zero_rows = handle_zero_values(results_df, n_resamples, list_of_metrics)\n",
        "        if zero_rows:\n",
        "            print(colored(\n",
        "                f\"ERROR: Found missing values in {len(zero_rows)} rows \"\n",
        "                f\"out of {len(results_df)}\", 'red'))\n",
        "            process_zero_values(results_df, zero_rows, list_of_metrics)\n",
        "        \n",
        "        # Reorganize metrics\n",
        "        metric_stats = reorganize_metrics(all_resamples_metrics, list_of_metrics)\n",
        "        # print(\"metric_stats_final\",metric_stats)\n",
        "\n",
        "        with open('metric_stats_final_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(metric_stats))\n",
        "\n",
        "        metric_names = list(metric_stats.keys()) #Final list of metrics for plotting\n",
        "        \n",
        "        # Verify metric names\n",
        "        metrics_names_loop = [metric.replace('_descr','') for metric in list_of_metrics]\n",
        "        assert metrics_names_loop == metric_names, \"Metric names mismatch\"\n",
        "        \n",
        "                    \n",
        "        # Save results\n",
        "        all_runs_model_metrics[model_id] = all_runs_metric_scores #Used in plotting metrics\n",
        "        #Dictionary in format {model_id:[{metric_1_run_1:[values], metric_2_run_1:[values], ...}, {metric_1_run_2:[values]....}]\n",
        "\n",
        "        all_models_stats[model_id] = plot_figures_metrics(\n",
        "            all_runs_model_metrics,\n",
        "            metric_names,\n",
        "            model_id,\n",
        "            judge_model\n",
        "        )\n",
        "        # print(\"all_models_stats\",all_models_stats)\n",
        "\n",
        "        with open('all_models_stats_main_'+str(model_name.split('/')[1])+'.txt', 'w') as f:\n",
        "            f.write(str(all_models_stats))\n",
        "        \n",
        "        # Save to files\n",
        "        with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "            json.dump(all_models_stats, f, indent=4)\n",
        "        with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "            json.dump(all_runs_model_metrics, f, indent=4)\n",
        "\n",
        "        print(\"Model\",model_id,\"saved\")\n",
        "        print(\"Models saved so far:\",list(all_models_stats.keys()))\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in plotting metrics\")\n",
        "        print(\"Error:\", e)\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    finally:\n",
        "        # Clear VRAM\n",
        "        del model, tokenizer, pipeline\n",
        "        torch.cuda.empty_cache()\n",
        "        print('-'*100)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aaaaaa in evaluator max_concurrency=4?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aaaaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "#In case we had to restart the loop - some models didn't run - Keep track of all model stats \n",
        "try:\n",
        "    with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "        all_models_stats = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    all_models_stats = {} #Used in comparison between models\n",
        "\n",
        "try:\n",
        "    with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "        all_runs_model_metrics = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    all_runs_model_metrics = {} #Used in plotting metrics\n",
        "\n",
        "#Initialize models\n",
        "for model_id in models:\n",
        "    \n",
        "    dataset_name=get_dataset_name(model_id, judge_model) #How the dataset will be named in Langsmith\n",
        "    dataset_langsmith=create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key)\n",
        "    model, tokenizer, pipeline = get_model(model_id)\n",
        "    model_name=model_id #Since model_name defined as global variable\n",
        "\n",
        "    # Evaluation\n",
        "    begin=time.time()\n",
        "    evaluation_all_resamples=[] #Used below to obtain the unique questions/answers and also the results of each resample\n",
        "\n",
        "    for resample_idx in range(n_resamples): \n",
        "        print(f\"\\nPerforming evaluation of resample {resample_idx+1}/{n_resamples} of {model_id}\")\n",
        "   \n",
        "        evaluation_results=evaluate(\n",
        "            predict, #Function that call our LLM and returns its output\n",
        "            data=dataset_langsmith.name, #Just using dataset_langsmith doesn't work \n",
        "            evaluators=[factor_evaluator], #Evaluators to use\n",
        "            # metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
        "            experiment_prefix=str(judge_model)+'_judge_with_'+str(model_id)+'_resample_'+str(resample_idx) # A prefix for your experiment names to easily identify them\n",
        "        )\n",
        "\n",
        "        evaluation_all_resamples.extend(evaluation_results) #Used below to get unique questions/answers and to select the predicted answers\n",
        "        #This was n_resamples*num_questions elements, for just one model\n",
        "        #We didn't use dataset_langsmith to extract q and a pairs since not the same order during inference\n",
        "    print(\"evaluation_all_resamples\",evaluation_all_resamples)\n",
        "\n",
        "    end=time.time()\n",
        "    print(\"Total time taken:\",end-begin)\n",
        "\n",
        "    chunk_size = len(example_inputs) #Number of questions\n",
        "\n",
        "    try: #Sometimes some errors with 1+ Q&A missing\n",
        "\n",
        "        #Extract metrics and save to df\n",
        "        results_df=pd.DataFrame() #Initialize empty df to be filled with results\n",
        "\n",
        "        #https://docs.smith.langchain.com/tutorials/Developers/evaluation\n",
        "        # Get unique questions/answers (take only first resample since they're repeated)\n",
        "        unique_results = evaluation_all_resamples[:chunk_size] #Includes the questions and actual answers of one resample only (same for the others)\n",
        "        list_of_questions = [x['example'].inputs['question'] for x in unique_results]\n",
        "        list_of_answers = [x['example'].outputs['answer'] for x in unique_results]\n",
        "\n",
        "        # Add base columns\n",
        "        results_df['questions'] = list_of_questions\n",
        "        results_df['answers'] = list_of_answers\n",
        "\n",
        "\n",
        "        all_resamples_metrics=[] #Keep track of all metrics over all resamples and all questions\n",
        "        #There will be n_resamples lists, each with num_questions*num_metrics sublists \n",
        "        #Each question will have 6 metric values like this: [EvaluationResult(key='completeness', score=4, value='To evaluate the ...\n",
        "        all_runs_metric_scores=[] #This will be appended to the input that plots metrics at the end. \n",
        "        #The format of it is [{metric1_descr_run1: [q1_score, q2_score, ...], metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "        #                     {metric1_descr_run2: [q1_score, q2_score, ...], metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "        #                     ...num_runs]\n",
        "\n",
        "        # Create columns for each resample's predicted answers and metrics\n",
        "        for resample_idx in range(n_resamples):\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            print(\"resample_results\",resample_results)\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results]\n",
        "            \n",
        "            # print(\"predicted_answers_init\",predicted_answers_init)\n",
        "            # predicted_answers = [x['run'].outputs['output'][0].content[0].text for x in resample_results]\n",
        "            print(\"predicted_answers_new\",predicted_answers)\n",
        "            # Add predicted answers and metrics side by side\n",
        "            results_df[f'predicted_answer_{resample_idx+1}'] = predicted_answers\n",
        "\n",
        "            # metrics = [x['evaluation_results']['results'] if x['run'].outputs['output'] is not None else 0 for x in resample_results] #0 to handle errors\n",
        "            metrics=[0] if not resample_results else [x['evaluation_results']['results'] if x['run'].outputs['output'] is not None else 0 for x in resample_results]\n",
        "            #A list with in the format: [EvaluationResult(key='completeness', score=4, value='To evaluate the .... - It has num_questions lists, each with num_metrics values\n",
        "            print(\"metrics\",metrics)\n",
        "            \n",
        "            all_resamples_metrics.append(metrics) #In each iteration we append the metrics (6 in total) of one resample for all questions - n at the end, one for each resample\n",
        "\n",
        "            individual_run_metric_scores={} #Keep track of scores of all metrics over all questions, for one resample\n",
        "\n",
        "            # Add metrics and evaluation prompts with their corresponding predicted answers\n",
        "            for metric_idx, metric_name in enumerate(list_of_metrics): #Get specific metric name and values over all questions for the current resample\n",
        "                # clean_metric_names = [m[metric_idx].key for m in metrics]  # Get all metric keys for the current resample, over all questiosn - should be the same metric\n",
        "\n",
        "                # Get all metric keys for the current resample over all questions, handling potential missing keys (values set to 0 above due to errors)\n",
        "                #The should all have the same metric name\n",
        "                clean_metric_names = []\n",
        "                metric_scores = []\n",
        "                metric_prompts = []\n",
        "\n",
        "                for m in metrics:\n",
        "                    try:\n",
        "                        key = m[metric_idx].key\n",
        "                        clean_metric_names.append(key)\n",
        "                        score = m[metric_idx].score #Scores of a given metric over all questions for a given resample\n",
        "                        prompt = m[metric_idx].value\n",
        "                    except:\n",
        "                        clean_metric_names.append(metric_name.replace('_descr',''))\n",
        "                        score = 0\n",
        "                        prompt = \"\"\n",
        "\n",
        "                    metric_scores.append(score)\n",
        "                    metric_prompts.append(prompt)\n",
        "\n",
        "                #Check that all metrics are in the same order\n",
        "                assert all(name == metric_name.replace('_descr','') for name in clean_metric_names), \"Metric keys mismatch\"\n",
        "                print(\"clean_metric_names\",clean_metric_names)\n",
        "\n",
        "                # metric_scores = [m[metric_idx].score for m in metrics] #Scores of a given metric over all questions for a given resample\n",
        "                print(\"metric_scores\",metric_scores)\n",
        "                # metric_prompts = [m[metric_idx].value for m in metrics]\n",
        "                print(\"metric_prompts\",metric_prompts)\n",
        "                results_df[f'metric_{clean_metric_names[0]}_{resample_idx+1}'] = metric_scores\n",
        "                results_df[f'prompt_{clean_metric_names[0]}_{resample_idx+1}'] = metric_prompts\n",
        "                individual_run_metric_scores[metric_name]=metric_scores #Keep track of scores of a given metric over all questions for one resample - length is num_metrics\n",
        "            print(\"individual_run_metric_scores\",individual_run_metric_scores)\n",
        "            print(\"resample_idx\",resample_idx)\n",
        "            #Has n_resamples lists, each with num_metrics sublists (each sublist has scores over all questions of one metric) \n",
        "            all_runs_metric_scores.append(individual_run_metric_scores) \n",
        "\n",
        "        all_runs_model_metrics[model_id]=all_runs_metric_scores #Used in plotting metrics\n",
        "        # Dictionary in format {model_id:[{metric_1_run_1:[values], metric_2_run_1:[values], ...}, {metric_1_run_2:[values]....}]\n",
        "        \n",
        "\n",
        "\n",
        "        #Reduce the variance (step 3.1) - CONFIRM AT THE END IF USED\n",
        "        # Calculate mean and standard error for each metric and question across K resamples\n",
        "        metric_stats_resampling = {}\n",
        "        num_questions = len(list_of_questions)\n",
        "        \n",
        "        for metric in list_of_metrics:\n",
        "            metric_stats_resampling[metric] = {\n",
        "                'means': [], # Mean score across K resamples for each question\n",
        "                'standard_errors': [], # Standard error of the mean for each question\n",
        "                'conditional_vars': [] # Conditional variance reduced by factor of K\n",
        "            }\n",
        "            \n",
        "            # For each question\n",
        "            for q in range(num_questions):\n",
        "                # Get K scores for this metric/question across all resamples\n",
        "                scores = [run[metric][q] for run in all_runs_metric_scores]\n",
        "                K = len(scores) # Number of resamples\n",
        "                \n",
        "                # Calculate mean across K resamples\n",
        "                mean = np.mean(scores)\n",
        "                # std = np.std(scores)\n",
        "                \n",
        "                # Calculate variance of scores\n",
        "                var = np.var(scores)\n",
        "                \n",
        "                # Calculate conditional variance reduced by factor of K\n",
        "                # Var(mean) = σ²/K where σ² is the variance of individual scores\n",
        "                conditional_var = var / K if K > 0 else 0\n",
        "                \n",
        "                # Standard error is sqrt of conditional variance\n",
        "                standard_error = np.sqrt(conditional_var)\n",
        "                \n",
        "                metric_stats_resampling[metric]['means'].append(mean)\n",
        "                metric_stats_resampling[metric]['standard_errors'].append(standard_error)\n",
        "                metric_stats_resampling[metric]['conditional_vars'].append(conditional_var)\n",
        "\n",
        "        print(\"metric_stats_resampling\",metric_stats_resampling)\n",
        "\n",
        "\n",
        "\n",
        "        #Save results before processing metrics\n",
        "        results_df.to_excel(f\"results_{judge_model.split('/')[1]}_judge_with_{model_id.replace('/','_')}_before_nan_replacement.xlsx\", index=False)\n",
        "\n",
        "        try:\n",
        "            # Handle 0 values across all resamples - These are errors\n",
        "            zero_rows = []\n",
        "            for resample_idx in range(n_resamples):\n",
        "                for metric in list_of_metrics:\n",
        "                    simple_metric_name=metric.replace('_descr','')\n",
        "                    print(\"simple_metric_name\",simple_metric_name)\n",
        "                    metric_col = f'metric_{simple_metric_name}_{resample_idx+1}'\n",
        "                    zero_indices = results_df[metric_col] == 0 #series with True/False\n",
        "                    if zero_indices.any():\n",
        "                        for idx in zero_indices[zero_indices].index: #Loop over all True values\n",
        "                            print(colored(f\"Missing value for metric '{simple_metric_name}' in resample {resample_idx+1}\", 'red'))\n",
        "                            print(colored(f\"Question: {results_df.loc[idx, 'questions']}\", 'green'))\n",
        "                            zero_rows.append(idx) #Keep track of all rows in which there is a 0 value anywhere (any metric, resample)\n",
        "            \n",
        "            zero_rows = list(set(zero_rows))  # Get unique indices\n",
        "            if zero_rows:\n",
        "                print(colored(f\"ERROR: Found missing values in {len(zero_rows)} rows out of {len(results_df)}\", 'red'))\n",
        "                print(colored(f\"Row indices with 0: {zero_rows}\", 'green'))\n",
        "\n",
        "\n",
        "            #Do not drop the above rows since only some are missing and not all (we have n_resamples answers for each row). \n",
        "            #Instead, replace with mean of the other resamples.\n",
        "            # Get all metric columns\n",
        "            metric_cols = [col for col in results_df.columns if col.startswith('metric_')]\n",
        "            print(\"metric_cols\",metric_cols)\n",
        "            # For each row with zeros\n",
        "            for row_idx in zero_rows:\n",
        "                # Find which metrics have zeros in this row\n",
        "                for metric_name in list_of_metrics:\n",
        "                    # Get all columns for this metric across resamples\n",
        "                    metric_resample_cols = [col for col in metric_cols if metric_name.replace('_descr','') in col]\n",
        "                    print(\"metric_resample_cols\",metric_resample_cols)\n",
        "                    # Get values for this metric across all resamples for this row\n",
        "                    values = results_df.loc[row_idx, metric_resample_cols]\n",
        "                    print(\"values\",values)\n",
        "                    try:\n",
        "                        values=values.values\n",
        "                        print(\"newvalues\",values)\n",
        "                    except:\n",
        "                        print(\"type of values\",type(values))\n",
        "                    # If any values are 0, replace with mean of non-zero values -DIDN'T DO IT YET - HAS TO BE DONE IN ALL VARIABLES, NOT ONLY IN DF\n",
        "                    if (values == 0).any():\n",
        "                        non_zero_values = values[values != 0]\n",
        "                        print(\"non_zero_values\",non_zero_values)\n",
        "                        if len(non_zero_values) > 0:  # Only replace if we have some non-zero values\n",
        "                            mean_value = non_zero_values.mean()\n",
        "                            # Replace zeros with the mean\n",
        "                            for col in metric_resample_cols:\n",
        "                                if results_df.loc[row_idx, col] == 0:\n",
        "                                    print(colored(f\"0 value in row {row_idx}, column {col} with mean {mean_value:.2f}\", 'yellow'))\n",
        "                                    # results_df.at[row_idx, col] = round(mean_value, 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(colored(f\"Error handling missing values: {str(e)}\", 'red'))\n",
        "            \n",
        "        # # # Save results after processing metrics\n",
        "        # results_df.to_excel(f\"results_{judge_model.split('/')[1]}_judge_with_{model_id.replace('/','_')}_after_nan_replacement.xlsx\", index=False)\n",
        "\n",
        "        # Reorganize metrics by type\n",
        "        metric_stats = {metric.replace('_descr', ''): [] for metric in list_of_metrics}\n",
        "        \n",
        "        print(\"all_resamples_metrics\",all_resamples_metrics)\n",
        "        for metric_name in list_of_metrics: #If above works and the order is correct, then this works too\n",
        "            clean_name = metric_name.replace('_descr', '')\n",
        "            print(\"clean_name\",clean_name)\n",
        "\n",
        "            #Each resample_metrics (num_resamples in total) has a list of num_questions lists, each having num_metrics values\n",
        "            #format of each sublist: [EvaluationResult(key='completeness', score=4, value='To evaluate the ...\n",
        "            for resample_metrics in all_resamples_metrics: \n",
        "                print(\"resample_metrics\",resample_metrics) #CONFIRM HERE THAT ORDER IS CORRECT\n",
        "                print(\"list_of_metrics.index(metric_name)\",list_of_metrics.index(metric_name))\n",
        "                print([x[list_of_metrics.index(metric_name)] if x!=0 and x!=[] else 0 for x in resample_metrics])\n",
        "                scores = [m[list_of_metrics.index(metric_name)].score if m!=0 and m!=[] else 0 for m in resample_metrics] #Scores of a given metric over all questions for one resample\n",
        "                print(\"scores\",scores)\n",
        "                metric_stats[clean_name].extend(scores) #Append the scores of a given metric over all questions for each resample\n",
        "                print(\"metric_stats\",metric_stats)\n",
        "\n",
        "        print(\"metric_statsnew\",metric_stats)\n",
        "\n",
        "        # Create final metric names and values for plotting\n",
        "        metric_names = list(metric_stats.keys()) #Same as list_of_metrics without '_descr' - just the metric names\n",
        "\n",
        "        metrics_names_loop=[metric.replace('_descr','') for metric in list_of_metrics]\n",
        "        assert metrics_names_loop==metric_names, \"Metric names mismatch\"\n",
        "\n",
        "        # Store summary stats for this model\n",
        "        all_models_stats[model_id]= plot_figures_metrics(\n",
        "            all_runs_model_metrics,  \n",
        "            metric_names, #list_of_metrics, \n",
        "            model_id, \n",
        "            judge_model\n",
        "        )\n",
        "\n",
        "        print(\"all_models_stats\",all_models_stats) #Used in model comparison below\n",
        "\n",
        "        #Save as json\n",
        "        with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "            json.dump(all_models_stats, f, indent=4)\n",
        "\n",
        "        #Save all_runs_model_metrics as json \n",
        "        with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "            json.dump(all_runs_model_metrics, f, indent=4)\n",
        "   \n",
        "        print(\"Model\",model_id, \"saved\")\n",
        "        print(\"Models saved so far\",list(all_models_stats.keys()))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in plotting metrics\")\n",
        "        print(\"Error:\", e)\n",
        "        print(\"Code that caused the error:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # Clear VRAM at the end of each iteration\n",
        "    del model, tokenizer, pipeline\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('-'*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(evaluation_all_resamples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for resample_idx in range(n_resamples):\n",
        "        if resample_idx<1:\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            print(\"stasrt\",start_idx)\n",
        "            print(\"end\",resample_idx*chunk_size)\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            print(\"resample_results\",resample_results)\n",
        "            print(\"lenresam\",len(resample_metrics))\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results] if resample_results else ['-'] * chunk_size\n",
        "            print(\"lenpred\",len(predicted_answers))\n",
        "            \n",
        "            # print(\"predicted_answers_init\",predicted_answers_init)\n",
        "            # predicted_answers = [x['run'].outputs['output'][0].content[0].text for x in resample_results]\n",
        "            print(\"predicted_answers_new\",predicted_answers)\n",
        "            # Add predicted answers and metrics side by side\n",
        "            results_df[f'predicted_answer_{resample_idx+1}'] = predicted_answers\n",
        "\n",
        "            metrics = [x['evaluation_results']['results'] if x['run'].outputs['output'] is not None else 0 for x in resample_results] #0 to handle errors\n",
        "            #A list with in the format: [EvaluationResult(key='completeness', score=4, value='To evaluate the .... - It has num_questions lists, each with num_metrics values\n",
        "            print(\"metrics\",metrics)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for resample_idx in range(n_resamples):\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            print(\"resample_results\",resample_results)\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results] if resample_results else ['-'] * chunk_size\n",
        "            print(\"xxx\",[x for x in resample_results])\n",
        "            # print(\"predicted_answers_init\",predicted_answers_init)\n",
        "            # predicted_answers = [x['run'].outputs['output'][0].content[0].text for x in resample_results]\n",
        "            print(\"predicted_answers_new\",predicted_answers)\n",
        "            print(len(predicted_answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for resample_idx in range(n_resamples):\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            print(\"resample_results\",resample_results)\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results] if resample_results else ['-'] * chunk_size\n",
        "            \n",
        "            # print(\"predicted_answers_init\",predicted_answers_init)\n",
        "            # predicted_answers = [x['run'].outputs['output'][0].content[0].text for x in resample_results]\n",
        "            print(\"predicted_answers_new\",predicted_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resample_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[x['run'].outputs['output'] for x in resample_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_runs_model_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_resamples_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resample_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_runs_model_metrics[model_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_all_resamples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "individual_run_metric_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_resamples_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load all_models_stats\n",
        "with open(f'stats_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "    all_models_stats = json.load(f)\n",
        "\n",
        "#Load all_runs_model_metrics\n",
        "with open(f'all_runs_model_metrics_{judge_model.split(\"/\")[1]}.json', 'r') as f:\n",
        "    all_runs_model_metrics = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_models_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_runs_model_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][0].keys()\n",
        "all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][0][f'{metric}']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][1][f'{metric}']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "# import itertools\n",
        "\n",
        "scores1=[5, 4, 4, 5, 5, 4, 5]\n",
        "scores2=[4, 5, 5, 3, 5, 5, 2]\n",
        "\n",
        "# Calculate differences for each question\n",
        "question_differences = np.array(scores1) - np.array(scores2)\n",
        "print(\"question_differences\",question_differences)\n",
        "\n",
        "# Calculate mean difference for this resample\n",
        "mean_diff = np.mean(question_differences) #Same as the formula in the paper since mean(a-b)=mean(a)-mean(b)\n",
        "print(\"mean_diff\",mean_diff)\n",
        "\n",
        "# Calculate standard error for this resample\n",
        "# n = len(question_differences)\n",
        "# se = np.sqrt(np.sum((question_differences - mean_diff)**2) / (n * (n-1))) if n > 1 else np.nan\n",
        "\n",
        "# Calculate standard errors for each model\n",
        "n = len(scores1)\n",
        "sea = np.sqrt(np.sum((scores1 - np.mean(scores1))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "seb = np.sqrt(np.sum((scores2 - np.mean(scores2))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "print(\"sea\",sea)\n",
        "print(\"seb\",seb)\n",
        "\n",
        "# Calculate the combined standard error as sqrt(sea^2 + seb^2)\n",
        "se = np.sqrt(sea**2 + seb**2)\n",
        "print(\"se\",se)\n",
        "\n",
        "# Calculate Pearson correlation\n",
        "correlation, _ = stats.pearsonr(scores1, scores2)\n",
        "print(\"correlation\",correlation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.mean(all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][0][f'{metric}']))\n",
        "print(np.mean(all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][1][f'{metric}']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.mean([a - b for a, b in zip(all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][0][f'{metric}'], \n",
        "                              all_runs_model_metrics['anthropic/claude-3-5-sonnet-20241022'][1][f'{metric}'])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aaaaaa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistical comparison between models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_model_performances(all_models_stats, all_runs_model_metrics):\n",
        "    \"\"\"\n",
        "    Performs statistical comparison between models using paired differences, standard errors,\n",
        "    and Pearson correlation coefficients following section 4.2 methodology.\n",
        "    \n",
        "    Args:\n",
        "        all_models_stats (dict): Dictionary containing statistics for each model\n",
        "        all_runs_model_metrics (dict): Dictionary containing raw metrics for each model/run/question\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary containing pairwise comparison results\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy import stats\n",
        "    import itertools\n",
        "    \n",
        "    # Get all model pairs for comparison\n",
        "    models = list(all_runs_model_metrics.keys())\n",
        "    model_pairs = list(itertools.combinations(models, 2))\n",
        "    \n",
        "    # Store results\n",
        "    comparison_results = {}\n",
        "    \n",
        "    for model1, model2 in model_pairs:\n",
        "        comparison_key = f\"{model1.split('/')[-1]}_vs_{model2.split('/')[-1]}\"\n",
        "        comparison_results[comparison_key] = {}\n",
        "        \n",
        "        # Get metrics (removing '_descr' suffix)\n",
        "        metrics = [metric.replace('_descr', '') for metric in list(all_runs_model_metrics[model1][0].keys())]\n",
        "        \n",
        "        for metric in metrics:\n",
        "            # Calculate differences and correlations for each resample\n",
        "            resample_differences = []\n",
        "            resample_ses = []\n",
        "            correlations = []\n",
        "            model1_variances = []  # Initialize list here\n",
        "            model2_variances = []  # Initialize list here\n",
        "            \n",
        "            # Iterate through resamples - Same number for both models\n",
        "            for resample_idx in range(len(all_runs_model_metrics[model1])):\n",
        "                # Get scores for both models for this resample\n",
        "                scores1 = all_runs_model_metrics[model1][resample_idx][f'{metric}_descr']\n",
        "                scores2 = all_runs_model_metrics[model2][resample_idx][f'{metric}_descr']\n",
        "                \n",
        "                # Calculate differences for each question\n",
        "                question_differences = np.array(scores1) - np.array(scores2)\n",
        "                \n",
        "                # Calculate mean difference for this resample\n",
        "                mean_diff = np.mean(question_differences) #Same as the formula in the paper since mean(a-b)=mean(a)-mean(b)\n",
        "                \n",
        "                # Calculate standard error for this resample - Paired analysis (section 4.2)\n",
        "                n = len(question_differences)\n",
        "                se = np.sqrt(np.sum((question_differences - mean_diff)**2) / (n * (n-1))) if n > 1 else np.nan\n",
        "\n",
        "                # # Calculate standard errors for each model - Unpaired analysis (section 4.1)\n",
        "                # n = len(scores1)\n",
        "                # sea = np.sqrt(np.sum((scores1 - np.mean(scores1))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "                # seb = np.sqrt(np.sum((scores2 - np.mean(scores2))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "\n",
        "                # # Calculate the combined standard error as sqrt(sea^2 + seb^2)\n",
        "                # se = np.sqrt(sea**2 + seb**2)\n",
        "\n",
        "                                # Calculate variances for each model\n",
        "                var1 = np.var(scores1, ddof=1)  # Using ddof=1 for sample variance\n",
        "                var2 = np.var(scores2, ddof=1)\n",
        "                model1_variances.append(var1)\n",
        "                model2_variances.append(var2)\n",
        "                \n",
        "                \n",
        "                # Calculate Pearson correlation\n",
        "                correlation, _ = stats.pearsonr(scores1, scores2)\n",
        "                \n",
        "                resample_differences.append(mean_diff)\n",
        "                resample_ses.append(se)\n",
        "                correlations.append(correlation)\n",
        "            \n",
        "            # Convert to numpy arrays\n",
        "            resample_differences = np.array(resample_differences)\n",
        "            resample_ses = np.array(resample_ses)\n",
        "            correlations = np.array(correlations)\n",
        "            model1_variances = np.array(model1_variances)\n",
        "            model2_variances = np.array(model2_variances)\n",
        "            print(\"resample_differences\",resample_differences)\n",
        "            print(\"resample_ses\",resample_ses)\n",
        "            print(\"correlations\",correlations)\n",
        "            print(f\"Model 1 variances: {model1_variances}\")\n",
        "            print(f\"Model 2 variances: {model2_variances}\")\n",
        "          \n",
        "            \n",
        "\n",
        "            # Calculate overall mean difference over all resamples\n",
        "            overall_mean_diff = np.mean(resample_differences)\n",
        "            print(\"overall_mean_diff\",overall_mean_diff)\n",
        "            \n",
        "            #We want an aggregated SE across all resamples for the same questions (same paired differences)\n",
        "            #This approach accounts for the fact that each resampling provides a different estimate of the variance of the same underlying distribution, \n",
        "            # and averaging these estimates gives a better representation of the overall uncertainty.\n",
        "\n",
        "            # Calculate pooled standard error across resamples\n",
        "            R = len(resample_differences)\n",
        "            pooled_se = np.sqrt(np.sum(resample_ses**2) / (R**2))\n",
        "            print(\"pooled_se\",pooled_se)\n",
        "            \n",
        "            # # If the resampling results are independent estimates of variance (i.e., combining uncertainty estimates from independent sources), the combined variance is\n",
        "            # # the sum of all individual variances, and the combined standard error is given below (goal to capture total variability)\n",
        "            # # Calculate the overall combined SE across all resamples\n",
        "            # combined_se = np.sqrt(np.nansum(np.array(resample_ses)**2))\n",
        "\n",
        "\n",
        "            # Calculate overall variance reduction across all resamples\n",
        "            n = len(scores1)\n",
        "            \n",
        "            # Calculate mean variances across resamples\n",
        "            mean_var1 = np.mean(model1_variances)  # Var(sA)\n",
        "            mean_var2 = np.mean(model2_variances)  # Var(sB)\n",
        "            \n",
        "            # Calculate mean correlation across resamples\n",
        "            mean_correlation = np.mean(correlations)\n",
        "            \n",
        "            # Calculate covariance between model scores\n",
        "            mean_cov = mean_correlation * np.sqrt(mean_var1 * mean_var2)  # Cov(sA, sB)\n",
        "            \n",
        "            # Calculate variance for unpaired case: Var(μA-B,unpaired) = (Var(sA) + Var(sB))/n\n",
        "            var_unpaired = (mean_var1 + mean_var2) / n\n",
        "            \n",
        "            # Calculate variance for paired case: Var(μA-B,paired) = (Var(sA) + Var(sB) - 2Cov(sA,sB))/n\n",
        "            var_paired = (mean_var1 + mean_var2 - 2 * mean_cov) / n\n",
        "            \n",
        "            # The reduction in variance is: Var(μA-B,unpaired) - Var(μA-B,paired) = 2Cov(xA,xB)/n\n",
        "            variance_reduction = 2 * mean_cov / n  # This should equal var_unpaired - var_paired\n",
        "            \n",
        "            # Calculate percentage reduction in variance\n",
        "            percent_reduction = (variance_reduction / var_unpaired) * 100 if var_unpaired != 0 else 0\n",
        "            \n",
        "            print(f\"\\nOverall Variance Reduction Analysis:\")\n",
        "            print(f\"Mean Model 1 variance (Var(sA)): {mean_var1:.6f}\")\n",
        "            print(f\"Mean Model 2 variance (Var(sB)): {mean_var2:.6f}\")\n",
        "            print(f\"Mean covariance (Cov(sA,sB)): {mean_cov:.6f}\")\n",
        "            print(f\"Unpaired variance: {var_unpaired:.6f}\")\n",
        "            print(f\"Paired variance: {var_paired:.6f}\") \n",
        "            print(f\"Variance reduction (2Cov(xA,xB)/n): {variance_reduction:.6f}\")\n",
        "            print(f\"Percent reduction: {percent_reduction:.1f}%\")\n",
        "\n",
        "            # Create individual filename from model comparison and metric\n",
        "            individual_filename = f\"variance_analysis_{comparison_key}_{metric}.txt\"\n",
        "            \n",
        "            # Save variance reduction results to both individual file and combined file\n",
        "            variance_results_text = f\"\\n=== Results from {individual_filename} ===\\n\"\n",
        "            variance_results_text += f\"Overall Variance Reduction Analysis:\\n\"\n",
        "            variance_results_text += f\"Mean Model 1 variance (Var(sA)): {mean_var1:.6f}\\n\"\n",
        "            variance_results_text += f\"Mean Model 2 variance (Var(sB)): {mean_var2:.6f}\\n\"\n",
        "            variance_results_text += f\"Mean covariance (Cov(sA,sB)): {mean_cov:.6f}\\n\"\n",
        "            variance_results_text += f\"Unpaired variance: {var_unpaired:.6f}\\n\"\n",
        "            variance_results_text += f\"Paired variance: {var_paired:.6f}\\n\"\n",
        "            variance_results_text += f\"Variance reduction (2Cov(xA,xB)/n): {variance_reduction:.6f}\\n\"\n",
        "            variance_results_text += f\"Percent reduction: {percent_reduction:.1f}%\\n\"\n",
        "\n",
        "            # Write to individual file\n",
        "            with open(individual_filename, 'a') as f:\n",
        "                f.write(variance_results_text)\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "            # # Calculate t-statistic and p-value\n",
        "            # t_stat = overall_mean_diff / pooled_se if pooled_se != 0 else np.nan\n",
        "            # df = R - 1  # degrees of freedom\n",
        "            # p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if not np.isnan(t_stat) else np.nan\n",
        "            \n",
        "            # # Calculate confidence interval\n",
        "            # t_crit = stats.t.ppf(0.975, df)  # 95% CI\n",
        "            # ci_margin = t_crit * pooled_se\n",
        "\n",
        "            # Write to combined file\n",
        "            with open(all_variance_results, 'a') as f:\n",
        "                f.write(variance_results_text)\n",
        "\n",
        "            # Calculate z-statistic and CI using standard normal distribution\n",
        "            z_stat = overall_mean_diff / pooled_se if pooled_se != 0 else np.nan\n",
        "            \n",
        "            # Calculate confidence interval using 1.96 for 95% CI\n",
        "            ci_margin = 1.96 * pooled_se\n",
        "            \n",
        "            # Calculate p-value using standard normal distribution\n",
        "            #For a two-tailed test p = 2 × (1 − Φ(|z|)), where Φ(z) is the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat))) if not np.isnan(z_stat) else np.nan\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            # # Calculate average Pearson correlation - not accurate when correlations close to 1 or -1, variances differences across resamples, sample size is small.\n",
        "            # avg_correlation = np.mean(correlations)\n",
        "\n",
        "            #Apply Fisher z-transformation\n",
        "            z_values = [0.5 * np.log((1 + r) / (1 - r)) for r in correlations]\n",
        "\n",
        "            # Compute the mean Fisher z-value\n",
        "            z_mean = np.mean(z_values)\n",
        "\n",
        "            #Back-transform to Pearson correlation scale\n",
        "            overall_correlation = (np.exp(2 * z_mean) - 1) / (np.exp(2 * z_mean) + 1)\n",
        "\n",
        "            print(f\"Overall Pearson Correlation: {overall_correlation}\")\n",
        "\n",
        "\n",
        "            \n",
        "            # Store results\n",
        "            comparison_results[comparison_key][metric] = {\n",
        "                \"mean_difference\": overall_mean_diff,\n",
        "                \"pooled_standard_error\": pooled_se,\n",
        "                \"ci_low\": overall_mean_diff - ci_margin,\n",
        "                \"ci_high\": overall_mean_diff + ci_margin,\n",
        "                # \"t_statistic\": t_stat,\n",
        "                \"z_statistic\": z_stat,\n",
        "                \"p_value\": p_value,\n",
        "                \"effect_size\": overall_correlation,\n",
        "                \"significant\": p_value < 0.05 if not np.isnan(p_value) else None,\n",
        "                \"better_model\": model1.split('/')[-1] if overall_mean_diff > 0 else model2.split('/')[-1],\n",
        "                \"pearson_correlation\": overall_correlation\n",
        "            }\n",
        "            \n",
        "            # Print results and save to file\n",
        "            results_text = f\"\\n{comparison_key} - {metric}:\\n\"\n",
        "            results_text += f\"Mean difference: {overall_mean_diff:.3f}\\n\"\n",
        "            results_text += f\"Pooled SE: {pooled_se:.3f}\\n\" \n",
        "            results_text += f\"95% CI: [{overall_mean_diff - ci_margin:.3f}, {overall_mean_diff + ci_margin:.3f}]\\n\"\n",
        "            # results_text += f\"t-statistic: {t_stat:.3f}\\n\"\n",
        "            results_text += f\"z-statistic: {z_stat:.3f}\\n\"\n",
        "            results_text += f\"p-value: {p_value:.3f}\\n\"\n",
        "            results_text += f\"Pearson correlation: {overall_correlation:.3f}\\n\"\n",
        "            results_text += f\"Better model: {comparison_results[comparison_key][metric]['better_model']}\\n\"\n",
        "            results_text += f\"Statistically significant: {p_value < 0.05 if not np.isnan(p_value) else None}\\n\"\n",
        "            \n",
        "            print(results_text)\n",
        "            \n",
        "            # # Save to file\n",
        "            # with open(f'comparison_results_{comparison_key}_{metric}.txt', 'w') as f:\n",
        "            #     f.write(results_text)\n",
        "    \n",
        "    return comparison_results\n",
        "\n",
        "# Example usage:\n",
        "comparison_results = compare_model_performances(all_models_stats, all_runs_model_metrics)\n",
        "\n",
        "# Save results to file - DO I NEED TO SAVE THIS? - JUST IF WE WANT EVERYTHING IN ONE FILE (better to keep and not keep the compariosn_results above)\n",
        "with open('model_comparison_results_v3.json', 'w') as f:\n",
        "    # Convert numpy types to native Python types for JSON serialization\n",
        "    def convert_to_serializable(obj):\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
        "            np.int16, np.int32, np.int64, np.uint8,\n",
        "            np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.bool_)):\n",
        "            return bool(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        elif obj is None:\n",
        "            return None\n",
        "        return obj\n",
        "    \n",
        "    serializable_results = json.loads(\n",
        "        json.dumps(comparison_results, default=convert_to_serializable)\n",
        "    )\n",
        "    json.dump(serializable_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"comparison_results\",comparison_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comparison results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract metrics and models from comparison_results\n",
        "metrics = [metric.replace('_descr', '') for metric in list_of_metrics]\n",
        "model_pairs = list(comparison_results.keys())\n",
        "\n",
        "# Create figure with subplots for each metric\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Model Comparison Results by Metric', fontsize=16)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Extract data for this metric\n",
        "    means = []\n",
        "    cis = []\n",
        "    labels = []\n",
        "    \n",
        "    for pair in model_pairs:\n",
        "        metric_data = comparison_results[pair][metric]\n",
        "        means.append(metric_data['mean_difference'])\n",
        "        # ci_margin = metric_data['ci_margin']\n",
        "        cis.append([metric_data['ci_low'], \n",
        "                   metric_data['ci_high']])\n",
        "        labels.append(pair)\n",
        "\n",
        "    # Create bar plot\n",
        "    bars = ax.bar(range(len(means)), means)\n",
        "    \n",
        "    # Add error bars for confidence intervals\n",
        "    ax.errorbar(range(len(means)), means, \n",
        "               yerr=[[m - ci[0] for m, ci in zip(means, cis)],\n",
        "                     [ci[1] - m for m, ci in zip(means, cis)]],\n",
        "               fmt='none', color='black', capsize=5)\n",
        "    \n",
        "    # Add horizontal line at y=0\n",
        "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_title(f'{metric.capitalize()}')\n",
        "    ax.set_xticks(range(len(means)))\n",
        "    ax.set_xticklabels(labels,ha='right') # rotation=45, \n",
        "    ax.set_ylabel('Mean Difference')\n",
        "    \n",
        "    # Color bars based on statistical significance\n",
        "    for j, bar in enumerate(bars):\n",
        "        if comparison_results[model_pairs[j]][metric]['p_value'] < 0.05:\n",
        "            bar.set_color('darkred')\n",
        "        else:\n",
        "            bar.set_color('lightgray')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot before showing\n",
        "plt.savefig('model_comparisons.png', bbox_inches='tight', dpi=300)\n",
        "\n",
        "# Show plot after saving\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import itertools\n",
        "# import numpy as np\n",
        "# from scipy import stats\n",
        "\n",
        "# def compare_models(all_results, alpha=0.05):\n",
        "#     \"\"\"\n",
        "#     Performs statistical comparison between all model pairs.\n",
        "    \n",
        "#     Args:\n",
        "#         all_results (list): List of evaluation results for each run\n",
        "#         alpha (float): Significance level for statistical tests\n",
        "    \n",
        "#     Returns:\n",
        "#         dict: Dictionary containing pairwise comparison results\n",
        "#     \"\"\"\n",
        "#     # Extract model names and scores from the evaluation results\n",
        "#     results_by_model = {}\n",
        "#     for result in all_results:\n",
        "#         # Get model name from the session name - handle case where 'with' is incorrectly extracted\n",
        "#         model_name = result['run'].session_name\n",
        "#         # Extract the full model name, not just a part\n",
        "#         if model_name not in results_by_model:\n",
        "#             results_by_model[model_name] = []\n",
        "            \n",
        "#         # Extract scores from evaluation results\n",
        "#         for eval_result in result['evaluation_results']['results']:\n",
        "#             results_by_model[model_name].append({\n",
        "#                 'metric': eval_result.key,\n",
        "#                 'score': eval_result.score\n",
        "#             })\n",
        "#     print(\"results_by_model\",results_by_model)\n",
        "    \n",
        "#     # Ensure we have at least 2 models to compare\n",
        "#     if len(results_by_model.keys()) < 2:\n",
        "#         print(\"Warning: Need at least 2 models to compare. Check session naming convention.\")\n",
        "#         return {}\n",
        "        \n",
        "#     model_pairs = list(itertools.combinations(results_by_model.keys(), 2))\n",
        "#     print(\"model_pairs\",model_pairs)\n",
        "#     comparison_results = {}\n",
        "    \n",
        "#     for model1, model2 in model_pairs:\n",
        "#         comparison_results[f\"{model1}_vs_{model2}\"] = {}\n",
        "        \n",
        "#         # Get all unique metrics\n",
        "#         metrics = set(r['metric'] for r in results_by_model[model1] + results_by_model[model2])\n",
        "        \n",
        "#         for metric in metrics:\n",
        "#             # Get scores for both models for this metric\n",
        "#             scores1 = [r['score'] for r in results_by_model[model1] if r['metric'] == metric]\n",
        "#             scores2 = [r['score'] for r in results_by_model[model2] if r['metric'] == metric]\n",
        "            \n",
        "#             if not scores1 or not scores2:\n",
        "#                 continue\n",
        "                \n",
        "#             # Perform t-test\n",
        "#             t_stat, p_value = stats.ttest_ind(scores1, scores2)\n",
        "            \n",
        "#             # Calculate effect size (Cohen's d)\n",
        "#             pooled_std = np.sqrt((np.var(scores1) + np.var(scores2)) / 2)\n",
        "#             effect_size = (np.mean(scores1) - np.mean(scores2)) / pooled_std\n",
        "            \n",
        "#             comparison_results[f\"{model1}_vs_{model2}\"][metric] = {\n",
        "#                 \"t_statistic\": t_stat,\n",
        "#                 \"p_value\": p_value,\n",
        "#                 \"effect_size\": effect_size,\n",
        "#                 \"significant\": p_value < alpha,\n",
        "#                 \"better_model\": model1 if np.mean(scores1) > np.mean(scores2) else model2\n",
        "#             }\n",
        "    \n",
        "#     return comparison_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Perform model comparisons\n",
        "# comparison_results = compare_models(evaluation_all_resamples)\n",
        "\n",
        "# # Print comparison results\n",
        "# print(\"\\nModel Comparison Results:\")\n",
        "# for comparison, metrics in comparison_results.items():\n",
        "#     print(f\"\\n{comparison}:\")\n",
        "#     for metric, results in metrics.items():\n",
        "#         print(f\"\\n{metric}:\")\n",
        "#         print(f\"  Better model: {results['better_model']}\")\n",
        "#         print(f\"  Effect size: {results['effect_size']:.3f}\")\n",
        "#         print(f\"  P-value: {results['p_value']:.3f}\")\n",
        "#         print(f\"  Significant: {results['significant']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Optionally, create a visualization of the comparison\n",
        "# def plot_final_comparison(all_models_stats):\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import numpy as np\n",
        "    \n",
        "#     if not all_models_stats:\n",
        "#         return\n",
        "        \n",
        "#     metrics = list(next(iter(all_models_stats.values())).keys())\n",
        "#     models = list(all_models_stats.keys())\n",
        "#     n_metrics = len(metrics)\n",
        "#     n_models = len(models)\n",
        "    \n",
        "#     fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "#     x = np.arange(n_metrics)\n",
        "#     width = 0.8 / n_models\n",
        "    \n",
        "#     for i, model in enumerate(models):\n",
        "#         means = [all_models_stats[model][metric]['mean'] for metric in metrics]\n",
        "#         errs = [all_models_stats[model][metric]['std_error'] for metric in metrics]\n",
        "        \n",
        "#         # Calculate 95% confidence intervals\n",
        "#         conf_intervals = []\n",
        "#         for mean, err in zip(means, errs):\n",
        "#             # For 95% CI, multiply standard error by 1.96\n",
        "#             conf_interval = 1.96 * err\n",
        "#             conf_intervals.append(conf_interval)\n",
        "        \n",
        "#         ax.bar(x + i*width - width*n_models/2, means, width,\n",
        "#                label=model.split('/')[-1],\n",
        "#                yerr=conf_intervals, capsize=5)\n",
        "        \n",
        "#         # Add confidence interval bounds as text\n",
        "#         for j, (mean, ci) in enumerate(zip(means, conf_intervals)):\n",
        "#             lower = mean - ci\n",
        "#             upper = mean + ci\n",
        "#             # Print upper limit above error bar\n",
        "#             ax.text(x[j] + i*width - width*n_models/2, mean + ci + 0.01,\n",
        "#                    f'{upper:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "#             # Print lower limit below error bar\n",
        "#             ax.text(x[j] + i*width - width*n_models/2, mean - ci - 0.01,\n",
        "#                    f'{lower:.3f}', ha='center', va='top', fontsize=8)\n",
        "    \n",
        "#     ax.set_ylabel('Score')\n",
        "#     ax.set_title('Metric Comparison Across Models\\nwith 95% Confidence Intervals')\n",
        "#     ax.set_xticks(x)\n",
        "#     ax.set_xticklabels(metrics, rotation=45)\n",
        "#     ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig('model_comparison.png', bbox_inches='tight')\n",
        "#     plt.show()\n",
        "#     plt.close()\n",
        "\n",
        "# # Create the comparison plot\n",
        "# plot_final_comparison(all_models_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def perform_paired_analysis(all_runs_model_metrics):\n",
        "#     \"\"\"\n",
        "#     Performs paired t-tests between all pairs of models for each metric.\n",
        "    \n",
        "#     Args:\n",
        "#         all_runs_model_metrics: Dictionary with model IDs as keys and lists of metric scores as values\n",
        "#     Returns:\n",
        "#         Dictionary containing pairwise comparison results\n",
        "#     \"\"\"\n",
        "#     from scipy import stats\n",
        "#     import numpy as np\n",
        "    \n",
        "#     # Store results\n",
        "#     comparison_results = {}\n",
        "    \n",
        "#     # Get all model pairs\n",
        "#     model_pairs = list(itertools.combinations(all_runs_model_metrics.keys(), 2))\n",
        "    \n",
        "#     for model1, model2 in model_pairs:\n",
        "#         comparison_key = f\"{model1}_vs_{model2}\"\n",
        "#         comparison_results[comparison_key] = {}\n",
        "        \n",
        "#         # Get metrics for both models\n",
        "#         metrics1 = all_runs_model_metrics[model1]\n",
        "#         metrics2 = all_runs_model_metrics[model2]\n",
        "        \n",
        "#         # For each metric type\n",
        "#         for metric_name in list_of_metrics:\n",
        "#             metric_name = metric_name.replace('_descr', '')\n",
        "            \n",
        "#             # Get all scores for this metric across resamples\n",
        "#             scores1 = []\n",
        "#             scores2 = []\n",
        "            \n",
        "#             # Collect scores from all resamples\n",
        "#             for resample_metrics1, resample_metrics2 in zip(metrics1, metrics2):\n",
        "#                 scores1.extend(resample_metrics1[metric_name + '_descr'])\n",
        "#                 scores2.extend(resample_metrics2[metric_name + '_descr'])\n",
        "            \n",
        "#             # Perform paired t-test\n",
        "#             t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
        "            \n",
        "#             # Calculate effect size (Cohen's d)\n",
        "#             diff = np.array(scores1) - np.array(scores2)\n",
        "#             cohens_d = np.mean(diff) / np.std(diff, ddof=1)\n",
        "            \n",
        "#             comparison_results[comparison_key][metric_name] = {\n",
        "#                 \"t_statistic\": t_stat,\n",
        "#                 \"p_value\": p_value,\n",
        "#                 \"effect_size\": cohens_d,\n",
        "#                 \"significant\": p_value < 0.05,\n",
        "#                 \"better_model\": model1 if np.mean(scores1) > np.mean(scores2) else model2,\n",
        "#                 \"mean_diff\": np.mean(scores1) - np.mean(scores2)\n",
        "#             }\n",
        "            \n",
        "#             # Print results\n",
        "#             print(f\"\\nResults for {metric_name} - {model1} vs {model2}:\")\n",
        "#             print(f\"t-statistic: {t_stat:.4f}\")\n",
        "#             print(f\"p-value: {p_value:.4f}\")\n",
        "#             print(f\"Cohen's d: {cohens_d:.4f}\")\n",
        "#             print(f\"Mean difference: {np.mean(scores1) - np.mean(scores2):.4f}\")\n",
        "#             print(f\"Better model: {comparison_results[comparison_key][metric_name]['better_model']}\")\n",
        "#             print(f\"Statistically significant: {p_value < 0.05}\")\n",
        "    \n",
        "#     return comparison_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # After all models have been evaluated\n",
        "# print(\"\\nPerforming paired analysis between models...\")\n",
        "# paired_analysis_results = perform_paired_analysis(all_runs_model_metrics)\n",
        "\n",
        "# # Optionally save results to file\n",
        "# import json\n",
        "# with open(f'paired_analysis_results_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "#     json.dump(paired_analysis_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tables with num questions, model_ids in columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Report average score per metric and std in parenthesis as percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Power Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "def perform_power_analysis(effect_size=0.5, alpha=0.05, power=0.8):\n",
        "    \"\"\"\n",
        "    Perform power analysis to determine required sample size.\n",
        "    \n",
        "    Args:\n",
        "        effect_size (float): Expected effect size (Cohen's d)\n",
        "        alpha (float): Significance level\n",
        "        power (float): Desired statistical power\n",
        "        \n",
        "    Returns:\n",
        "        int: Required sample size per group\n",
        "    \"\"\"\n",
        "    analysis = TTestIndPower()\n",
        "    sample_size = analysis.solve_power(\n",
        "        effect_size=effect_size,\n",
        "        alpha=alpha,\n",
        "        power=power,\n",
        "        alternative='two-sided'\n",
        "    )\n",
        "    return int(np.ceil(sample_size))\n",
        "\n",
        "# First, determine required sample size\n",
        "required_samples = perform_power_analysis(effect_size=0.1254, alpha=0.05, power=0.8)  #These parameters result in a sample size of 1000\n",
        "print(f\"Required samples per model for statistical power: {required_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For real-time inference (below implementation only for meta-llama/Meta-Llama-3.1-8B-Instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# # del pipeline #Otherwise too much memory is used\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto')\n",
        "\n",
        "# #Example of real-time response generation\n",
        "# messages=[{\"role\": \"user\", \"content\": \"What is the chemical formula of water?\"}]\n",
        "\n",
        "# inputs_tokenized = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#     return_dict=True,\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# input_ids = inputs_tokenized['input_ids']\n",
        "\n",
        "# # Generate tokens one by one\n",
        "# max_length = 256\n",
        "# output_ids = input_ids\n",
        "# for _ in range(256):\n",
        "#     outputs = model.generate(\n",
        "#         output_ids,\n",
        "#         max_new_tokens=1,\n",
        "#         do_sample=True,\n",
        "#         top_k=50,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "#     new_token_id = outputs[0, -1].item()\n",
        "#     if new_token_id == tokenizer.eos_token_id:\n",
        "#         break\n",
        "#     output_ids = torch.cat([output_ids, outputs[:, -1:]], dim=1)\n",
        "#     new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
        "#     print(new_token, end=\"\", flush=True)\n",
        "\n",
        "# print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other evaluators from Langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations\n",
        "# https://docs.smith.langchain.com/old/evaluation/quickstart\n",
        "\n",
        "# from langsmith.evaluation import LangChainStringEvaluator\n",
        "\n",
        "# eval_llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0.0, seed=42)\n",
        "\n",
        "# #Evaluators\n",
        "# qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm}) #LLM just gives 'correct' or 'incorrect' based on reference answer\n",
        "# context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm}) #Also uses reference context of example outputs to do the above\n",
        "# cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm}) #Same as above but with chain of thought 'reasoning'\n",
        "\n",
        "#Prompts Used internally:\n",
        "\n",
        "# 1) context_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. \n",
        "# It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 2) cot_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "# Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# EXPLANATION: step by step reasoning here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 3) qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# TRUE ANSWER: true answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, use custom prompts as shown below (and set {\"prompt\": PROMPT} as additional argument inside the config above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "# _PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in chemical engineering answers to questions.\n",
        "# You are grading the following question:\n",
        "# {query}\n",
        "# Here is the real answer:\n",
        "# {answer}\n",
        "# You are grading the following predicted answer:\n",
        "# {result}\n",
        "# Respond with CORRECT or INCORRECT:\n",
        "# \"\"\"\n",
        "\n",
        "# PROMPT = PromptTemplate(\n",
        "#     input_variables=[\"query\", \"result\", \"answer\"], template=_PROMPT_TEMPLATE\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes: Non-reproducible results, even when seed set (https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed), temperature=0 (top_p should not change when we changed temperature - smaller values result in more constrained and focused response - https://medium.com/@rasithbm/chatopenai-parameters-83bef49f6384)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b9c5f781e34806b9756d9e3e553a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cacd8a8bd03b4d0d83d266fe85e8ee65",
              "IPY_MODEL_abf8eb8102384433a59628820355d272",
              "IPY_MODEL_be7aa2993d57460c9d4f23c090e42c36"
            ],
            "layout": "IPY_MODEL_75aa3a3eb1b8420f9f26d404505e46cc"
          }
        },
        "0adc7382479c412f9c9230a17b56ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c13fc64f2e143b29105ec10e444b779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ace1e934716404a9340bce56cb1a3cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e3db7de6fcc04e6ba738f7ae78cef24d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1bd3eb0157a3477f907dae0c8fdbbec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24713d9c124b41488af127cfd3d1321e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331036e81d104ab49c44bcbde2d873f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec4c77240854f3ebab46e0b7d307f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ace1e934716404a9340bce56cb1a3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9db2c468cf4dc598a80da6a548d034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9d371e98fc45329cf381bc36a290ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331036e81d104ab49c44bcbde2d873f7",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd3eb0157a3477f907dae0c8fdbbec4",
            "value": ""
          }
        },
        "6e84ac6346d8450d9814b9f1a647164c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75aa3a3eb1b8420f9f26d404505e46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87311a42fde0441fb2e88a0655a95f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c13fc64f2e143b29105ec10e444b779",
              "IPY_MODEL_a2ac8fac33444da794be1f25a9c0d702",
              "IPY_MODEL_c0e527b08dd942a684246e2db22ed22d"
            ],
            "layout": "IPY_MODEL_cf7de095d0514bbf937d0632c777a232"
          }
        },
        "9253c9636a6c4e20b215ff11c928be07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "981e94324ba64b548259b1f1d297a564": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24713d9c124b41488af127cfd3d1321e",
            "placeholder": "​",
            "style": "IPY_MODEL_0adc7382479c412f9c9230a17b56ea42",
            "value": " 9/? [00:06&lt;00:00,  6.70s/it]"
          }
        },
        "98971ab8fe5c411f9c8c4c77753a745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bc61359ce44954bd235566d9ada2d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f628c84495494694a62ca9c181ec63ba",
            "value": 1
          }
        },
        "9b527616d5a647d88a33b14ee2712211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9d371e98fc45329cf381bc36a290ae",
              "IPY_MODEL_98971ab8fe5c411f9c8c4c77753a745f",
              "IPY_MODEL_981e94324ba64b548259b1f1d297a564"
            ],
            "layout": "IPY_MODEL_c1ffb867972444579481d5408abcdba9"
          }
        },
        "a2ac8fac33444da794be1f25a9c0d702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec4c77240854f3ebab46e0b7d307f74",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcfaa70f8c14dfcb0528b0cc0573db3",
            "value": 2
          }
        },
        "abf8eb8102384433a59628820355d272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6d3c439b254aa793c5d39304170849",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fac87e8a3a044de994d726896d479de3",
            "value": 1
          }
        },
        "b68644c249c04e059c924e1165a01370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7aa2993d57460c9d4f23c090e42c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6defd5c41f94c0ebda4080bb09c19c3",
            "placeholder": "​",
            "style": "IPY_MODEL_6e84ac6346d8450d9814b9f1a647164c",
            "value": " 9/? [01:10&lt;00:00,  5.27s/it]"
          }
        },
        "c0e527b08dd942a684246e2db22ed22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed1e9eb6e6bc4452b3d12aecffb6cc2a",
            "placeholder": "​",
            "style": "IPY_MODEL_9253c9636a6c4e20b215ff11c928be07",
            "value": " 2/2 [00:18&lt;00:00,  8.66s/it]"
          }
        },
        "c1ffb867972444579481d5408abcdba9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6d3c439b254aa793c5d39304170849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cacd8a8bd03b4d0d83d266fe85e8ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68644c249c04e059c924e1165a01370",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9db2c468cf4dc598a80da6a548d034",
            "value": ""
          }
        },
        "cf7de095d0514bbf937d0632c777a232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3db7de6fcc04e6ba738f7ae78cef24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8bc61359ce44954bd235566d9ada2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ed1e9eb6e6bc4452b3d12aecffb6cc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f628c84495494694a62ca9c181ec63ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6defd5c41f94c0ebda4080bb09c19c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac87e8a3a044de994d726896d479de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdcfaa70f8c14dfcb0528b0cc0573db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
