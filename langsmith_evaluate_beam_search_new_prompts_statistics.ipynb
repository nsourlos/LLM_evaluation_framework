{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5Kh-zLB9lk"
      },
      "source": [
        "# Evaluate LLM results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no2PHIOWCBdA"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U2Dpuc2xtmmS"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install datasets==2.20.0\n",
        "# !pip install -U langsmith==0.1.99\n",
        "# !pip install langchain_openai==0.1.22\n",
        "# !pip install langchain==0.2.13\n",
        "# !pip install langchain_community==0.2.12                          \n",
        "# !pip install transformers==4.44.0\n",
        "# !pip install termcolor==2.4.0\n",
        "# !pip install accelerate==0.33.0\n",
        "# !pip install pandas==2.2.2\n",
        "# !pip install openpyxl==3.1.5\n",
        "# !pip install python-dotenv==1.0.1\n",
        "# !pip install einops==0.8.0\n",
        "# !pip install wheel==0.44.0\n",
        "# !pip install sentencepiece==0.2.0\n",
        "# !pip install protobuf==5.27.3 #Mistral models needs this\n",
        "# !pip install groq==0.10.0 #Groq models needs this\n",
        "# !pip install matplotlib==3.9.2\n",
        "# !pip install seaborn==0.13.2\n",
        "\n",
        "# !pip install flash-attn==2.6.3 #Install it at the end after wheel has been installed\n",
        "# !pip install anthropic==0.34.1 #Anthropic models needs this\n",
        "\n",
        "# #Only if CPU is used\n",
        "# !pip install torch==2.0.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RunPod specific parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For RunPod change to persistent storage directory\n",
        "import os\n",
        "os.chdir('/workspace')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify Path and Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path ='/workspace/Example_QA_data_raw.xlsx' #Dataset generated with the help of GPT-4o - Has to be an excel file with 'input' and 'output' columns\n",
        "#'/Users/nikolaossourlo/Desktop/Example_QA_data_raw.xlsx' #For MacOS\n",
        "#'C:/Users/soyrl/Desktop/Example_QA_data_raw.xlsx' #For Windows\n",
        "# '/content/drive/My Drive/Example_QA_data_raw.xlsx' #For Google Colab\n",
        "#'/home/nikolaossourlo/Example_QA_data_raw.xlsx' #For Delft Blue\n",
        "# '/workspace/Example_QA_data_raw.xlsx' #For RunPod\n",
        "\n",
        "custom_cache_dir=\"/workspace/cache/huggingface\" #Save models here so that we don't have to download them again\n",
        "#\"/scratch/nikolaossourlo/cache\" in Delft Blue\n",
        "\n",
        "# Check if custom_cache_dir is defined, otherwise use default behavior\n",
        "try:\n",
        "    cache_dir=custom_cache_dir\n",
        "except:\n",
        "    cache_dir=None\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
        "\n",
        "# Get the OpenAI API key\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "\n",
        "#Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# Log in with your Hugging Face token\n",
        "login(token=os.getenv('HF_TOKEN'))\n",
        "\n",
        "# print(openai_api_key)\n",
        "# print(langsmith_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select model and name for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Model to generate responses to questions\n",
        "models=[ \n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", #Takes 2.5-3mins in A4500 (20GB VRAM) and in Delft Blue (35min for 44Q&A pairs in V100 32GB)\n",
        "    # \"microsoft/Phi-3.5-mini-instruct\", #Took 5mins in A40 with 48GB VRAM, 2mins in A4500 with 20GB VRAM, 3mins in Delft Blue (50min for 44Q&A pairs in V100 32GB)\n",
        "    # \"mistralai/Mistral-7B-Instruct-v0.3\", #4mins in A40 with 48GB VRAM, 2.5mins in A4500 with 20GB VRAM and in Delft Blue\n",
        "    # \"Qwen/Qwen2-7B-Instruct\", #4mins in A40 with 48GB VRAM, 2 mins in A4500 with 20GB VRAM, 2.5mins in Delft Blue\n",
        "    # 'AI-MO/NuminaMath-7B-TIR', #2.5 in A4500 with 20GB VRAM and in Delft Blue - We can also try 01-ai/Yi-Coder-9B-Chat\n",
        "    # 'microsoft/Phi-3-mini-4k-instruct', #6 mins in RTX3090\n",
        "    # \"google/gemma-2-9b-it\", #More than 20GB of GPU memory needed - Works with A40 with 48GB VRAM (8mins), but not with A4500 - 20GB, and V100 - 32GB, 4.5mins in Delft Blue\n",
        "    # 'mistralai/Mistral-Nemo-Instruct-2407', #12B parameters, 11mins in 2 RTX3090, 16mins in V100 with 32GB VRAM (48mins to run over all 44 Q&A pairs)\n",
        "    # 'openai/gpt-4o-mini' #Costs very low ~0.01$ for 9 Q&A pairs.\n",
        "    ] #All above models need ~130GB space, the last needs ~30GB . For 44 Q&A pairs it takes ~50min/model\n",
        "\n",
        "# Groq models are defined as: groq_website/model_name e.g. 'groq_website/llama-3.1-70b-versatile'\n",
        "# OpenAI models are defined as: 'openai/model_name', e.g. 'openai/gpt-4o-mini'\n",
        "# Anthropic models are defined as 'anthropic/model_name', e.g. 'anthropic/claude-3-haiku-20240307' - Couldn't use due to billing issues\n",
        "\n",
        "# I couldn't run 'nvidia/Mistral-NeMo-Minitron-8B-Base', \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" (Conflicting dependencies),\n",
        "# 'google/recurrentgemma-9b-it' # RecurrentGemmaForCausalLM.forward() got an unexpected keyword argument 'position_ids'\n",
        "\n",
        "#Takes 6h for 25Q&A pairs with beam search (n=5) in V100 32GB (costs ~3$) for all models below (except Gemma) - Large models take more time (2min/generation for Mistral 12B)\n",
        "\n",
        "#Define model to act as a judge\n",
        "judge_model='openai/gpt-4o-mini' #If used with Llama, only 0.01$ for 9 Q&A pairs for gpt-4o-mini, and 0.22$ for gpt-4o\n",
        "\n",
        "#Define maximum number of tokes in the judge LLM output\n",
        "max_output_tokens=500\n",
        "\n",
        "#Limit of tokens in the generated response from LLM\n",
        "generate_max_tokens=1000\n",
        "\n",
        "#Inference on whole dataset?\n",
        "inference_on_whole_dataset=True\n",
        "\n",
        "#Number of times to resample the dataset\n",
        "n_resamples=5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define prompts for custom evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_prompt=\"\"\"\n",
        "You are an autoregressive language model that acts as a judge in comparing a predicted vs an actual answer to a questions.\n",
        "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend \n",
        "a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. \n",
        "Your users are experts in chemical engineering, so they already know you're a language model and your capabilities and limitations, so don't \n",
        "remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. \n",
        "Don't be verbose in your answers, but do provide details and examples where it might help the explanation. \n",
        "\"\"\" #This is common for all prompts below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to completeness compared to the completeness of a given actual, golden standard answer. \n",
        "The completeness metric evaluates the extent to which the user's question is answered in full in the predicted response. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: There is no response.\n",
        "2: No parts of a suitable answer are present.\n",
        "3: Few elements of a complete answer are present.\n",
        "4: Most elements of a complete answer are present.\n",
        "5: The response covers all elements of a complete answer.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "relevance_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to relevance compared to the relevance of a given actual, golden standard answer. \n",
        "The relevance metric evaluates the amount of irrelevant information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response answers something else, not the user's question.\n",
        "2: The response answers the user's question but the information provided is mostly irrelevant.\n",
        "3: The response answers the user's question but contains more irrelevant information than relevant information.\n",
        "4: The response answers the user's question, and shares a bit of irrelevant information.\n",
        "5: The response answers the user's question and contains no irrelevant information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "conciseness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to conciseness compared to the conciseness of a given actual, golden standard answer. \n",
        "The conciseness metric evaluates the amount of unexpected extra information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response is too long and stops before completion or enters an infinite loop.\n",
        "2: The response includes a lot of extra information and uses flowery language.\n",
        "3: The response includes a lot of extra information or uses flowery language.\n",
        "4: The response is short and includes a small amount of extra information.\n",
        "4: The response is as short as possible while still answering the prompt.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "confidence_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to confidence compared to the confidence of a given actual, golden standard answer. \n",
        "The condifence metric evaluates the degree of assurance that is conveyed the response that the predicted answer is correct. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: Complete Rejection. The response makes it clear that the given answer is incorrect or that no correct answer can be provided.\n",
        "2: Doubt and Disagreement. The response suggests that the answer is likely incorrect or raises significant concerns.\n",
        "3: Uncertainty. The response indicates that the answer could be correct, but there is significant doubt or insufficient evidence.\n",
        "4: Moderate Agreement. The response leans towards the answer being correct but acknowledges some uncertainty.\n",
        "5: Full Endorsement. The reponse confidentely asserts that the given answer is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "factuality_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to factuality compared to the factuality of a given actual, golden standard answer.\n",
        " The factuality metric evaluates the degree of hallucination contained in a response or, in other words, how accurate a given response is.\n",
        "You can assign a score from 1 to 5, with the following interpretations:\n",
        "1: The response is a complete hallucination\n",
        "2: The response is mostly a hallucination but does not change key information from the prompt (such as chemical identifiers).\n",
        "3: The response contains large amounts of both hallucinations and factual information.\n",
        "4: The response includes mostly factual information with slight hallucinations.\n",
        "5: The response only includes factual information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "judgement_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to judgement compared to the judgement of a given actual, golden standard answer.\n",
        "The judgment metric assesses how strongly the response implies its correctness, taking into account the actual accuracy of the answer.\n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response confidently claims a hallucination as truth.\n",
        "2: The response misinterprets information received in the prompt.\n",
        "3: The response shows that the model is unsure about the answer or states that information is theoretical.\n",
        "4: The response is wrong but it is made clear that the answer is wrong or that the model is unable to provide a correct answer.\n",
        "5: The response is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#How the dataset will be named in Langsmith\n",
        "def get_dataset_name(model_name, judge_model):\n",
        "    try: #For Hugging Face models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name.split('/')[1]+'_with_judge_'+judge_model+'_beam_search_statistics'\n",
        "    except: #For OpenAI models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name+'_with_judge_'+judge_model+'_beam_search_statistics'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX9V2ASWCQG5"
      },
      "source": [
        "Google Drive mount (If run in Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLsdaFvRthOE",
        "outputId": "ee976853-2292-4eee-a380-812283627e56"
      },
      "outputs": [],
      "source": [
        "if 'content/drive/My Drive' in file_path:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lVqBHaT2s6Aq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "qa=pd.read_excel(file_path) #Read Excel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6zdJxKCubI"
      },
      "source": [
        "Create Dataset from df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oUw8Puxfs6Az"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "loaded_dataset=Dataset.from_pandas(qa)\n",
        "\n",
        "if inference_on_whole_dataset==False:\n",
        "    loaded_dataset = loaded_dataset.train_test_split(test_size=0.2, seed=42) #Used if going to fine-tune in part of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vf6thikds6A1"
      },
      "outputs": [],
      "source": [
        "if inference_on_whole_dataset==False:\n",
        "    dataset_train=loaded_dataset['train']\n",
        "    dataset_test=loaded_dataset['test']\n",
        "else:\n",
        "    dataset_test=loaded_dataset #When we use the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXxkzQoHs6A5"
      },
      "source": [
        "Create Langsmith Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtdIrA3Ds6A8",
        "outputId": "90a9b4dd-e91a-4773-934b-2bf58cd8e3a8"
      },
      "outputs": [],
      "source": [
        "#https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets\n",
        "\n",
        "from langsmith import Client\n",
        "\n",
        "example_inputs = [(x['input'],x['output']) for x in dataset_test]\n",
        "print(example_inputs)\n",
        "\n",
        "def create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key):\n",
        "\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    try:\n",
        "        #Load the dataset if already exists\n",
        "        for existing_dataset in client.list_datasets():\n",
        "            if existing_dataset.name==dataset_name:\n",
        "                dataset_langsmith=existing_dataset\n",
        "        for x in dataset_langsmith:\n",
        "            print(\"Dataset Loaded\")\n",
        "            break\n",
        "\n",
        "    except: #Otherwise create it\n",
        "        print(\"Dataset not found. Creating new dataset\")\n",
        "        # Storing inputs in a dataset lets us run chains and LLMs over a shared set of examples.\n",
        "        dataset_langsmith = client.create_dataset(dataset_name=dataset_name,\n",
        "                                                description=\"Q&A chemical engineering.\")\n",
        "\n",
        "        for input_prompt, output_answer in example_inputs:\n",
        "            client.create_example(\n",
        "                inputs={\"question\": input_prompt.replace('\\n', ' ')},\n",
        "                outputs={\"answer\": output_answer.replace('\\n', ' ')},\n",
        "                # metadata={\"source\": \"Wikipedia\"},\n",
        "                dataset_id=dataset_langsmith.id,\n",
        "            )\n",
        "\n",
        "    return dataset_langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/cookbook/introduction\n",
        "# https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators\n",
        "# https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator\n",
        "\n",
        "from langsmith.schemas import Run, Example\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from termcolor import colored\n",
        "\n",
        "list_of_metrics=['completeness_descr','relevance_descr','conciseness_descr','confidence_descr','factuality_descr','judgement_descr']\n",
        "\n",
        "#Function that compares the real answer with the predicted answer of an LLM and returns a score based on the evaluation\n",
        "def factor_evaluator(run: Run, example: Example) -> dict: \n",
        "    # print(\"Run:\",run)\n",
        "\n",
        "    question=run.inputs.get(\"inputs\")['question']\n",
        "    # print(\"Question:\",question)\n",
        "    actual_answer = example.outputs.get(\"answer\")\n",
        "    # print(\"Real answer:\",example.outputs.get(\"answer\"))\n",
        "    predicted_answer = run.outputs.get(\"output\")\n",
        "    # print(\"Predicted Answer:\",answer)\n",
        "    \n",
        "    # Check if there is output from LLM\n",
        "    if not predicted_answer:\n",
        "        print(\"No output from LLM\")\n",
        "        return {\"key\": \"custom_metric\" , \"score\": 0} \n",
        "    \n",
        "    else:\n",
        "        scores={} #Store scores for each metric\n",
        "        descriptions={} #Store descriptions for each metric\n",
        "        \n",
        "        for metric_name in list_of_metrics: #Iterate through all metrics\n",
        "            print(\"Evaluating based on:\",metric_name)\n",
        "            metric_value=common_prompt+eval(metric_name) #Get the actual description of the metric\n",
        "\n",
        "            # Define roles and placeholders\n",
        "            chat_template = ChatPromptTemplate.from_messages(\n",
        "            [(\"system\", metric_value),\n",
        "                (\"user\", \"Question: {question}, Actual answer: {actual_answer}, Predicted answer: {predicted_answer}\"),\n",
        "                # (\"ai\", \"It's sunny and warm outside.\"), #Use this if we want to use few shot prompts\n",
        "            ]\n",
        "            )\n",
        "\n",
        "            messages = chat_template.format_messages(question=question, actual_answer=actual_answer, predicted_answer=predicted_answer)\n",
        "            # print(\"Messages:\",messages)\n",
        "\n",
        "            formatted_messages = [(role, msg.content) for role, msg in zip([\"system\", \"user\"], messages)]\n",
        "            # print(\"Formatted messages:\",formatted_messages) #[('system', 'You are an autoregressive lan....', 'user':.....)]\n",
        "\n",
        "            # Initialize the model and get response\n",
        "            llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0, max_tokens=max_output_tokens, seed=42)\n",
        "            ai_response = llm.invoke(formatted_messages)\n",
        "\n",
        "            # Output\n",
        "            # print(colored(\"System message:\"+ messages[0].content,'blue'))\n",
        "            print(colored(\"User message:\"+ messages[1].content, 'green'))\n",
        "            print(colored(\"AI message:\"+ ai_response.content,'red'))\n",
        "\n",
        "            #Decide what the final score is based on output\n",
        "            if \"FINAL SCORE:\" in ai_response.content: \n",
        "                score = int(ai_response.content.split(\"FINAL SCORE:\")[1])\n",
        "            else:\n",
        "                print(\"Invalid response from LLM:\", ai_response.content)\n",
        "                score = 0\n",
        "\n",
        "            scores[metric_name]=score\n",
        "            descriptions[metric_name]=ai_response.content\n",
        "            print(\"Scores:\",scores)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"results\":[ #We always need 'key', 'score' pairs\n",
        "            {\"key\": \"completeness\" , \"score\": scores['completeness_descr'],\"value\":descriptions['completeness_descr']},\n",
        "            {\"key\": \"relevance\" , \"score\": scores['relevance_descr'], \"value\":descriptions['relevance_descr']},\n",
        "            {\"key\": \"conciseness\" , \"score\": scores['conciseness_descr'], \"value\":descriptions['conciseness_descr']},\n",
        "            {\"key\": \"confidence\" , \"score\": scores['confidence_descr'], \"value\":descriptions['confidence_descr']},\n",
        "            {\"key\": \"factuality\" , \"score\": scores['factuality_descr'], \"value\":descriptions['factuality_descr']},\n",
        "            {\"key\": \"judgement\" , \"score\": scores['judgement_descr'], \"value\":descriptions['judgement_descr']}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Models that Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.random.manual_seed(0) #Set for reproducibility\n",
        "\n",
        "def initialize_model(model_id):\n",
        "    # # Check if mps acceleration is available (For MacOS)\n",
        "    # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    # print(f\"Using device {device}\")\n",
        "    # model.to(device)\n",
        "\n",
        "    # transformers.set_seed(42) #Tried for reproducibility but didn't work\n",
        "    \n",
        "    pipeline = transformers.pipeline( \n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\":cache_dir},\n",
        "            # trust_remote_code=True,\n",
        "            device_map=\"auto\" #Use 'cuda' if one GPU available (works in Delft Blue with 32GB VRAM) - 'auto' the alternative for distributed over all available GPUs\n",
        "        )\n",
        "    return pipeline\n",
        "\n",
        "def get_model(model_id):\n",
        "    \"\"\"Given a model name, return the loaded model, tokenizer, and pipeline\"\"\"\n",
        "\n",
        "    if 'openai' not in model_id and 'groq_website' not in model_id: #For Hugging Face models\n",
        "        pipeline=initialize_model(model_id)\n",
        "\n",
        "    #Returns below variables if defined, and returns None for any that are not.\n",
        "    model = locals().get('model', None)\n",
        "    tokenizer = locals().get('tokenizer', None)\n",
        "    pipeline = locals().get('pipeline', None)\n",
        "\n",
        "    return model, tokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def predict(inputs: dict) -> dict:\n",
        "    \"\"\"Given a question, return the answer from the model\"\"\"\n",
        "    \n",
        "    #Get these variables from the global scope\n",
        "    global model_name\n",
        "    \n",
        "    messages = [ #Only use the questions to ask the model to generate the response\n",
        "      {\"role\": \"user\", \"content\": inputs['question']},\n",
        "    ]\n",
        "\n",
        "    if 'gemma' not in model_name: #Gemma doesn't support system message\n",
        "      messages.insert(0, {\"role\": \"system\", \"content\": \"You are a language model specialized in chemical engineering. Answer the following question:\"})\n",
        "    else: #For gemma add system prompt in user message\n",
        "      messages[0]['content']=\"You are a language model specialized in chemical engineering. Answer the following question: \" + messages[0]['content']\n",
        "    # print(\"Prompt:\",messages)\n",
        "\n",
        "    generation_args = { \n",
        "        \"max_new_tokens\": max_output_tokens, \n",
        "        \"return_full_text\": False, \n",
        "        \"temperature\": 0.1, # 1e-8,  #Has to be positive number - not considered from model when do_sample is False (reproducible results)\n",
        "        \"do_sample\": True, #Selects highest probability token if sets to False\n",
        "        \"num_beams\" : 5, #3 can also work if computationally intensive - more info on https://huggingface.co/blog/how-to-generate\n",
        "        #Warnings will be raised by some models\n",
        "\n",
        "        #If we only set temp!=0 or if we also set do_sample=False then warning: `do_sample` is set to `False`. However, `temperature` is set to `1e-08` \n",
        "        # -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
        "        # That means that the temperature is probably ignored\n",
        "        # Sometimes, results not reproducible if only temp is set\n",
        "      } \n",
        "    \n",
        "    if 'openai' not in model_name and 'groq_website' not in model_name: #For Hugging Face models\n",
        "      response=pipeline(messages, **generation_args)[0]['generated_text']\n",
        "      print(model_name,':',response)\n",
        "\n",
        "    else: \n",
        "      if 'openai' in model_name:\n",
        "        try:\n",
        "          import openai\n",
        "          from langsmith.wrappers import wrap_openai\n",
        "                  \n",
        "          # Define OpenAI client\n",
        "          openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "          \n",
        "          response = openai_client.chat.completions.create(messages=messages, temperature=0, model=model_name.split('/')[1],  seed=42) \n",
        "          # print(\"Response:\",response.choices[0].message.content)\n",
        "          response=response.choices[0].message.content #That's the response without formatting\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"OpenAI Model ID:\",model_name)\n",
        "\n",
        "      elif 'groq_website' in model_name:\n",
        "        try:\n",
        "          from groq import Groq\n",
        "          client = Groq()\n",
        "          actual_model_name=model_name.split('/')[1]\n",
        "          response = client.chat.completions.create(\n",
        "              model=actual_model_name,\n",
        "              max_tokens=generate_max_tokens,\n",
        "              temperature=0,\n",
        "              messages=messages)\n",
        "          # print(\"Response from Groq:\",response.choices[0].message.content)\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"Groq Model ID:\",model_name)\n",
        "\n",
        "    return {\"output\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import numpy as np\n",
        "# from scipy.stats import t\n",
        "# from collections import Counter\n",
        "# for run_idx, metric_values in enumerate(all_runs_model_metrics[model_name]):\n",
        "#     # Extract all values from the metric_values dictionary\n",
        "#     metric_names = [name.replace('_descr', '') for name in metric_values]\n",
        "#     values=list(metric_values.values())\n",
        "#     # Flatten the values list\n",
        "#     values = [val for sublist in values for val in sublist]\n",
        "#     # run_values = list(metric_values.values())\n",
        "#     # print(run_values)\n",
        "#     print(values)\n",
        "#     print(metric_names)\n",
        "#     # print(metric_values)\n",
        "\n",
        "#     # values=list(metric_values.values())\n",
        "\n",
        "\n",
        "\n",
        "#     # # Split values into groups for each metric\n",
        "#     grouped_values = [values[i:i+2] for i in range(0, len(values), 2)]\n",
        "#     print(grouped_values)\n",
        "\n",
        "#     # Create a figure and axis\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "\n",
        "#     # Define colors for each metric\n",
        "#     colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "#     # Plot each metric's values\n",
        "#     for i, (metric, vals) in enumerate(zip(metric_names, grouped_values)):\n",
        "#         # Create a bar for each value\n",
        "#         for j, val in enumerate(vals):\n",
        "#             plt.bar(i + j * 0.2, val, width=0.2, color=colors[i], alpha=0.5, label=metric if j == 0 else \"\")\n",
        "\n",
        "#     # Add labels and title\n",
        "#     plt.xlabel('Metrics')\n",
        "#     plt.ylabel('Score')\n",
        "#     plt.title('Overlapping Bar Plot of Metric Scores')\n",
        "#     plt.xticks(np.arange(len(metric_names)) + 0.1, metric_names)\n",
        "#     plt.legend(title='Metrics')\n",
        "\n",
        "#     # Show the plot\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "not in order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# # Split values into groups for each metric\n",
        "# grouped_values = [values[i:i+2] for i in range(0, len(values), 2)]\n",
        "\n",
        "# # Create a figure and axis\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# # Define colors for each metric\n",
        "# colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "# # Create a dictionary to store all scores and their frequencies for each metric\n",
        "# score_frequencies = {}\n",
        "\n",
        "# # Keep track of which metrics we've added to the legend\n",
        "# legend_added = set()\n",
        "\n",
        "# # Plot each metric's frequency\n",
        "# for i, (metric, vals) in enumerate(zip(metric_names, grouped_values)):\n",
        "#     # Count frequency of each score\n",
        "#     counts = Counter(vals)\n",
        "    \n",
        "#     # For each score in this metric\n",
        "#     for score, freq in counts.items():\n",
        "#         # Plot bar at this score\n",
        "#         plt.bar(score, freq,\n",
        "#                 width=0.4,  # Made bars thinner\n",
        "#                 color=colors[i],\n",
        "#                 alpha=0.5,\n",
        "#                 label=metric if metric not in legend_added else \"\",  # Only add to legend once\n",
        "#                 bottom=0 if score not in score_frequencies else score_frequencies[score])\n",
        "        \n",
        "#         # Update the total height for this score\n",
        "#         if score in score_frequencies:\n",
        "#             score_frequencies[score] += freq\n",
        "#         else:\n",
        "#             score_frequencies[score] = freq\n",
        "        \n",
        "#         # Mark this metric as added to the legend\n",
        "#         legend_added.add(metric)\n",
        "\n",
        "# # Add labels and title\n",
        "# plt.xlabel('Score')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.title('Distribution of Scores by Metric')\n",
        "# plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# # Set x-axis ticks\n",
        "# plt.xticks(np.arange(1, 6))\n",
        "\n",
        "# # Adjust layout to prevent legend cutoff\n",
        "# plt.tight_layout()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "good stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# from collections import Counter\n",
        "\n",
        "# # Values and metric names\n",
        "# values = [4, 3, 4, 3, 3, 3, 5, 3, 5, 3, 5, 3]\n",
        "# metric_names = ['completeness', 'relevance', 'conciseness', 'confidence', 'factuality', 'judgement']\n",
        "\n",
        "# # Split values into groups for each metric\n",
        "# grouped_values = [values[i:i+2] for i in range(0, len(values), 2)]\n",
        "\n",
        "# # Create a figure and axis\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# # Define colors for each metric\n",
        "# colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "# # First count all frequencies per score per metric\n",
        "# score_metric_counts = {}\n",
        "# for i, (metric, vals) in enumerate(zip(metric_names, grouped_values)):\n",
        "#     counts = Counter(vals)\n",
        "#     for score, freq in counts.items():\n",
        "#         if score not in score_metric_counts:\n",
        "#             score_metric_counts[score] = {}\n",
        "#         score_metric_counts[score][metric] = freq\n",
        "\n",
        "# # Keep track of which metrics we've added to the legend\n",
        "# legend_added = set()\n",
        "\n",
        "# # For each score, plot metrics in order of frequency (highest frequency at bottom)\n",
        "# for score in sorted(score_metric_counts.keys()):\n",
        "#     # Sort metrics by frequency for this score\n",
        "#     sorted_metrics = sorted(score_metric_counts[score].items(), \n",
        "#                           key=lambda x: x[1], \n",
        "#                           reverse=True)  # highest frequency first\n",
        "    \n",
        "#     bottom = 0\n",
        "#     for metric, freq in sorted_metrics:\n",
        "#         i = metric_names.index(metric)  # get index for color\n",
        "#         plt.bar(score, freq,\n",
        "#                 width=0.4,\n",
        "#                 color=colors[i],\n",
        "#                 alpha=0.5,\n",
        "#                 label=metric if metric not in legend_added else \"\",\n",
        "#                 bottom=bottom)\n",
        "#         bottom += freq\n",
        "#         legend_added.add(metric)\n",
        "\n",
        "# plt.xlabel('Score')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.title('Distribution of Scores by Metric')\n",
        "# plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "# plt.xticks(np.arange(1, 6))\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_figures_metrics(all_runs_model_metrics, metric_names, model_name, judge_model): #Plot figures with distributions of metrics\n",
        "\n",
        "    \"\"\"\n",
        "    Plot figures showing distributions of evaluation metrics across multiple runs.\n",
        "\n",
        "    Args:\n",
        "        all_runs_model_metrics (dict): Dictionary containing metrics for all runs, organized by model\n",
        "        metric_names (list): List of metric names to plot\n",
        "        model_name (str): Name of the model being evaluated\n",
        "        judge_model (str): Name of the model used for judging/evaluation\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary statistics for all runs\n",
        "\n",
        "    This function creates visualizations of metric distributions using separate subplots for each metric.\n",
        "    It calculates and plots histograms, means, and confidence intervals for the metric scores across \n",
        "    multiple evaluation runs. The function returns summary statistics including means and confidence \n",
        "    intervals for each metric.\n",
        "    \"\"\"\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    from scipy.stats import t\n",
        "    from collections import Counter\n",
        "\n",
        "    summary_stats_all_runs={}\n",
        "\n",
        "    for run_idx, metric_values in enumerate(all_runs_model_metrics[model_name]):\n",
        "\n",
        "        # Colors for separate plots\n",
        "        colors = sns.color_palette(\"Set3\", len(metric_names))\n",
        "\n",
        "        # Create two figures - one with separate subplots and one overlaid\n",
        "        fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 18))        \n",
        "        plt.subplots_adjust(hspace=0.6, top=0.94)\n",
        "\n",
        "        # Set titles\n",
        "        fig.suptitle(f'Metric Distributions for {model_name}', fontsize=16)\n",
        "\n",
        "        # Define the bin edges explicitly to ensure consistency\n",
        "        bin_edges = np.arange(0.0, 5.6, 0.2)  # Adjust to cover the range 1-5 with bins of width 1\n",
        "\n",
        "        # Extract all values from the metric_values dictionary\n",
        "        metric_names = [name.replace('_descr', '') for name in metric_values]\n",
        "\n",
        "        #Summary statistics over one run\n",
        "        summary_stats_run={}\n",
        "\n",
        "        for i, (metric_name, values) in enumerate(metric_values.items()):\n",
        "\n",
        "            # Remove _descr from metric name if present\n",
        "            metric_name_loop = metric_name.replace('_descr', '') #This is over one run and over one metric (but over all questions)\n",
        "            metric_name=metric_names[i]\n",
        "            assert metric_name_loop==metric_name, \"Metric name mismatch\"\n",
        "\n",
        "            mean_value=np.mean(values) #Mean of the metric over single run and over single metric (but over all questions)\n",
        "            std_errors=np.std(values, ddof=1)/np.sqrt(len(values)) # ddof=1 to divide by n-1 to calculate the sample standard deviation, default (ddof=0) calculates the population sd\n",
        "\n",
        "            #JUST FOR CONFRIMATION\n",
        "            assert np.std(values, ddof=1)==np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), \"Standard deviation calculation mismatch\"\n",
        "            # print(\"Standard deviation calculation confirmed\", np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), np.std(values, ddof=1)) #works\n",
        "\n",
        "            # Plot on individual subplot\n",
        "            sns.histplot(values, bins=bin_edges, color=colors[i], ax=axes[i], kde=False)\n",
        "            \n",
        "            # Calculate confidence intervals - didn't use t_critical=t.ppf(0.975, df=len(values)-1) since we're using sample standard deviation\n",
        "            margin_of_error = 1.96 * std_errors\n",
        "            ci_low = mean_value - margin_of_error\n",
        "            ci_high = mean_value + margin_of_error\n",
        "\n",
        "            # Add error bar to show confidence interval on individual plots\n",
        "            # axes[i].errorbar(mean_value, axes[i].get_ylim()[1]/2, \n",
        "            #                 xerr=margin_of_error,\n",
        "            #                 color='black',\n",
        "            #                 capsize=5,\n",
        "            #                 capthick=1,\n",
        "            #                 elinewidth=2,\n",
        "            #                 marker='o')\n",
        "            # Create error bar object but don't add it yet\n",
        "\n",
        "            # Store error bar parameters for this metric\n",
        "            if i == 0:\n",
        "                error_bars = []\n",
        "            error_bars.append((mean_value, axes[i].get_ylim()[1]/2, margin_of_error))\n",
        "\n",
        "            summary_stats_run[metric_name]={\"mean\":mean_value,\"std_error\":std_errors,\"ci_low\":ci_low,\"ci_high\":ci_high}\n",
        "            axes[i].set_title(f'{metric_name} (Mean: {np.mean(mean_value):.2f} Â± {np.mean(std_errors):.2f} SE, CI: {ci_low:.2f}-{ci_high:.2f})')\n",
        "            axes[i].set_xlim(0, 5.5) #Keep 0 in case of errors\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].set_yticks(range(0, 11, 5)) #Set y-ticks at intervals of 5\n",
        "\n",
        "            # Hide x-axis labels and ticks for all but the last subplot\n",
        "            if i < len(metric_names) - 1:\n",
        "                axes[i].set_xlabel('')\n",
        "            else:\n",
        "                axes[i].set_xlabel('Values')\n",
        "\n",
        "\n",
        "\n",
        "        # Save version without error bars\n",
        "        plt.figure(fig.number)\n",
        "        plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions_no_error_bars.png')\n",
        "\n",
        "        # Add error bars and save version with them\n",
        "        for i, error_bar in enumerate(error_bars):\n",
        "            mean, ylim, margin = error_bar\n",
        "            axes[i].errorbar(mean, ylim,\n",
        "                           xerr=margin,\n",
        "                           color='black',\n",
        "                           capsize=5,\n",
        "                           capthick=1,\n",
        "                           elinewidth=2,\n",
        "                           marker='o')\n",
        "\n",
        "        plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions.png')\n",
        "        \n",
        "        plt.close('all')\n",
        "\n",
        "            # Print summary statistics\n",
        "        print(\"\\nSummary Statistics over run {run_idx}:\")\n",
        "        print(\"-\" * 50)\n",
        "        for metric, stats in summary_stats_run.items():\n",
        "            print(f\"{metric}:\")\n",
        "            print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "            print(f\"  Standard Error: {stats['std_error']:.2f}\")\n",
        "            print(f\"  CI: {stats['ci_low']:.2f}-{stats['ci_high']:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        summary_stats_all_runs[run_idx]=summary_stats_run\n",
        "\n",
        "        values=list(metric_values.values())\n",
        "        values = [val for sublist in values for val in sublist] #Flatten the values list\n",
        "\n",
        "        #  Split values into groups for each metric\n",
        "        grouped_values = [values[i:i+2] for i in range(0, len(values), 2)] #List of lists, each sublist is a metric's values for one run over all questions\n",
        "\n",
        "        # Create a figure and axis\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Define colors for each metric\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "        # First count all frequencies per score per metric\n",
        "        score_metric_counts = {}\n",
        "\n",
        "        # Plot each metric's values\n",
        "        for i, (metric, vals) in enumerate(zip(metric_names, grouped_values)):\n",
        "\n",
        "            width = 0.8 / len(vals)\n",
        "            # Create a bar for each value\n",
        "            for j, val in enumerate(vals):\n",
        "                plt.bar(i + j * width, val, width=width, color=colors[i], alpha=0.5, label=metric if j == 0 else \"\")\n",
        "                # i is the index of metric and determines the base position of a group of bars corresponding to that metric.\n",
        "                # j*width adds an offset to the base position to separate individual bars within the same group. \n",
        "                # Each j corresponds to a different value in vals, creating distinct bars for the values of the same metric.\n",
        "                # By combining the above two, we get the exact x-position of a specific bar\n",
        "\n",
        "            #Below for the distribution overlay plot\n",
        "            counts = Counter(vals) #Count the frequency of each score in vals\n",
        "            for score, freq in counts.items():\n",
        "                if score not in score_metric_counts:\n",
        "                    score_metric_counts[score] = {}\n",
        "                score_metric_counts[score][metric] = freq #Keeps track of how many times each metric gets a specific score over all questions\n",
        "                # {4: {'completeness': 1, 'confidence': 1, 'factuality': 1, 'judgement': 1}, 3: {'completeness': 1, 'relevance': 2, 'conciseness': 2, ....}\n",
        "            \n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Metrics')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Overlapping Bar Plot of Metric Scores')\n",
        "        plt.xticks(np.arange(len(metric_names)) + 0.1, metric_names)\n",
        "        plt.yticks(range(6))  # Set y-axis ticks to integers from 0 to 5\n",
        "        plt.figure(fig.number)\n",
        "        plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions_stacked.png')\n",
        "        plt.close('all')\n",
        "\n",
        "        # Keep track of which metrics we've added to the legend\n",
        "        legend_added = set()\n",
        "\n",
        "        # For each score, plot metrics in order of frequency (highest frequency at bottom)\n",
        "        for score in sorted(score_metric_counts.keys()):\n",
        "            # Sort metrics by frequency for this score\n",
        "            sorted_metrics = sorted(score_metric_counts[score].items(), \n",
        "                                key=lambda x: x[1], \n",
        "                                reverse=True)  # highest frequency first\n",
        "            \n",
        "            bottom = 0\n",
        "            for metric, freq in sorted_metrics:\n",
        "                i = metric_names.index(metric)  # get index for color\n",
        "                plt.bar(score, freq,\n",
        "                        width=0.4,\n",
        "                        color=colors[i],\n",
        "                        alpha=0.5,\n",
        "                        label=metric if metric not in legend_added else \"\",\n",
        "                        bottom=bottom)\n",
        "                bottom += freq\n",
        "                legend_added.add(metric)\n",
        "\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Distribution of Scores by Metric')\n",
        "        plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.xticks(np.arange(1, 6))\n",
        "        plt.tight_layout()\n",
        "        plt.figure(fig.number)\n",
        "        plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_run_'+str(run_idx)+'_metric_distributions_overlay.png')\n",
        "        plt.close('all')\n",
        "\n",
        "        #loop over metric_values here to get average of each question metric over all runs\n",
        "\n",
        "    return summary_stats_all_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform the Evaluation over all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "# Keep track of all model stats\n",
        "all_model_stats = {} #Used in comparison between models\n",
        "\n",
        "# metrics_models={}\n",
        "\n",
        "all_runs_model_metrics={} #Used in plotting metrics\n",
        "\n",
        "#Initialize models\n",
        "for model_id in models:\n",
        "    \n",
        "    dataset_name=get_dataset_name(model_id, judge_model) #How the dataset will be named in Langsmith\n",
        "    dataset_langsmith=create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key)\n",
        "    model, tokenizer, pipeline = get_model(model_id)\n",
        "    # print(f\"\\nEvaluating model: {model_id}\")\n",
        "    model_name=model_id #Since model_name defined as global variable\n",
        "\n",
        "    # Evaluation\n",
        "    begin=time.time()\n",
        "    all_evaluation_results=[] #Used below to obtain the unique questions/answers\n",
        "\n",
        "    for i in range(n_resamples):\n",
        "        print(f\"\\nPerforming evaluation resample {i+1}/{n_resamples} of {model_id}\")\n",
        "   \n",
        "        evaluation_results=evaluate(\n",
        "            predict, #Function that call our LLM and returns its output\n",
        "            data=dataset_langsmith.name, #Just using dataset_langsmith doesn't work \n",
        "            evaluators=[factor_evaluator], #Evaluators to use\n",
        "            # metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
        "            experiment_prefix=str(judge_model)+'_judge_with_'+str(model_id)+'_resample_'+str(i) # A prefix for your experiment names to easily identify them\n",
        "        )\n",
        "\n",
        "        all_evaluation_results.extend(evaluation_results) #Used below to get unique questions/answers and to select the predicted answers\n",
        "\n",
        "    end=time.time()\n",
        "    print(\"Total time taken:\",end-begin)\n",
        "\n",
        "    chunk_size = len(example_inputs) #Number of questions\n",
        "\n",
        "    try: #Sometimes some errors with 1+ Q&A missing\n",
        "\n",
        "        #Extract metrics and save to df\n",
        "        #Initialize empty df to be filled with results\n",
        "        results_df=pd.DataFrame()\n",
        "\n",
        "        #https://docs.smith.langchain.com/tutorials/Developers/evaluation\n",
        "        # Get unique questions/answers (take only first resample since they're repeated)\n",
        "        unique_results = all_evaluation_results[:chunk_size] #Includes the questions and actual answers of one resample only (same for the others)\n",
        "        list_of_questions = [x['example'].inputs['question'] for x in unique_results]\n",
        "        list_of_answers = [x['example'].outputs['answer'] for x in unique_results]\n",
        "\n",
        "        # Add base columns\n",
        "        results_df['questions'] = list_of_questions\n",
        "        results_df['answers'] = list_of_answers\n",
        "\n",
        "        all_metrics=[] #Keep track of all metrics over all resamples and all questions\n",
        "\n",
        "        all_runs_metric_scores=[] #This will be appended to the input that plots metrics at the end\n",
        "        # Create columns for each resample's predicted answers and metrics\n",
        "        for i in range(n_resamples):\n",
        "            start_idx = i * chunk_size\n",
        "            resample_results = all_evaluation_results[start_idx:start_idx + chunk_size]\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results]\n",
        "            metrics = [x['evaluation_results']['results'] for x in resample_results if x['run'].outputs['output'] is not None] #CONDITION COPIED FROM BELOW\n",
        "            #A list with the following format: [EvaluationResult(key='completeness', score=4, value='To evaluate the ....\n",
        "            \n",
        "            # Add predicted answers and metrics side by side\n",
        "            results_df[f'predicted_answer_{i+1}'] = predicted_answers\n",
        "\n",
        "            all_metrics.append(metrics) #In each iteration we append the metrics (6 in total) of one resample for all questions - n at the end, one for each resample\n",
        "\n",
        "            individual_run_metric_scores={}\n",
        "\n",
        "            # Add metrics and evaluation prompts with their corresponding predicted answers\n",
        "            for metric_idx, metric_name in enumerate(list_of_metrics): #Get specific metric name and values over all questions for the current resample\n",
        "                # print(\"list_of_metrics\",list_of_metrics)\n",
        "                # print(\"metric keys\",[m[metric_idx].key for m in metrics])\n",
        "\n",
        "                # print(\"metrics\",metrics)\n",
        "                print(\"metric_keys[0]\",[m[metric_idx].key for m in metrics][0])\n",
        "                print(\"metric_name\",metric_name)\n",
        "                metric_name_loop=[m[metric_idx].key for m in metrics][0].replace('_descr','')\n",
        "                assert metric_name_loop==metric_name, \"Metric keys mismatch\"\n",
        "\n",
        "                metric_scores = [m[metric_idx].score for m in metrics] #Scores of a given metric over all question for a given resample\n",
        "                metric_prompts = [m[metric_idx].value for m in metrics]\n",
        "                results_df[f'metric_{metric_name}_{i+1}'] = metric_scores\n",
        "                results_df[f'prompt_{metric_name}_{i+1}'] = metric_prompts\n",
        "                individual_run_metric_scores[metric_name]=metric_scores #Keep track of scores of a given metric over all questions for all resamples - len should be 5*\n",
        "\n",
        "            all_runs_metric_scores.append(individual_run_metric_scores)\n",
        "\n",
        "        all_runs_model_metrics[model_id]=all_runs_metric_scores #Used in plotting metrics\n",
        "        # Dictionary in format {model_id:[{metric_1_run_1:[values], metric_2_run_2:[values], ...}, {metric_1_run_2:[values]....}]\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Handle NaN values across all resamples\n",
        "            nan_rows = []\n",
        "            for i in range(n_resamples):\n",
        "                for metric in list_of_metrics:\n",
        "                    metric_col = f'metric_{metric}_{i+1}'\n",
        "                    nan_indices = results_df[metric_col].isna()\n",
        "                    if nan_indices.any():\n",
        "                        for idx in nan_indices[nan_indices].index:\n",
        "                            print(colored(f\"Missing value for metric '{metric}' in resample {i+1}\", 'red'))\n",
        "                            print(colored(f\"Question: {results_df.loc[idx, 'questions']}\", 'green'))\n",
        "                            nan_rows.append(idx)\n",
        "            \n",
        "            nan_rows = list(set(nan_rows))  # Get unique indices\n",
        "            if nan_rows:\n",
        "                print(colored(f\"ERROR: Found NaN values in {len(nan_rows)} rows out of {len(results_df)}\", 'red'))\n",
        "                print(colored(f\"Row indices with NaN: {nan_rows}\", 'green'))\n",
        "                # results_df = results_df.drop(nan_rows) #Do not drop now since resampling - USE nanmean instead!\n",
        "        except Exception as e:\n",
        "            print(colored(f\"Error handling NaN values: {str(e)}\", 'red'))\n",
        "            \n",
        "\n",
        "\n",
        "        # Save results before processing metrics\n",
        "        results_df.to_excel(f\"results_{judge_model.split('/')[1]}_judge_with_{model_id.replace('/','_')}.xlsx\", index=False)\n",
        "\n",
        "        # Reorganize metrics by type\n",
        "        metric_stats = {metric.replace('_descr', ''): [] for metric in list_of_metrics}\n",
        "        \n",
        "        # print(\"all_metrics\",all_metrics)\n",
        "        for metric_name in list_of_metrics: #IF ABOVE WORKS WITH CORRECT ORDER, THEN THIS WORKS TOO\n",
        "            clean_name = metric_name.replace('_descr', '')\n",
        "            print(\"clean_name\",clean_name)\n",
        "            for resample_metrics in all_metrics:\n",
        "                print(\"resample_metrics\",resample_metrics)\n",
        "                print(\"list_of_metrics.index(metric_name)\",list_of_metrics.index(metric_name))\n",
        "                print([x[list_of_metrics.index(metric_name)] for x in resample_metrics])\n",
        "                scores = [m[list_of_metrics.index(metric_name)].score for m in resample_metrics]\n",
        "                print(\"scores\",scores)\n",
        "                metric_stats[clean_name].extend(scores)\n",
        "                print(\"metric_stats\",metric_stats)\n",
        "\n",
        "        # print(\"metric_statsnew\",metric_stats)\n",
        "\n",
        "        # Create final metric names and values for plotting\n",
        "        metric_names = list(metric_stats.keys()) #Same as list_of_metrics?\n",
        "        # metric_values = [metric_stats[name] for name in metric_names]\n",
        "        print(\"list_of_metrics\",[metric.replace('_descr','') for metric in list_of_metrics])\n",
        "        print(\"metric_names\",metric_names)\n",
        "        metrics_names_loop=[metric.replace('_descr','') for metric in list_of_metrics]\n",
        "        assert metrics_names_loop==metric_names, \"Metric names mismatch\"\n",
        "        # print(\"metric_values\",metric_values) #total len should be num_resamples*num_questions. the first x questions of same resamples, then the next x quesitons of next resample etc. \n",
        "        # format if needed: [[4, 3, 4, 3, 4, 3, 4, 3, 4, 3], [3, 2, 3, 3, 3, 3, 3, 2, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 3, 4, 3, 4, 3, 4, 3, 4, 3],\n",
        "        #  [4, 3, 4, 3, 4, 3, 4, 3, 4, 3], [4, 3, 4, 2, 5, 3, 4, 3, 5, 2]] #List with 6 sublists, each with 10 values (run1_q1, run1_q2, run2_q1, run2_q2, etc)\n",
        "\n",
        "        # Store summary stats for this model\n",
        "        all_model_stats[model_id]= plot_figures_metrics(\n",
        "            all_runs_model_metrics,  \n",
        "            list_of_metrics,#metric_names, \n",
        "            model_id, \n",
        "            judge_model\n",
        "        )\n",
        "        print(\"all_model_stats\",all_model_stats) #Used in model comparison below\n",
        "\n",
        "\n",
        "        # metrics_models[model_id]=all_metrics\n",
        "        # print(\"metrics_models\",metrics_models)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occur in plotting metrics\")\n",
        "        print(e)\n",
        "\n",
        "    # Clear VRAM at the end of each iteration\n",
        "    del model, tokenizer, pipeline\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('-'*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_model_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_runs_model_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistical comparison between models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add this after the evaluation loop to print final comparison\n",
        "def print_final_comparison(all_model_stats):\n",
        "    print(\"\\nFinal Comparison Across All Models:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get all metrics from first model (assuming all models have same metrics)\n",
        "    if not all_model_stats:\n",
        "        print(\"No statistics available\")\n",
        "        return\n",
        "        \n",
        "    metrics = list(next(iter(all_model_stats.values())).keys())\n",
        "    \n",
        "    for metric in metrics:\n",
        "        print(f\"\\n{metric.upper()}:\")\n",
        "        print(\"-\" * 40)\n",
        "        for model, stats in all_model_stats.items():\n",
        "            model_name = model.split('/')[-1]  # Get just the model name without path\n",
        "            mean = stats[metric]['mean']\n",
        "            se = stats[metric]['std_error']\n",
        "            # Calculate 95% confidence interval\n",
        "            ci_lower = mean - 1.96 * se\n",
        "            ci_upper = mean + 1.96 * se\n",
        "            print(f\"{model_name:30} {mean:.2f} Â± {se:.2f} (95% CI: [{ci_lower:.2f}, {ci_upper:.2f}])\")\n",
        "\n",
        "# Print the final comparison\n",
        "print_final_comparison(all_model_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def compare_models(all_results, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Performs statistical comparison between all model pairs.\n",
        "    \n",
        "    Args:\n",
        "        all_results (list): List of evaluation results for each run\n",
        "        alpha (float): Significance level for statistical tests\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing pairwise comparison results\n",
        "    \"\"\"\n",
        "    # Extract model names and scores from the evaluation results\n",
        "    results_by_model = {}\n",
        "    for result in all_results:\n",
        "        # Get model name from the session name - handle case where 'with' is incorrectly extracted\n",
        "        model_name = result['run'].session_name\n",
        "        # Extract the full model name, not just a part\n",
        "        if model_name not in results_by_model:\n",
        "            results_by_model[model_name] = []\n",
        "            \n",
        "        # Extract scores from evaluation results\n",
        "        for eval_result in result['evaluation_results']['results']:\n",
        "            results_by_model[model_name].append({\n",
        "                'metric': eval_result.key,\n",
        "                'score': eval_result.score\n",
        "            })\n",
        "    print(\"results_by_model\",results_by_model)\n",
        "    \n",
        "    # Ensure we have at least 2 models to compare\n",
        "    if len(results_by_model.keys()) < 2:\n",
        "        print(\"Warning: Need at least 2 models to compare. Check session naming convention.\")\n",
        "        return {}\n",
        "        \n",
        "    model_pairs = list(itertools.combinations(results_by_model.keys(), 2))\n",
        "    print(\"model_pairs\",model_pairs)\n",
        "    comparison_results = {}\n",
        "    \n",
        "    for model1, model2 in model_pairs:\n",
        "        comparison_results[f\"{model1}_vs_{model2}\"] = {}\n",
        "        \n",
        "        # Get all unique metrics\n",
        "        metrics = set(r['metric'] for r in results_by_model[model1] + results_by_model[model2])\n",
        "        \n",
        "        for metric in metrics:\n",
        "            # Get scores for both models for this metric\n",
        "            scores1 = [r['score'] for r in results_by_model[model1] if r['metric'] == metric]\n",
        "            scores2 = [r['score'] for r in results_by_model[model2] if r['metric'] == metric]\n",
        "            \n",
        "            if not scores1 or not scores2:\n",
        "                continue\n",
        "                \n",
        "            # Perform t-test\n",
        "            t_stat, p_value = stats.ttest_ind(scores1, scores2)\n",
        "            \n",
        "            # Calculate effect size (Cohen's d)\n",
        "            pooled_std = np.sqrt((np.var(scores1) + np.var(scores2)) / 2)\n",
        "            effect_size = (np.mean(scores1) - np.mean(scores2)) / pooled_std\n",
        "            \n",
        "            comparison_results[f\"{model1}_vs_{model2}\"][metric] = {\n",
        "                \"t_statistic\": t_stat,\n",
        "                \"p_value\": p_value,\n",
        "                \"effect_size\": effect_size,\n",
        "                \"significant\": p_value < alpha,\n",
        "                \"better_model\": model1 if np.mean(scores1) > np.mean(scores2) else model2\n",
        "            }\n",
        "    \n",
        "    return comparison_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform model comparisons\n",
        "comparison_results = compare_models(all_evaluation_results)\n",
        "\n",
        "# Print comparison results\n",
        "print(\"\\nModel Comparison Results:\")\n",
        "for comparison, metrics in comparison_results.items():\n",
        "    print(f\"\\n{comparison}:\")\n",
        "    for metric, results in metrics.items():\n",
        "        print(f\"\\n{metric}:\")\n",
        "        print(f\"  Better model: {results['better_model']}\")\n",
        "        print(f\"  Effect size: {results['effect_size']:.3f}\")\n",
        "        print(f\"  P-value: {results['p_value']:.3f}\")\n",
        "        print(f\"  Significant: {results['significant']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, create a visualization of the comparison\n",
        "def plot_final_comparison(all_model_stats):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    \n",
        "    if not all_model_stats:\n",
        "        return\n",
        "        \n",
        "    metrics = list(next(iter(all_model_stats.values())).keys())\n",
        "    models = list(all_model_stats.keys())\n",
        "    n_metrics = len(metrics)\n",
        "    n_models = len(models)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    x = np.arange(n_metrics)\n",
        "    width = 0.8 / n_models\n",
        "    \n",
        "    for i, model in enumerate(models):\n",
        "        means = [all_model_stats[model][metric]['mean'] for metric in metrics]\n",
        "        errs = [all_model_stats[model][metric]['std_error'] for metric in metrics]\n",
        "        \n",
        "        # Calculate 95% confidence intervals\n",
        "        conf_intervals = []\n",
        "        for mean, err in zip(means, errs):\n",
        "            # For 95% CI, multiply standard error by 1.96\n",
        "            conf_interval = 1.96 * err\n",
        "            conf_intervals.append(conf_interval)\n",
        "        \n",
        "        ax.bar(x + i*width - width*n_models/2, means, width,\n",
        "               label=model.split('/')[-1],\n",
        "               yerr=conf_intervals, capsize=5)\n",
        "        \n",
        "        # Add confidence interval bounds as text\n",
        "        for j, (mean, ci) in enumerate(zip(means, conf_intervals)):\n",
        "            lower = mean - ci\n",
        "            upper = mean + ci\n",
        "            # Print upper limit above error bar\n",
        "            ax.text(x[j] + i*width - width*n_models/2, mean + ci + 0.01,\n",
        "                   f'{upper:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "            # Print lower limit below error bar\n",
        "            ax.text(x[j] + i*width - width*n_models/2, mean - ci - 0.01,\n",
        "                   f'{lower:.3f}', ha='center', va='top', fontsize=8)\n",
        "    \n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Metric Comparison Across Models\\nwith 95% Confidence Intervals')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics, rotation=45)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Create the comparison plot\n",
        "plot_final_comparison(all_model_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_paired_analysis(all_runs_model_metrics):\n",
        "    \"\"\"\n",
        "    Performs paired t-tests between all pairs of models for each metric.\n",
        "    \n",
        "    Args:\n",
        "        all_runs_model_metrics: Dictionary with model IDs as keys and lists of metric scores as values\n",
        "    Returns:\n",
        "        Dictionary containing pairwise comparison results\n",
        "    \"\"\"\n",
        "    from scipy import stats\n",
        "    import numpy as np\n",
        "    \n",
        "    # Store results\n",
        "    comparison_results = {}\n",
        "    \n",
        "    # Get all model pairs\n",
        "    model_pairs = list(itertools.combinations(all_runs_model_metrics.keys(), 2))\n",
        "    \n",
        "    for model1, model2 in model_pairs:\n",
        "        comparison_key = f\"{model1}_vs_{model2}\"\n",
        "        comparison_results[comparison_key] = {}\n",
        "        \n",
        "        # Get metrics for both models\n",
        "        metrics1 = all_runs_model_metrics[model1]\n",
        "        metrics2 = all_runs_model_metrics[model2]\n",
        "        \n",
        "        # For each metric type\n",
        "        for metric_name in list_of_metrics:\n",
        "            metric_name = metric_name.replace('_descr', '')\n",
        "            \n",
        "            # Get all scores for this metric across resamples\n",
        "            scores1 = []\n",
        "            scores2 = []\n",
        "            \n",
        "            # Collect scores from all resamples\n",
        "            for resample_metrics1, resample_metrics2 in zip(metrics1, metrics2):\n",
        "                scores1.extend(resample_metrics1[metric_name + '_descr'])\n",
        "                scores2.extend(resample_metrics2[metric_name + '_descr'])\n",
        "            \n",
        "            # Perform paired t-test\n",
        "            t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
        "            \n",
        "            # Calculate effect size (Cohen's d)\n",
        "            diff = np.array(scores1) - np.array(scores2)\n",
        "            cohens_d = np.mean(diff) / np.std(diff, ddof=1)\n",
        "            \n",
        "            comparison_results[comparison_key][metric_name] = {\n",
        "                \"t_statistic\": t_stat,\n",
        "                \"p_value\": p_value,\n",
        "                \"effect_size\": cohens_d,\n",
        "                \"significant\": p_value < 0.05,\n",
        "                \"better_model\": model1 if np.mean(scores1) > np.mean(scores2) else model2,\n",
        "                \"mean_diff\": np.mean(scores1) - np.mean(scores2)\n",
        "            }\n",
        "            \n",
        "            # Print results\n",
        "            print(f\"\\nResults for {metric_name} - {model1} vs {model2}:\")\n",
        "            print(f\"t-statistic: {t_stat:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4f}\")\n",
        "            print(f\"Cohen's d: {cohens_d:.4f}\")\n",
        "            print(f\"Mean difference: {np.mean(scores1) - np.mean(scores2):.4f}\")\n",
        "            print(f\"Better model: {comparison_results[comparison_key][metric_name]['better_model']}\")\n",
        "            print(f\"Statistically significant: {p_value < 0.05}\")\n",
        "    \n",
        "    return comparison_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After all models have been evaluated\n",
        "print(\"\\nPerforming paired analysis between models...\")\n",
        "paired_analysis_results = perform_paired_analysis(all_runs_model_metrics)\n",
        "\n",
        "# Optionally save results to file\n",
        "import json\n",
        "with open(f'paired_analysis_results_{judge_model.split(\"/\")[1]}.json', 'w') as f:\n",
        "    json.dump(paired_analysis_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tables with num questions, model_ids in columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Report average score per metric and std in parenthesis as percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Power Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "def perform_power_analysis(effect_size=0.5, alpha=0.05, power=0.8):\n",
        "    \"\"\"\n",
        "    Perform power analysis to determine required sample size.\n",
        "    \n",
        "    Args:\n",
        "        effect_size (float): Expected effect size (Cohen's d)\n",
        "        alpha (float): Significance level\n",
        "        power (float): Desired statistical power\n",
        "        \n",
        "    Returns:\n",
        "        int: Required sample size per group\n",
        "    \"\"\"\n",
        "    analysis = TTestIndPower()\n",
        "    sample_size = analysis.solve_power(\n",
        "        effect_size=effect_size,\n",
        "        alpha=alpha,\n",
        "        power=power,\n",
        "        alternative='two-sided'\n",
        "    )\n",
        "    return int(np.ceil(sample_size))\n",
        "\n",
        "# First, determine required sample size\n",
        "required_samples = perform_power_analysis(effect_size=0.1254, alpha=0.05, power=0.8)  #These parameters result in a sample size of 1000\n",
        "print(f\"Required samples per model for statistical power: {required_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For real-time inference (below implementation only for meta-llama/Meta-Llama-3.1-8B-Instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# # del pipeline #Otherwise too much memory is used\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto')\n",
        "\n",
        "# #Example of real-time response generation\n",
        "# messages=[{\"role\": \"user\", \"content\": \"What is the chemical formula of water?\"}]\n",
        "\n",
        "# inputs_tokenized = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#     return_dict=True,\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# input_ids = inputs_tokenized['input_ids']\n",
        "\n",
        "# # Generate tokens one by one\n",
        "# max_length = 256\n",
        "# output_ids = input_ids\n",
        "# for _ in range(256):\n",
        "#     outputs = model.generate(\n",
        "#         output_ids,\n",
        "#         max_new_tokens=1,\n",
        "#         do_sample=True,\n",
        "#         top_k=50,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "#     new_token_id = outputs[0, -1].item()\n",
        "#     if new_token_id == tokenizer.eos_token_id:\n",
        "#         break\n",
        "#     output_ids = torch.cat([output_ids, outputs[:, -1:]], dim=1)\n",
        "#     new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
        "#     print(new_token, end=\"\", flush=True)\n",
        "\n",
        "# print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other evaluators from Langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations\n",
        "# https://docs.smith.langchain.com/old/evaluation/quickstart\n",
        "\n",
        "# from langsmith.evaluation import LangChainStringEvaluator\n",
        "\n",
        "# eval_llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0.0, seed=42)\n",
        "\n",
        "# #Evaluators\n",
        "# qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm}) #LLM just gives 'correct' or 'incorrect' based on reference answer\n",
        "# context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm}) #Also uses reference context of example outputs to do the above\n",
        "# cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm}) #Same as above but with chain of thought 'reasoning'\n",
        "\n",
        "#Prompts Used internally:\n",
        "\n",
        "# 1) context_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. \n",
        "# It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 2) cot_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "# Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# EXPLANATION: step by step reasoning here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 3) qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# TRUE ANSWER: true answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, use custom prompts as shown below (and set {\"prompt\": PROMPT} as additional argument inside the config above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "# _PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in chemical engineering answers to questions.\n",
        "# You are grading the following question:\n",
        "# {query}\n",
        "# Here is the real answer:\n",
        "# {answer}\n",
        "# You are grading the following predicted answer:\n",
        "# {result}\n",
        "# Respond with CORRECT or INCORRECT:\n",
        "# \"\"\"\n",
        "\n",
        "# PROMPT = PromptTemplate(\n",
        "#     input_variables=[\"query\", \"result\", \"answer\"], template=_PROMPT_TEMPLATE\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes: Non-reproducible results, even when seed set (https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed), temperature=0 (top_p should not change when we changed temperature - smaller values result in more constrained and focused response - https://medium.com/@rasithbm/chatopenai-parameters-83bef49f6384)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b9c5f781e34806b9756d9e3e553a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cacd8a8bd03b4d0d83d266fe85e8ee65",
              "IPY_MODEL_abf8eb8102384433a59628820355d272",
              "IPY_MODEL_be7aa2993d57460c9d4f23c090e42c36"
            ],
            "layout": "IPY_MODEL_75aa3a3eb1b8420f9f26d404505e46cc"
          }
        },
        "0adc7382479c412f9c9230a17b56ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c13fc64f2e143b29105ec10e444b779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ace1e934716404a9340bce56cb1a3cf",
            "placeholder": "â",
            "style": "IPY_MODEL_e3db7de6fcc04e6ba738f7ae78cef24d",
            "value": "Loadingâcheckpointâshards:â100%"
          }
        },
        "1bd3eb0157a3477f907dae0c8fdbbec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24713d9c124b41488af127cfd3d1321e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331036e81d104ab49c44bcbde2d873f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec4c77240854f3ebab46e0b7d307f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ace1e934716404a9340bce56cb1a3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9db2c468cf4dc598a80da6a548d034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9d371e98fc45329cf381bc36a290ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331036e81d104ab49c44bcbde2d873f7",
            "placeholder": "â",
            "style": "IPY_MODEL_1bd3eb0157a3477f907dae0c8fdbbec4",
            "value": ""
          }
        },
        "6e84ac6346d8450d9814b9f1a647164c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75aa3a3eb1b8420f9f26d404505e46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87311a42fde0441fb2e88a0655a95f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c13fc64f2e143b29105ec10e444b779",
              "IPY_MODEL_a2ac8fac33444da794be1f25a9c0d702",
              "IPY_MODEL_c0e527b08dd942a684246e2db22ed22d"
            ],
            "layout": "IPY_MODEL_cf7de095d0514bbf937d0632c777a232"
          }
        },
        "9253c9636a6c4e20b215ff11c928be07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "981e94324ba64b548259b1f1d297a564": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24713d9c124b41488af127cfd3d1321e",
            "placeholder": "â",
            "style": "IPY_MODEL_0adc7382479c412f9c9230a17b56ea42",
            "value": "â9/?â[00:06&lt;00:00,ââ6.70s/it]"
          }
        },
        "98971ab8fe5c411f9c8c4c77753a745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bc61359ce44954bd235566d9ada2d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f628c84495494694a62ca9c181ec63ba",
            "value": 1
          }
        },
        "9b527616d5a647d88a33b14ee2712211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9d371e98fc45329cf381bc36a290ae",
              "IPY_MODEL_98971ab8fe5c411f9c8c4c77753a745f",
              "IPY_MODEL_981e94324ba64b548259b1f1d297a564"
            ],
            "layout": "IPY_MODEL_c1ffb867972444579481d5408abcdba9"
          }
        },
        "a2ac8fac33444da794be1f25a9c0d702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec4c77240854f3ebab46e0b7d307f74",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcfaa70f8c14dfcb0528b0cc0573db3",
            "value": 2
          }
        },
        "abf8eb8102384433a59628820355d272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6d3c439b254aa793c5d39304170849",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fac87e8a3a044de994d726896d479de3",
            "value": 1
          }
        },
        "b68644c249c04e059c924e1165a01370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7aa2993d57460c9d4f23c090e42c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6defd5c41f94c0ebda4080bb09c19c3",
            "placeholder": "â",
            "style": "IPY_MODEL_6e84ac6346d8450d9814b9f1a647164c",
            "value": "â9/?â[01:10&lt;00:00,ââ5.27s/it]"
          }
        },
        "c0e527b08dd942a684246e2db22ed22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed1e9eb6e6bc4452b3d12aecffb6cc2a",
            "placeholder": "â",
            "style": "IPY_MODEL_9253c9636a6c4e20b215ff11c928be07",
            "value": "â2/2â[00:18&lt;00:00,ââ8.66s/it]"
          }
        },
        "c1ffb867972444579481d5408abcdba9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6d3c439b254aa793c5d39304170849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cacd8a8bd03b4d0d83d266fe85e8ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68644c249c04e059c924e1165a01370",
            "placeholder": "â",
            "style": "IPY_MODEL_5c9db2c468cf4dc598a80da6a548d034",
            "value": ""
          }
        },
        "cf7de095d0514bbf937d0632c777a232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3db7de6fcc04e6ba738f7ae78cef24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8bc61359ce44954bd235566d9ada2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ed1e9eb6e6bc4452b3d12aecffb6cc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f628c84495494694a62ca9c181ec63ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6defd5c41f94c0ebda4080bb09c19c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac87e8a3a044de994d726896d479de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdcfaa70f8c14dfcb0528b0cc0573db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
