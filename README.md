# LLM Evaluation Framework

[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://www.python.org/)

[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)
[![Maintenance](https://img.shields.io/maintenance/yes/2025)](https://github.com/nsourlos/semi-automated_installation_exe_msi_files-Windows_10)

A Python-based framework for evaluating Large Language Models (LLMs) based on [Anthropic's research paper](https://arxiv.org/pdf/2411.00640) and using the [DRACO AI dataset](https://huggingface.co/datasets/draco-ai/trial01).

## ğŸ“‹ Table of Contents
- [Quick Start](#ğŸš€-quick-start)
- [Features](#âœ¨-features)
- [Installation](#ğŸ“¥-installation)
- [Configuration](#âš™ï¸-configuration)
- [Usage](#ğŸš€-usage)
- [Project Structure](#ğŸ“-project-structure)
- [Example Plots](#ğŸ“Š-example-plots)
- [Contributing](#ğŸ¤-contributing)
- [Troubleshooting](#ğŸ”§-troubleshooting)
- [License](#ğŸ“„-license)

## ğŸš€ Quick Start
1. Clone the repo
2. Set up environment variables
3. Install dependencies
4. Run [`python main.py`](main.py)

---

## âœ¨Features
- **ğŸ¤– Multiple Model Support**: OpenAI, Anthropic, Together AI, Groq, OpenRouter, Gemini, HuggingFace
- **ğŸ“Š Evaluation Metrics**: Completeness, relevance, conciseness, confidence, factuality, judgement, and custom
- **ğŸ” RAG Implementation**: FAISS vectorstore with BGE embeddings and reranking
- **ğŸ› ï¸ Tool Usage**: Code execution, simulation running, SmolAgents integration
- **âš–ï¸ Multiple Judges**: Support for secondary judge models
- **ğŸ“ˆ Statistical Analysis**: Comprehensive statistics and visualization
- **ğŸŒ Cross-Platform**: Windows, macOS, and Linux support

---

## ğŸ“¥ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/nsourlos/LLM_evaluation_framework.git
   cd LLM_evaluation_framework
   ```

2. **Create and activate a virtual environment:**
   ```bash
   python -m venv DRACO
   source DRACO/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

3. **Install the package in editable mode:**
   ```bash
   pip install -r requirements.txt 
   #Optionally ipywidgets==8.1.7 for Running in Jupyter notebook
   #Optionally flash-attn==2.6.3 for GPU support
   ```

4. **(Optional) Use environment within Jupyter Notebook**
   ```bash
   pip install ipykernel
   python -m ipykernel install --user --name=DRACO --display-name "Python (DRACO)"
   ```


5. **(Optional) Set up code execution environment:**
   ```bash
   # When using code execution features, a separate environment is needed
   # to safely run generated code without conflicts
   conda create -n test_LLM python==3.10 -y
   conda activate test_LLM
   pip install -r data/requirements_code_execution.txt
   ```
   * Note: If using venv instead of conda, paths in [src/llm_eval/utils/paths.py](src/llm_eval/utils/paths.py) must be modified to point to the correct venv location

   This creates an isolated environment for running generated code, preventing potential conflicts with the main evaluation environment.


---

## âš™ï¸ Configuration

### ğŸ”‘ Environment Variables
1. Rename [`env_example`](env_example) to `env` and add your API keys:
```
OPENAI_API_KEY="your_openai_api_key"
GEMINI_API_KEY="your_gemini_api_key"
TOGETHER_API_KEY="your_together_api_key"
GROQ_API_KEY="your_groq_api_key"
ANTHROPIC_API_KEYO="your_anthropic_api_key"
HF_TOKEN="your_huggingface_token"
OPEN_ROUTER_API_KEY="your_openrouter_api_key"
```

### ğŸ“‚ Path Configuration
Edit [`src/llm_eval/utils/paths.py`](src/llm_eval/utils/paths.py) to set your system-specific paths:
- For the corresponding OS: Set `base_path` and `venv_path`

### âš¡ Parameters Configuration
Edit [`src/llm_eval/config.py`](src/llm_eval/config.py) to configure:
- `excel_file_name`: Your dataset Excel file
- `embedding_model`: Model for RAG embeddings
- `reranker_model_name`: Model for reranking
- `models`: List of models to evaluate (e.g. OpenAI, Together, Gemini models)
- `judge_model`: Models used to judge the results
- `commercial_api_providers`: Use to distinguish commercial and HuggingFace models
- `max_output_tokens`: Maximum tokens in judge LLM output
- `generate_max_tokens`: Token limit for regular model responses
- `generation_max_tokens_thinking`: Token limit for reasoning model responses
- `domain`: Domain of evaluation (e.g. "Water" Engineering)
- `n_resamples`: Number of times to resample the dataset
- `continue_from_resample`: Which resample iteration to continue from
- `tool_usage`: Enable/disable tool usage for answering questions
- `use_RAG`: Enable/disable RAG (Retrieval Augmented Generation)
- `use_smolagents`: Enable/disable SmolAgents for code execution

---

## ğŸ“ Excel File Format

The input Excel file must contain at least two columns:
- `input`: The questions or prompts to evaluate
- `output`: The expected answers or ground truth

Additional columns may be added:
- `id`: Column to uniquely identify questions
- `origin_file`: The json file from which the question-answer pair was extracted
- `topic/subtopic`: The topic/subtopic of the question
- `Reference`: Information from where the question-answer pair was obtained

---

## ğŸš€ Usage

1. **Configure parameters:**
   - Set up your environment variables in [`env_example`](env_example) and rename it to `env`
   - Configure paths in [`src/llm_eval/utils/paths.py`](src/llm_eval/utils/paths.py)
   - Modify prompts and list of metrics in [`src/llm_eval/evaluation/prompts.py`](src/llm_eval/evaluation/prompts.py)
   - Adjust parameters in [`src/llm_eval/config.py`](src/llm_eval/config.py)

2. **Run the evaluation:**
   ```bash
   python main.py 
   # Optionally `python main.py | tee data/log.txt` to save terminal output to txt file
   ```

The script will:
- Load and process your Excel dataset
- Run evaluations on specified models
- Generate Excel results files
- Create JSON files for statistics
- Produce visualization plots

---

## ğŸ“ Project Structure

```
llm_evaluation_framework/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ llm_eval/
â”‚       â”œâ”€â”€ config.py               # All configuration parameters
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ data_loader.py      # Functions for loading data and models
â”‚       â”‚   â”œâ”€â”€ model_utils.py      # Model initialization and utilities
â”‚       â”œâ”€â”€ evaluation/
â”‚       â”‚   â”œâ”€â”€ evaluator.py        # Evaluation functions
â”‚       â”‚   â””â”€â”€ prompts.py          # All evaluation prompt strings
â”‚       â”œâ”€â”€ providers/
â”‚       â”‚   â””â”€â”€ api_handlers.py     # Helper functions for LLM APIs
â”‚       â”œâ”€â”€ tools/
â”‚       â”‚   â”œâ”€â”€ code_execution.py   # Logic for tool handling
â”‚       â”‚   â””â”€â”€ tool_usage.py       # Tool usage definition and decision logic
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ paths.py            # OS-specific path configurations
â”‚           â”œâ”€â”€ plotting.py         # Visualization functions
â”‚           â”œâ”€â”€ processing.py       # Processing and Excel file creation
â”‚           â”œâ”€â”€ rag.py              # RAG implementation
â”‚           â”œâ”€â”€ scoring.py          # Scoring utilities
â”‚           â””â”€â”€ statistics.py       # Statistical calculations
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ convert_DRACO_to_excel.ipynb     # Create Excel file from json files with question-answer pairs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ requirements_code_execution.txt  # Dependencies for code execution environment
â”‚   â”œâ”€â”€ network_0.inp                    # Input file for network comparison
â”‚   â”œâ”€â”€ network_test.inp                 # Input file for network testing scenarios
â”‚   â”œâ”€â”€ compare_networks_test.py         # Test script for network comparison functionality
â”‚   â”œâ”€â”€ compare_networks.py              # Main network comparison implementation
â”‚   â””â”€â”€ DRACO.xlsx                       # Sample Excel dataset for evaluation
â”œâ”€â”€ runpod/
â”‚   â”œâ”€â”€ README_runpod.md                # RunPod instructions
â”‚   â””â”€â”€ runpod_initialize.ipynb         # Notebook that automatically initialize runpod and copies files to it
â”œâ”€â”€ example_imgs/
â”‚   â”œâ”€â”€ metric_comparison_grid.png                       #Example image of a comparison grid of models for different metrics
â”‚   â”œâ”€â”€ model_performance_summary.png                    #Example image of metric comparisons between models for different metrics
â”‚   â”œâ”€â”€ model_statistical_comparisons.png                #Example image of statistical comparisons between models   
â”‚   â”œâ”€â”€ spider_chart_judge_deepseek-ai_DeepSeek-V3.png   #Example image of spider graph comparisons between metrics for different models
â”œâ”€â”€ main.py                         # Main script
â”œâ”€â”€ env_example                     # Environment variables (to be renamed to env)
â”œâ”€â”€ requirements.txt                # Dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸ“Š Example Plots

The framework generates various visualization plots to help analyze the evaluation results. Here are some examples of a comparison of two models:

### Model Performance Summary
![Model Performance Summary](example_imgs/model_performance_summary.png)
*Overall performance summary of evaluated models*

### Spider Chart Analysis
![Spider Chart](example_imgs/spider_chart.png)
*Spider chart showing metric distribution*

### Metric Comparison Grid
![Metric Comparison Grid](example_imgs/metric_comparison_grid.png)
*Comparison of different metrics across models*

### Statistical Comparisons
![Statistical Comparisons](example_imgs/model_statistical_comparisons.png)
*Statistical comparison between models with p-values*

---

## ğŸ¤ Contributing

When making changes:
1. Maintain backward compatibility
2. Preserve original function signatures
3. Keep all comments and logging

---

## âœ… To-Do

- [x] Remove Langsmith
- [ ] Replace txt saves with logging

---

## ğŸ”§ Troubleshooting

All operations are logged in txt files to track errors. To modify list of metrics to be evaluated, change the list_of_metrics in [prompts.py](src/llm_eval/evaluation/prompts.py)

---

## ğŸ“„ License

[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)

To be decided .... 