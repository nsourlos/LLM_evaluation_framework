{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5Kh-zLB9lk"
      },
      "source": [
        "# Evaluate LLM results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Limitations of current implementation / To do in future:\n",
        "\n",
        "- Do not use Langsmith\n",
        "- We need to manually check the file 'final_score_log'\n",
        "- For thinking models, no correct response if answer within '<think>' - Happened for Qwen3-235B\n",
        "- For thinking models, we removed the thinking tokens from the judge evaluation to avoid possible error limits like error 400 for OpenAI\n",
        "- If run verification code for the network flow questions and there is no e.g. AB value and we get an error in response, we might get a score of 1 which is bigger than 0 we get if we replace the missing value\n",
        "- In this [paper](https://arxiv.org/pdf/2411.00640), check paragraph 'a pooled standard error across all KN answers will be inconsistent' - This is what we implemented for now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no2PHIOWCBdA"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U2Dpuc2xtmmS"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install datasets==2.20.0\n",
        "# !pip install -U langsmith==0.1.128 #0.1.99\n",
        "# !pip install langchain_openai==0.2.0 #0.1.22\n",
        "# !pip install langchain==0.3.0 #0.2.13\n",
        "# !pip install langchain_community==0.3.0 #0.2.12  \n",
        "# !pip install langchain-huggingface==0.1.0                      \n",
        "# !pip install transformers==4.44.0\n",
        "# !pip install torch==2.1.0\n",
        "# !pip install termcolor==2.4.0\n",
        "# !pip install accelerate==0.33.0\n",
        "# !pip install pandas==2.2.2\n",
        "# !pip install openpyxl==3.1.5\n",
        "# !pip install python-dotenv==1.0.1\n",
        "# !pip install einops==0.8.0\n",
        "# !pip install wheel==0.44.0\n",
        "# !pip install sentencepiece==0.2.0\n",
        "# !pip install protobuf==5.27.3 #Mistral models needs this\n",
        "# !pip install groq==0.10.0 #Groq models needs this\n",
        "# !pip install matplotlib==3.9.2\n",
        "# !pip install seaborn==0.13.2\n",
        "# !pip install scipy==1.14.1\n",
        "# !pip install statsmodels==0.14.4\n",
        "# !pip install anthropic==0.40.0 #Anthropic models needs this\n",
        "# !pip install together==1.3.14 #Together models needs this\n",
        "# !pip install google-generativeai==0.8.4\n",
        "# !pip install google-genai==0.8.0\n",
        "# !pip install sentence-transformers==3.3.1\n",
        "# !pip install faiss-gpu==1.7.2\n",
        "\n",
        "# !pip install flash-attn==2.6.3 #Install it at the end after wheel has been installed\n",
        "\n",
        "# #Only if CPU is used\n",
        "# !pip install torch==2.0.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !jupyter lab --ServerApp.iopub_data_rate_limit=1e10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "excel_file_name='DRACO_basic_and_tough.xlsx' #specify excel with Q&As - Has to be an excel file with at least 'input' and 'output' columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select models for predictions, judges, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_model='BAAI/bge-m3' #Based on leaderboard (https://huggingface.co/spaces/mteb/leaderboard) small and with great retrieval performance\n",
        "reranker_model_name=\"BAAI/bge-reranker-base\"\n",
        "\n",
        "#Model to generate responses to questions - Sometimes we might have to restart session and comment out the models that have already been run\n",
        "models=[ \n",
        "    \"huggingface/Qwen/Qwen2.5-7B-Instruct\",\n",
        "    # \"together/deepseek-ai/DeepSeek-V3\", #non-reasoning model\n",
        "    # \"together/Qwen/Qwen3-235B-A22B-fp8-tput\",\n",
        "    # \"openai/o3-2025-04-16\", #200K context length, 100K output tokens\n",
        "    # \"openai/o4-mini\", #200K context length, 100K output tokens\n",
        "    # \"together/Qwen/QwQ-32B\", #131072 context length    # \"Qwen/QwQ-32B-AWQ\",\n",
        "    \"together/deepseek-ai/DeepSeek-R1\", #164K context length\n",
        "    # \"openai/gpt-4o-2024-08-06\",\n",
        "\n",
        "\n",
        "    # \"gemini/gemini-2.5-pro-exp-03-25\", #1,048,576 input tokens length - error limits based on https://ai.google.dev/gemini-api/docs/rate-limits#free-tier - pro preview not allowed\n",
        "    # \"gemini/gemini-2.5-flash-preview-04-17\", #Thoughts only in Google studio, not in API - https://discuss.ai.google.dev/t/thoughts-are-missing-cot-not-included-anymore/63653/8\n",
        "\n",
        "    # \"together/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "    # \"openai/o1\", #200K context length, Max Output Tokens 100K #o1-2024-12-17\n",
        "    # \"openai/o1-mini\", #16384 completion tokens 128K context length, Max Output Tokens 65536 #o1-mini-2024-09-12\n",
        "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", #128K context length - Generation limit probably is 8193\n",
        "    # 'microsoft/phi-4', #14B parameters\n",
        "    # 'together/meta-llama/Llama-Vision-Free',\n",
        "    # \"openai/gpt-4.1\",\n",
        "    # \"openai/o3-mini\", #200K context length, Max Output Tokens 100K #o3-mini-2025-01-31\n",
        "    # \"together/deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
        "    # \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    # \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    # \"meta-llama/Meta-Llama-3.1-8B-Instruct\", #A4500 (20GB VRAM) and in Delft Blue (V100 32GB)\n",
        "    # \"microsoft/Phi-3.5-mini-instruct\", #A40 with 48GB VRAM, A4500 with 20GB VRAM, Delft Blue \n",
        "    # \"mistralai/Mistral-7B-Instruct-v0.3\", #A40 with 48GB VRAM, A4500 with 20GB VRAM and in Delft Blue\n",
        "    # \"Qwen/Qwen2-7B-Instruct\", #A40 with 48GB VRAM, A4500 with 20GB VRAM, Delft Blue\n",
        "    # 'AI-MO/NuminaMath-7B-TIR', #A4500 with 20GB VRAM and in Delft Blue - We can also try 01-ai/Yi-Coder-9B-Chat\n",
        "    # 'microsoft/Phi-3-mini-4k-instruct', #RTX3090\n",
        "    # \"google/gemma-2-9b-it\", #More than 20GB of GPU memory needed - Works with A40 with 48GB VRAM, but not with A4500 - 20GB, and V100 - 32GB, Delft Blue\n",
        "    # 'mistralai/Mistral-Nemo-Instruct-2407', #12B parameters, 2 RTX3090, V100 with 32GB VRAM\n",
        "    # \"anthropic/claude-3-5-sonnet-20241022\",\n",
        "    'openai/gpt-4o-mini' #Costs very low ~0.01$ for 9 Q&A pairs.\n",
        "    ] #Takes 7+hours in A40 for the 13 of the above models with 7Q&A paris and 4 resamples. Cost ±3$ (±180GB)\n",
        "\n",
        "# Groq models are defined as: groq_website/model_name e.g. 'groq_website/llama-3.1-70b-versatile'\n",
        "# OpenAI models are defined as: 'openai/model_name', e.g. 'openai/gpt-4o-mini'\n",
        "# Anthropic models are defined as 'anthropic/model_name', e.g. 'anthropic/claude-3-haiku-20240307' - Couldn't use due to billing issues\n",
        "# Together models are defined as 'together/model_name', e.g. 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free'\n",
        "# OpenRouter models are defined as 'openrouter/model_name', e.g. 'openrouter/deepseek/deepseek-r1:free' - Do not work due to extremely limited quota\n",
        "# Gemini models are defined as 'gemini/model_name', e.g. 'gemini/gemini-2.0-flash-exp'\n",
        "\n",
        "# I couldn't run 'nvidia/Mistral-NeMo-Minitron-8B-Base', \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" (Conflicting dependencies),\n",
        "# 'google/recurrentgemma-9b-it' # RecurrentGemmaForCausalLM.forward() got an unexpected keyword argument 'position_ids'\n",
        "#Large models take more time (2min/generation for Mistral 12B)\n",
        "\n",
        "#Define model to act as a judge - Only possible to use openai, gemini, and together models for now\n",
        "judge_model='openai/gpt-4o-mini'\n",
        "judge_model_2='together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\n",
        "\n",
        "#Used below to distinguish commercial and Hugging Face models\n",
        "commercial_api_providers=['openai','groq_website','anthropic','together', 'openrouter', 'gemini']\n",
        "\n",
        "#Define maximum number of tokes in the judge LLM output\n",
        "max_output_tokens=1000 \n",
        "\n",
        "#Limit of tokens in the generated response from LLM - For reasoning models we increase it to 16000 to include reasoning steps - had to define it below.\n",
        "generate_max_tokens=2000\n",
        "generation_max_tokens_thinking=16000 #This is the output generation tokens - We have to make sure that this along with input tokens not exceed context length\n",
        "\n",
        "#Domain - Chemical/Water Engineering or anything else\n",
        "domain=\"Water\"\n",
        "\n",
        "#Inference on whole dataset? - Not of use for now, maybe in the future if we want to use only part of it\n",
        "inference_on_whole_dataset=True\n",
        "\n",
        "#Number of times to resample the dataset\n",
        "n_resamples=2 #4 reduces the variance to 50%\n",
        "\n",
        "#Decide if in our dataset we want to enable tool usage to answer questions\n",
        "tool_usage=True\n",
        "\n",
        "# #This will result in evaluating the actual code/inp file contents and not the results of the simulation against the ground truth\n",
        "# text_code_evaluation=True #For now we evaluate based on the text of the inp file\n",
        "\n",
        "# Define the RAG model - True or False - Current implementation just fit most similar Q&As as input from excel - interestingly, most models fail even if response in context!\n",
        "use_RAG=False\n",
        "\n",
        "#Use smolagents for code execution - True or False\n",
        "use_smolagents=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OS specific parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "venv_name = \"test_LLM\"\n",
        "\n",
        "if platform.system() == \"Windows\":\n",
        "    base_path = r\"C:\\Users\\soyrl\\Desktop\\LLMs\\runpod_files_LLMs\\local_test\"\n",
        "    venv_path=r\"C:\\ProgramData\\Anaconda3\\Scripts\\conda.exe\"\n",
        "elif platform.system() == \"Darwin\": #MacOS\n",
        "    base_path = \"/Users/nikolaossourlo/Desktop/LLMs/runpod_files_LLMs/local_test\"\n",
        "    venv_path = \"/opt/anaconda3/etc/profile.d/conda.sh\" \n",
        "elif platform.system() == \"Linux\": \n",
        "    #For RunPod set to '/workspace' which is the persistent storage directory - For local Linux set to \"/home/username/path/to/folder\"\n",
        "    base_path = \"/workspace\"\n",
        "    venv_path = f\"/workspace/{venv_name}/bin/activate\"\n",
        "else:\n",
        "    raise RuntimeError(\"Unsupported OS\")\n",
        "\n",
        "file_path=os.path.join(base_path,excel_file_name)\n",
        "custom_cache_dir=os.path.join(base_path,'cache/huggingface')\n",
        "\n",
        "print(\"Base Path is:\",base_path)\n",
        "print(\"File Path is:\", file_path)\n",
        "print(\"Custom Cache Directory is:\",custom_cache_dir)\n",
        "print(\"Venv Path is:\", venv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(base_path) #For RunPod change to persistent storage directory - for local PC to folder with data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports and Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if custom_cache_dir is defined, otherwise use default behavior\n",
        "try:\n",
        "    cache_dir=custom_cache_dir #Save models here so that we don't have to download them again\n",
        "except:\n",
        "    cache_dir=None\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import traceback\n",
        "import time\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import torch\n",
        "import pandas as pd\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "from datasets import Dataset\n",
        "import transformers\n",
        "\n",
        "from langsmith.utils import LangSmithConnectionError\n",
        "from langsmith import Client\n",
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "from langsmith.schemas import Run, Example\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from termcolor import colored\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import ast\n",
        "import platform\n",
        "\n",
        "#Smolagents related import\n",
        "from e2b_code_interpreter import Sandbox\n",
        "import sys\n",
        "\n",
        "#RAG related imports\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS #Better than cosine similarity since it's scales better\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
        "\n",
        "# Get the OpenAI API key\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY_DRACO')\n",
        "together_api_key = os.getenv('TOGETHER_API_KEY_DRACO')\n",
        "open_router_api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
        "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
        "groq_api_key=os.getenv('GROQ_API_KEY')\n",
        "\n",
        "#Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# Log in with your Hugging Face token\n",
        "login(token=os.getenv('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define prompts for custom evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_prompt=\"\"\" \n",
        "You are an autoregressive language model that acts as a judge in comparing a predicted vs an actual answer to a questions.\n",
        "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend \n",
        "a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. \n",
        "Your users are experts in \"\"\"+ domain +\"\"\" engineering, so they already know you're a language model and your capabilities and limitations, so don't \n",
        "remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. \n",
        "Don't be verbose in your answers, but do provide details and examples where it might help the explanation.\n",
        "\"\"\" #This is common for all prompts below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to completeness compared to the completeness of a given actual, golden standard answer. \n",
        "The completeness metric evaluates the extent to which the user's question is answered in full in the predicted response. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: There is no response.\n",
        "2: No parts of a suitable answer are present.\n",
        "3: Few elements of a complete answer are present.\n",
        "4: Most elements of a complete answer are present.\n",
        "5: The response covers all elements of a complete answer.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "relevance_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to relevance compared to the relevance of a given actual, golden standard answer. \n",
        "The relevance metric evaluates the amount of irrelevant information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response answers something else, not the user's question.\n",
        "2: The response answers the user's question but the information provided is mostly irrelevant.\n",
        "3: The response answers the user's question but contains more irrelevant information than relevant information.\n",
        "4: The response answers the user's question, and shares a bit of irrelevant information.\n",
        "5: The response answers the user's question and contains no irrelevant information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "conciseness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to conciseness compared to the conciseness of a given actual, golden standard answer. \n",
        "The conciseness metric evaluates the amount of unexpected extra information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response is too long and stops before completion or enters an infinite loop.\n",
        "2: The response includes a lot of extra information and uses flowery language.\n",
        "3: The response includes a lot of extra information or uses flowery language.\n",
        "4: The response is short and includes a small amount of extra information.\n",
        "5: The response is as short as possible while still answering the prompt.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "confidence_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to confidence compared to the confidence of a given actual, golden standard answer. \n",
        "The condifence metric evaluates the degree of assurance that is conveyed the response that the predicted answer is correct. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: Complete Rejection. The response makes it clear that the given answer is incorrect or that no correct answer can be provided.\n",
        "2: Doubt and Disagreement. The response suggests that the answer is likely incorrect or raises significant concerns.\n",
        "3: Uncertainty. The response indicates that the answer could be correct, but there is significant doubt or insufficient evidence.\n",
        "4: Moderate Agreement. The response leans towards the answer being correct but acknowledges some uncertainty.\n",
        "5: Full Endorsement. The reponse confidentely asserts that the given answer is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "factuality_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to factuality compared to the factuality of a given actual, golden standard answer.\n",
        " The factuality metric evaluates the degree of hallucination contained in a response or, in other words, how accurate a given response is.\n",
        "You can assign a score from 1 to 5, with the following interpretations:\n",
        "1: The response is a complete hallucination\n",
        "2: The response is mostly a hallucination but does not change key information from the prompt\n",
        "3: The response contains large amounts of both hallucinations and factual information.\n",
        "4: The response includes mostly factual information with slight hallucinations.\n",
        "5: The response only includes factual information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\" \n",
        "\n",
        "\n",
        "judgement_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to judgement compared to the judgement of a given actual, golden standard answer.\n",
        "The judgment metric assesses how strongly the response implies its correctness, taking into account the actual accuracy of the answer.\n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response confidently claims a hallucination as truth.\n",
        "2: The response misinterprets information received in the prompt.\n",
        "3: The response shows that the model is unsure about the answer or states that information is theoretical.\n",
        "4: The response is wrong but it is made clear that the answer is wrong or that the model is unable to provide a correct answer.\n",
        "5: The response is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "general_descr = \"\"\"\n",
        "You are a strict but fair expert in water engineering, acting as a judge. You will be given a question , a predicted answer, and an actual answer. \n",
        "Your task is to evaluate the predicted answer on a scale from 0 to 5, where 5 indicates a fully correct and complete response, and 0 indicates a fully incorrect or irrelevant answer. \n",
        "If the question asks for a specific number or set of numbers, assign a score of 5 only if the predicted answer matches exactly the actual answer or is accurate within a tolerance of ±0.01 \n",
        "(correct up to two decimal places). If any required number is outside this margin, assign a score of 0. For conceptual or open-ended questions, evaluate based on accuracy, \n",
        "completeness, and clarity, using the full 1–5 scale as appropriate. If there is no predicted answer, assign the lowest possible score.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Google Drive mount (If run in Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if 'content/drive/My Drive' in file_path:\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test code for comparing two water networks works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_python_script(python_file):\n",
        "    script_path = os.path.join(base_path, python_file)\n",
        "\n",
        "    if platform.system() == \"Windows\":\n",
        "        cmd = [venv_path, \"run\", \"-n\", venv_name, \"python\", script_path]\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        return ' '.join(cmd), process\n",
        "\n",
        "    elif platform.system() == \"Darwin\":\n",
        "        bash_command = f\"source {venv_path} && conda activate {venv_name} && MPLBACKEND=Agg python {script_path}\"\n",
        "        process = subprocess.Popen(['bash', '-c', bash_command], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        return bash_command, process\n",
        "\n",
        "    elif platform.system() == \"Linux\":\n",
        "        bash_command = f\"source {venv_path} && MPLBACKEND=Agg python {script_path}\"\n",
        "        process = subprocess.Popen(['bash', '-c', bash_command], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        return bash_command, process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_command, test_process = run_python_script('compare_networks_test.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Command:\", test_command)\n",
        "stdout, stderr = test_process.communicate()\n",
        "print(\"STDOUT:\\n\", stdout)\n",
        "print(\"STDERR:\\n\", stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    # if not any(provider in model_name for provider in commercial_api_providers): #For Hugging Face models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "#How the dataset will be named in Langsmith\n",
        "def get_dataset_name(model_name, judge_model, use_RAG=False, use_smolagents=False):\n",
        "    model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "    base_path = f\"{domain}_Engineering_Evaluation_{model_parameter}_with_judge_{judge_model}_beam_tool_usage\"\n",
        "    \n",
        "    if use_smolagents:\n",
        "        base_path += \"_smol\"\n",
        "\n",
        "    if use_RAG:\n",
        "        return f\"{base_path}_RAG\"\n",
        "    else:\n",
        "        return f\"{base_path}_{str(tool_usage)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "lVqBHaT2s6Aq"
      },
      "outputs": [],
      "source": [
        "qa=pd.read_excel(file_path) #Read Excel\n",
        "qa=qa[['input','output']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6zdJxKCubI"
      },
      "source": [
        "Create Dataset from df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "oUw8Puxfs6Az"
      },
      "outputs": [],
      "source": [
        "loaded_dataset=Dataset.from_pandas(qa)\n",
        "\n",
        "if inference_on_whole_dataset==False:\n",
        "    loaded_dataset = loaded_dataset.train_test_split(test_size=0.2, seed=42) #Used if going to fine-tune in part of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "vf6thikds6A1"
      },
      "outputs": [],
      "source": [
        "if inference_on_whole_dataset==False:\n",
        "    dataset_train=loaded_dataset['train']\n",
        "    dataset_test=loaded_dataset['test']\n",
        "else:\n",
        "    dataset_test=loaded_dataset #When we use the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXxkzQoHs6A5"
      },
      "source": [
        "Create Langsmith Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtdIrA3Ds6A8",
        "outputId": "90a9b4dd-e91a-4773-934b-2bf58cd8e3a8"
      },
      "outputs": [],
      "source": [
        "#https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets\n",
        "example_inputs = [(x['input'],x['output']) for x in dataset_test]\n",
        "print(example_inputs)\n",
        "\n",
        "def create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key):\n",
        "\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    try:\n",
        "        #Load the dataset if already exists\n",
        "        for existing_dataset in client.list_datasets():\n",
        "            if existing_dataset.name==dataset_name:\n",
        "                dataset_langsmith=existing_dataset\n",
        "        for x in dataset_langsmith:\n",
        "            print(\"Dataset Loaded\")\n",
        "            break\n",
        "\n",
        "    except: #Otherwise create it\n",
        "        print(\"Dataset not found. Creating new dataset\")\n",
        "        # Storing inputs in a dataset lets us run chains and LLMs over a shared set of examples.\n",
        "        dataset_langsmith = client.create_dataset(dataset_name=dataset_name,\n",
        "                                                description=\"Q&A_\"+ domain + \"_engineering.\")\n",
        "\n",
        "        for input_prompt, output_answer in example_inputs:\n",
        "            client.create_example(\n",
        "                inputs={\"question\": input_prompt.replace('\\n', ' ')},\n",
        "                outputs={\"answer\": output_answer.replace('\\n', ' ')},\n",
        "                # metadata={\"source\": \"Wikipedia\"},\n",
        "                dataset_id=dataset_langsmith.id,\n",
        "            )\n",
        "\n",
        "    return dataset_langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define tools and functions to decide if they should be used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_definitions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"extract_code\",\n",
        "            \"description\": \"Use this tool whenever a user asks to write code but not when they just ask for a calculation. Do not use it for INP/EPANET file creation.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"model_output\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The text to analyze for the presence of code.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"model_output\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"run_simulation\",\n",
        "            \"description\": \"Use this tool whether a INP/EPANET file needs to be created and/or a simulation should be run.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"model_output\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"the content of the INP file to run the simulation\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"model_output\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"no_tool_needed\",\n",
        "            \"description\": \"Use this when no specialized tool is needed to answer the user's question. The LLM can respond directly with general knowledge.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"model_output\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The direct response from the LLM without using any specialized tools.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"model_output\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "\n",
        "    # # Template\n",
        "    # {\n",
        "    #     \"type\": \"function\",\n",
        "    #     \"function\": {\n",
        "    #         \"name\": \"add\",\n",
        "    #         \"description\": \"Adds two numbers together\",\n",
        "    #         \"parameters\": {\n",
        "    #             \"type\": \"object\",\n",
        "    #             \"properties\": {\n",
        "    #                 \"number1\": {\n",
        "    #                     \"type\": \"number\",\n",
        "    #                     \"description\": \"The first number to add\",\n",
        "    #                 },\n",
        "    #                 \"number2\": {\n",
        "    #                     \"type\": \"number\",\n",
        "    #                     \"description\": \"The second number to add\",\n",
        "    #                 },\n",
        "    #             },\n",
        "    #             \"required\": [\"number1\", \"number2\"],\n",
        "    #         },\n",
        "    #     },\n",
        "    # },\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decide_tool_usage(query, tools=tool_definitions, judge_model=judge_model, openai_api_key=openai_api_key):\n",
        "    \"\"\"Decide if a tool should be used based on the query, and if yes, output the tool name(s).\"\"\"\n",
        "\n",
        "    # Construct prompt for the judge\n",
        "    tool_descriptions = \"\\n\".join(\n",
        "        f\"Tool Name: {tool['function']['name']}\\nDescription: {tool['function']['description']}\\nParameters: {', '.join(tool['function']['parameters']['properties'].keys())}\"\n",
        "        for tool in tools\n",
        "    )\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "        Given a user question, determine if any tool from the provided list should be used to answer the question.\n",
        "        Consider:\n",
        "        1. The capability of each tool, based on its name, description, and parameters, to provide useful information for answering the question\n",
        "        2. If using no tool might be better than using a potentially misleading tool\n",
        "\n",
        "        User Question: {query}\n",
        "\n",
        "        Available Tools:\n",
        "        {tool_descriptions}\n",
        "\n",
        "        Should a tool be used for answering the question? If yes, specify the tool name(s). Respond with 'No' or the tool name(s).\n",
        "    \"\"\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that determines tool usage.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    # Use OpenAI to judge tool usage\n",
        "    import openai\n",
        "    from langsmith.wrappers import wrap_openai\n",
        "    client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        model=\"_\".join(judge_model.split('/')[1:]),\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    tool_decision = response.choices[0].message.content.strip()\n",
        "    print(\"Tool Decision:\", tool_decision)\n",
        "    \n",
        "    if tool_decision.lower() == 'no':\n",
        "        return ['no_tool_needed'] #None\n",
        "    else:\n",
        "        return tool_decision.split(', ') #This returns a list of tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define scoring for difficult questions in which output is a dictionary with values (require code execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_numerical_outputs(a, b, tol):\n",
        "    \"\"\"\n",
        "    Compares two scalars, lists, or dictionaries of floats.\n",
        "    Returns a score from 0 to 5 based on how many values match within an absolute tolerance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    a, b : float, list of floats, or dict of {str: float}\n",
        "        Values to compare. If lists or dicts, they must be of equal length/keys.\n",
        "    tol : float\n",
        "        Absolute tolerance. Two numbers match if abs(a - b) <= tol.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        A score from 0 (no match) to 5 (perfect match), proportional to the fraction of values within tolerance.\n",
        "    \"\"\"\n",
        "    def is_scalar(x):\n",
        "        return isinstance(x, (float, int))\n",
        "\n",
        "    def safe_float(x):\n",
        "        try:\n",
        "            return float(x)\n",
        "        except Exception:\n",
        "            raise ValueError(f\"Cannot convert value {x} to float.\")\n",
        "\n",
        "    if is_scalar(a) and is_scalar(b):\n",
        "        return 5.0 if abs(safe_float(a) - safe_float(b)) <= tol else 0.0\n",
        "\n",
        "    elif isinstance(a, dict) and isinstance(b, dict):\n",
        "        common_keys = a.keys() & b.keys()\n",
        "        if not common_keys:\n",
        "            raise ValueError(\"No common keys to compare.\")\n",
        "        matches = sum(\n",
        "            abs(safe_float(a[k]) - safe_float(b[k])) <= tol for k in common_keys\n",
        "        )\n",
        "        return 5.0 * matches / len(common_keys)\n",
        "\n",
        "    elif hasattr(a, '__len__') and hasattr(b, '__len__'):\n",
        "        if len(a) != len(b):\n",
        "            raise ValueError(\"Sequences must have the same length.\")\n",
        "        matches = sum(\n",
        "            abs(safe_float(x) - safe_float(y)) <= tol for x, y in zip(a, b)\n",
        "        )\n",
        "        return 5.0 * matches / len(a)\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Inputs must be scalars, sequences of floats, or dictionaries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/cookbook/introduction\n",
        "# https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators\n",
        "# https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator\n",
        "\n",
        "#Metrics needed in the format below to avoid confusion with the actual prompt\n",
        "list_of_metrics=['completeness_descr','relevance_descr','conciseness_descr','confidence_descr','factuality_descr','judgement_descr', 'general_descr']\n",
        "\n",
        "#Function that compares the real answer with the predicted answer of an LLM and returns a score based on the evaluation\n",
        "def factor_evaluator(run: Run, example: Example, judge_model=judge_model) -> dict: #Cannot use model_name as input\n",
        "    # print(\"Run:\",run)\n",
        "\n",
        "    question=run.inputs.get(\"inputs\")['question']\n",
        "    # print(\"Question:\",question)\n",
        "    actual_answer = example.outputs.get(\"answer\")\n",
        "    # print(\"Real answer:\",example.outputs.get(\"answer\"))\n",
        "    predicted_answer = run.outputs.get(\"output\")\n",
        "    # print(\"Predicted Answer:\",predicted_answer)\n",
        "\n",
        "    def remove_thinking_tags(s):\n",
        "        if '</think>' in s: #Check for reasoning traces and only keep the part after the trace\n",
        "            think_end = s.find('</think>')\n",
        "            if s[think_end + len('</think>'):].strip():\n",
        "                s = s[think_end + len('</think>'):].strip()\n",
        "        return s\n",
        "\n",
        "    def extract_json_dict(s): #extract a dictionary from a string\n",
        "        if not isinstance(s, str):\n",
        "            return None\n",
        "\n",
        "        s = remove_thinking_tags(s)\n",
        "\n",
        "        # Now try various methods to extract the dictionary\n",
        "        # Method 1: Direct JSON parsing of the whole string\n",
        "        try:\n",
        "            return json.loads(s)\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            pass\n",
        "\n",
        "        # Method 2: JSON parsing after stripping common wrapping characters like backticks and newlines\n",
        "        try:\n",
        "            s_clean = s.strip('` \\n')\n",
        "            return json.loads(s_clean)\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            pass\n",
        "\n",
        "        # Method 3: Python literal evaluation of the whole string\n",
        "        try:\n",
        "            result = ast.literal_eval(s)\n",
        "            if isinstance(result, dict):\n",
        "                return result\n",
        "        except (ValueError, SyntaxError, TypeError): # Added TypeError for robustness\n",
        "            pass\n",
        "\n",
        "        # Method 4: Regex for ```json ... ``` code blocks\n",
        "        match = re.search(r'```json\\s*({.*?})\\s*```', s, re.DOTALL)\n",
        "        if match:\n",
        "            try:\n",
        "                return json.loads(match.group(1))\n",
        "            except (json.JSONDecodeError, TypeError): # Added TypeError for robustness\n",
        "                pass\n",
        "        \n",
        "        # Method 5: Regex for `json ... ` (inline markdown) code blocks\n",
        "        match = re.search(r'`json\\s*({.*?})\\s*`', s, re.DOTALL)\n",
        "        if match:\n",
        "            try:\n",
        "                return json.loads(match.group(1))\n",
        "            except (json.JSONDecodeError, TypeError): # Added TypeError for robustness\n",
        "                pass\n",
        "        \n",
        "        # Method 6: Find a dictionary at the very end of the string.\n",
        "        # This handles cases like \"Some explanatory text... {json_dict}\"\n",
        "        # or \"Some explanatory text... {'python_dict_literal': True}\"\n",
        "        \n",
        "        # Remove trailing whitespace from the string to check its actual end.\n",
        "        s_trimmed_for_ending_check = s.rstrip()\n",
        "        \n",
        "        if s_trimmed_for_ending_check.endswith('}'):\n",
        "            # Find the last opening brace '{'. This is crucial for \"last occurrence\".\n",
        "            last_open_brace_index = s_trimmed_for_ending_check.rfind('{')\n",
        "            \n",
        "            if last_open_brace_index != -1:\n",
        "                # Candidate dictionary string is from the last '{' to the end of the trimmed string.\n",
        "                dict_candidate_str = s_trimmed_for_ending_check[last_open_brace_index:]\n",
        "                \n",
        "                # Attempt to parse this candidate string.\n",
        "                try:\n",
        "                    # Try json.loads first (for strict JSON).\n",
        "                    parsed_dict = json.loads(dict_candidate_str)\n",
        "                    return parsed_dict\n",
        "                except (json.JSONDecodeError, TypeError):\n",
        "                    try:\n",
        "                        # If json.loads fails, try ast.literal_eval (for Python dict literals).\n",
        "                        # ast.literal_eval can parse other literals (lists, numbers, etc.),\n",
        "                        # so we must check if the result is actually a dictionary.\n",
        "                        parsed_literal = ast.literal_eval(dict_candidate_str)\n",
        "                        if isinstance(parsed_literal, dict):\n",
        "                            return parsed_literal\n",
        "                    except (ValueError, SyntaxError, TypeError):\n",
        "                        # If both parsing methods fail for this candidate,\n",
        "                        # it means the substring from the last '{' to the end is not a valid dict.\n",
        "                        # We fall through and the function will eventually return None.\n",
        "                        pass\n",
        "        \n",
        "        return None\n",
        "\n",
        "\n",
        "    scores={} #Store scores for each metric\n",
        "    descriptions={} #Store descriptions for each metric\n",
        "    \n",
        "    # Check if there is output from LLM\n",
        "    if not predicted_answer:\n",
        "        print(\"No output from LLM\")\n",
        "        keys=[]\n",
        "        for metric_name in list_of_metrics: #Some models might not give answers for some questions (e.g. o1 sometimes)\n",
        "            keys.append(metric_name.split('_descr')[0])\n",
        "            scores[metric_name]=0\n",
        "            descriptions[metric_name]='-'\n",
        "        results = {\n",
        "            \"results\": [{\"key\": key, \"score\": scores[key + \"_descr\"], \"value\": descriptions[key + \"_descr\"]} for key in keys]}\n",
        "        return results\n",
        "\n",
        "    else:\n",
        "        for metric_name in list_of_metrics: #Iterate through all metrics\n",
        "            print(\"Evaluating based on:\",metric_name)\n",
        "            metric_value=common_prompt+eval(metric_name) #Get the actual description of the metric\n",
        "\n",
        "            # Define roles and placeholders\n",
        "            chat_template = ChatPromptTemplate.from_messages(\n",
        "            [(\"system\", metric_value),\n",
        "                (\"user\", \"Question: {question}, Actual answer: {actual_answer}, Predicted answer: {predicted_answer}\"),\n",
        "                # (\"ai\", \"It's sunny and warm outside.\"), #Use this if we want to use few shot prompts\n",
        "            ]\n",
        "            )\n",
        "\n",
        "            if '</think>' in predicted_answer: #For thinking models, we need to remove the thinking tags for the judge to avoid run out of tokens errors\n",
        "                predicted_answer = remove_thinking_tags(predicted_answer)\n",
        "\n",
        "            messages = chat_template.format_messages(question=question, actual_answer=actual_answer, predicted_answer=predicted_answer)\n",
        "            # print(\"Messages:\",messages)\n",
        "\n",
        "            formatted_messages = [(role, msg.content) for role, msg in zip([\"system\", \"user\"], messages)]\n",
        "            # print(\"Formatted messages:\",formatted_messages) #[('system', 'You are an autoregressive lan....', 'user':.....)]\n",
        "\n",
        "            print(\"Judge used to evaluate answers:\", judge_model, '\\n')\n",
        "\n",
        "            # Initialize the model and get response\n",
        "            if 'gpt-4o-mini' in judge_model:\n",
        "                llm = ChatOpenAI(\n",
        "                    model_name=\"_\".join(judge_model.split('/')[1:]), \n",
        "                    api_key=openai_api_key, \n",
        "                    temperature=0, \n",
        "                    max_tokens=max_output_tokens, \n",
        "                    seed=42\n",
        "                )\n",
        "                ai_response = llm.invoke(formatted_messages)\n",
        "            elif 'gemini' in judge_model:\n",
        "                llm = ChatGoogleGenerativeAI(\n",
        "                    model=\"_\".join(judge_model.split('/')[1:]),  # e.g., 'gemini-2-flash'\n",
        "                    google_api_key=gemini_api_key,\n",
        "                    temperature=0,\n",
        "                    max_output_tokens=max_output_tokens,\n",
        "                )\n",
        "                ai_response=llm.invoke(messages)\n",
        "            elif 'together' in judge_model:\n",
        "                llm = ChatOpenAI(\n",
        "                    base_url=\"https://api.together.xyz/v1\",  # <- Together's API endpoint\n",
        "                    model=\"/\".join(judge_model.split(\"/\")[1:]),  # e.g., 'llama-3-70b-chat'\n",
        "                    api_key=together_api_key,\n",
        "                    temperature=0,\n",
        "                    max_tokens=max_output_tokens,\n",
        "                )\n",
        "                ai_response = llm.invoke(formatted_messages)\n",
        "\n",
        "            # Output\n",
        "            # print(colored(\"System message:\"+ messages[0].content,'blue'))\n",
        "            # print(colored(\"User message:\"+ messages[1].content, 'green'))\n",
        "            # print(colored(\"AI message:\"+ ai_response.content,'red'))\n",
        "\n",
        "            try:\n",
        "                # Get judge model name for logging\n",
        "                judge_log_name = \"_\".join(judge_model.split('/')[1:])\n",
        "                with open(f\"user_ai_messages_{judge_log_name}.txt\") as log:\n",
        "                    log.write(f\"User message: \\n{messages[1].content} \\n \\n\")\n",
        "                    log.write(f\"AI message: \\n{ai_response.content} \\n \\n\")\n",
        "            except:\n",
        "                print(\"Unable to obtain short judge name\")\n",
        "                with open(f\"user_ai_messages.txt\") as log:\n",
        "                    log.write(f\"Unable to obtain short judge name or error in writing responses with judge {judge_model}\")\n",
        "\n",
        "\n",
        "            #Decide what the final score is based on output\n",
        "            if \"FINAL SCORE:\" in ai_response.content: \n",
        "                try:\n",
        "                    score = int(ai_response.content.split(\"FINAL SCORE:\")[1])\n",
        "                except: #If more text after it due to e.g. thinking => 'FINAL SCORE:5 is not appropriate... ' - If we try to fix this by matching the last one, other errors might occur\n",
        "                    with open(f\"final_score_log.txt\", \"a\") as log:\n",
        "                        log.write(f\"More text after FINAL SCORE, possible due to thinking.\\n\")\n",
        "                        log.write(f\"{ai_response.content}\\n\\n\")\n",
        "                        log.write(f\"--------------------------------\\n\\n\")\n",
        "                    try:\n",
        "                        # Find the last occurrence of \"FINAL SCORE:\" and extract the number after it\n",
        "                        score = int(ai_response.content.split(\"FINAL SCORE:\")[-1])\n",
        "                    except:\n",
        "                        with open(f\"final_score_log.txt\", \"a\") as log:\n",
        "                            log.write(f\"Error extracting score from last occurrence. \\n\")\n",
        "                        try: # Try to extract score using regex pattern matching for \"FINAL SCORE: X\" format - might not the first occurence and in between thinking process\n",
        "                            score_match = re.search(r'FINAL SCORE:\\s*(\\d+)', ai_response.content)\n",
        "                            if score_match:\n",
        "                                score = int(score_match.group(1))\n",
        "                                with open(f\"final_score_log.txt\", \"a\") as log:\n",
        "                                    log.write(f\"Managed to obtain final score with 're'. Score used {score} \\n \\n\")\n",
        "                                continue\n",
        "                        except:\n",
        "                            print(\"Error extracting score from last occurrence. Set it to 0.\")\n",
        "                            with open(f\"final_score_log.txt\", \"a\") as log:\n",
        "                                log.write(f\"Error extracting score from last occurrence. Set it to 0. Response was: \\n {ai_response.content} \\n \\n\")\n",
        "                            score=0\n",
        "            else:\n",
        "                print(\"Invalid response from LLM:\", ai_response.content)\n",
        "                try:\n",
        "                    with open(f\"invalid_responses_{judge_log_name}.txt\") as log:\n",
        "                        log.write(f\"Invalid response from LLM: {ai_response.content} \\n \\n\")\n",
        "                except:\n",
        "                    with open(\"invalid_responses.txt\") as log:\n",
        "                        log.write(f\"Invalid response from LLM: {ai_response.content} \\n \\n\")\n",
        "                score = 0 #For cases where the LLM doesn't return a score - Otherwise we are gonna get an error\n",
        "\n",
        "            \n",
        "            try:\n",
        "                # Get judge model name for logging\n",
        "                judge_log_name = \"_\".join(judge_model.split('/')[1:])\n",
        "\n",
        "                predicted_dict = extract_json_dict(predicted_answer.strip())\n",
        "                actual_dict = extract_json_dict(actual_answer)\n",
        "\n",
        "                try: #For network-related questions\n",
        "                    # Log the raw answers\n",
        "                    with open(f\"test_score_log_{judge_log_name}.txt\", \"a\") as log:\n",
        "                        log.write(f\"Predicted answer: {predicted_answer.strip()} \\n\")\n",
        "                        log.write(f\"Actual answer: {actual_answer} \\n\")\n",
        "                        if predicted_dict is not None:\n",
        "                            log.write(f\"Predicted dictionary: {predicted_dict} \\n\") \n",
        "                        if actual_dict is not None:\n",
        "                            log.write(f\"Actual dictionary: {actual_dict} \\n\")\n",
        "                        log.write(f\"--------------------------------\\n\\n\")\n",
        "                    \n",
        "                    # Verify numerical outputs if we successfully extracted dictionaries\n",
        "                    if predicted_dict and actual_dict:\n",
        "                        test_score = verify_numerical_outputs(actual_dict, predicted_dict, tol=0.01)\n",
        "                        \n",
        "                        with open(f\"test_score_log_{judge_log_name}.txt\", \"a\") as log:\n",
        "                            log.write(f\"Score set from {score} to {test_score} for predicted answer {predicted_answer.strip()} \\n \\n\")\n",
        "                        \n",
        "                        score = test_score\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(\"Error verifying numerical outputs:\", str(e))\n",
        "                    with open(f\"test_score_log_{judge_log_name}.txt\", \"a\") as log:\n",
        "                        log.write(f\"Error verifying numerical outputs: {str(e)}\\n\")\n",
        "                        log.write(f\"{predicted_answer}\\n\\n\")\n",
        "                        log.write(f\"--------------------------------\\n\\n\")\n",
        "            except:\n",
        "                with open(f\"test_score_unchanged_log_{judge_log_name}.txt\", \"a\") as log:\n",
        "                    log.write(f\"Predicted answer: {predicted_answer.strip()} \\n\")\n",
        "                    log.write(f\"Actual answer: {actual_answer} \\n\")\n",
        "                    log.write(f\"--------------------------------\\n\\n\")\n",
        "\n",
        "            scores[metric_name]=score\n",
        "            descriptions[metric_name]=ai_response.content\n",
        "            print(\"Scores:\",scores)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    # print('Scores from evaluation:',scores, \"\\n\")\n",
        "    # print('Descriptions from evaluation:', descriptions, \"\\n\")\n",
        "    # print(\"--------------------------------\")\n",
        "\n",
        "    return {\n",
        "        \"results\":[ #We always need 'key', 'score' pairs\n",
        "            {\"key\": \"completeness\" , \"score\": scores['completeness_descr'],\"value\":descriptions['completeness_descr']},\n",
        "            {\"key\": \"relevance\" , \"score\": scores['relevance_descr'], \"value\":descriptions['relevance_descr']},\n",
        "            {\"key\": \"conciseness\" , \"score\": scores['conciseness_descr'], \"value\":descriptions['conciseness_descr']},\n",
        "            {\"key\": \"confidence\" , \"score\": scores['confidence_descr'], \"value\":descriptions['confidence_descr']},\n",
        "            {\"key\": \"factuality\" , \"score\": scores['factuality_descr'], \"value\":descriptions['factuality_descr']},\n",
        "            {\"key\": \"judgement\" , \"score\": scores['judgement_descr'], \"value\":descriptions['judgement_descr']},\n",
        "            {\"key\": \"general\" , \"score\": scores['general_descr'], \"value\":descriptions['general_descr']},\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add second judge - Use excel as a starting point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_second_judge(input_excel, list_of_metrics, num_resamples, model_name, judge_model_2=judge_model): #Do not run this in parallel since it might cause issues with the responses\n",
        "    \"\"\"\n",
        "    Applies the second judge model to all predictions in the Excel file, for all resamples and metrics.\n",
        "    Appends results as new columns and saves a new Excel file.\n",
        "    \"\"\"\n",
        "\n",
        "    if judge_model_2!='openai/gpt-4o-mini': #Only run second judge if we don't have the main judge - gpt-4o-mini is the default\n",
        "        print(\"Applying judge:\", judge_model_2)\n",
        "        df = pd.read_excel(input_excel)\n",
        "        \n",
        "        # Extract clean metric names\n",
        "        clean_metric_names = [m.replace('_descr', '') for m in list_of_metrics]\n",
        "        judge_name = \"_\".join(judge_model_2.split('/')[1:])\n",
        "        \n",
        "        # Prepare columns for the second judge's results\n",
        "        for resample_idx in range(num_resamples):\n",
        "            for metric in clean_metric_names:\n",
        "                df[f\"metric_{metric}_{resample_idx+1}_{judge_name}\"] = 0\n",
        "                df[f\"prompt_{metric}_{resample_idx+1}_{judge_name}\"] = '--' #If no answer generated - Same as just '-' of first judge\n",
        "\n",
        "        for resample_idx in range(num_resamples):\n",
        "            print(f\"\\nPerforming evaluation of resample {resample_idx+1}/{num_resamples} of {model_name} and judge {judge_model_2}\")\n",
        "            # with open(f\"evaluation_log_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as f: #Only keep track of models for second judge\n",
        "            #     f.write(f\"\\nPerforming evaluation of resample {resample_idx+1}/{num_resamples} of {model_name} and judge {judge_model_2}\")\n",
        "\n",
        "            for idx, row in tqdm(df.iterrows(), total=len(df)): #Loop over all rows (not in the column names)\n",
        "                pred_col = f'predicted_answer_{resample_idx+1}' #Get column name so that we get its values below\n",
        "                if pred_col not in row or pd.isna(row[pred_col]): #if we have nan in that row, skip it and don't do anything (df already has 0 in values and '--' in prompts for those)\n",
        "                    with open(f\"non_existing_cols_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as f: \n",
        "                        f.write(f\"Skipping {idx} predicted answer col:{pred_col} not in {row[pred_col]}\\n\")\n",
        "                        f.write(f\"Skipping {idx} pd.isna(row[pred_col]):{pd.isna(row[pred_col])} not in \\n {row}\\n\\n\")\n",
        "                    continue  # Skip if this resample is missing\n",
        "\n",
        "                # Create Run object with correct structure to match factor_evaluator expectations\n",
        "                run = type('Run', (), {})()\n",
        "                run.inputs = {\"inputs\": {\"question\": row['questions']}}\n",
        "                run.outputs = {\"output\": row[pred_col]}\n",
        "\n",
        "                # Create Example object with correct structure\n",
        "                example = type('Example', (), {})()\n",
        "                example.outputs = {\"answer\": row['answers']}\n",
        "\n",
        "                judge2_results = factor_evaluator(run, example, judge_model_2)\n",
        "                #returns a dict with one key 'results' and a list with num_of_metrics dicts, each in the format below:\n",
        "                #{'results': [{'key': 'completeness', 'score': 5, 'value': \"To evaluate...\n",
        "\n",
        "                for result in judge2_results['results']:\n",
        "                    metric = result['key']\n",
        "                    with open(f\"changed_scores_model_{'_'.join(model_name.split('/')[1:])}_judge_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as f:\n",
        "                        f.write(f\"factor_evaluator_scores:{result['score']} set at index {idx} in col metric_{metric} for resample {resample_idx+1} and judge {'_'.join(judge_model_2.split('/')[1:])}\\n\\n\")\n",
        "                    df.at[idx, f\"metric_{metric}_{resample_idx+1}_{'_'.join(judge_model_2.split('/')[1:])}\"] = result['score']\n",
        "                    df.at[idx, f\"prompt_{metric}_{resample_idx+1}_{'_'.join(judge_model_2.split('/')[1:])}\"] = result['value']\n",
        "\n",
        "        filename_excel = (f\"results_{'_'.join(judge_model_2.split('/')[1:])}_judge_with_\"\n",
        "            f\"{model_name.replace('/','_')}.xlsx\")\n",
        "        \n",
        "        print(f\"Saving with name: {filename_excel}\")\n",
        "        output_excel = f\"{filename_excel}\"\n",
        "        df.to_excel(output_excel, index=False)\n",
        "        print(f\"Second judge results saved to {output_excel}\")\n",
        "        # save_results(df, judge_model_2, model_name) #This might be needed and has to be modified if we plan to use a thinking model as judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Models that Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(0) #Set for reproducibility\n",
        "\n",
        "def initialize_model(model_id):\n",
        "    # # Check if mps acceleration is available (For MacOS)\n",
        "    # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    # print(f\"Using device {device}\")\n",
        "    # model.to(device)\n",
        "\n",
        "    # transformers.set_seed(42) #Tried for reproducibility but didn't work\n",
        "    \n",
        "    pipeline = transformers.pipeline( \n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\":cache_dir},\n",
        "            # trust_remote_code=True,\n",
        "            device_map=\"auto\" #Use 'cuda' if one GPU available (works with 32GB VRAM for 7B models) - 'auto' the alternative for distributed over all available GPUs\n",
        "        )\n",
        "    return pipeline\n",
        "\n",
        "def get_model(model_id):\n",
        "    \"\"\"Given a model name, return the loaded model, tokenizer, and pipeline\"\"\"\n",
        "\n",
        "    if not any(provider in model_id for provider in commercial_api_providers): #For Hugging Face models\n",
        "        pipeline=initialize_model(model_id)\n",
        "\n",
        "    #Returns below variables if defined, and returns None for any that are not.\n",
        "    model = locals().get('model', None)\n",
        "    tokenizer = locals().get('tokenizer', None)\n",
        "    pipeline = locals().get('pipeline', None)\n",
        "\n",
        "    return model, tokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RAG and embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_vectorstore(list_of_questions, list_of_answers, embedding_model):\n",
        "    \"\"\"Initialize the embedding model and FAISS vectorstore for the dataset.\"\"\"\n",
        "    # Initialize embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model,\n",
        "        cache_folder=cache_dir,\n",
        "        model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"} #We might even use 'mps' for MacOS\n",
        "    )\n",
        "    \n",
        "    # Create documents from Q&A pairs\n",
        "    documents = [\n",
        "        Document(\n",
        "            page_content=question,\n",
        "            metadata={\"answer\": answer}\n",
        "        ) for question, answer in zip(list_of_questions, list_of_answers)\n",
        "    ]\n",
        "    \n",
        "    # Create and save FAISS index\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    \n",
        "    # Optionally save the index for later use\n",
        "    # vectorstore.save_local(\"faiss_index\")\n",
        "    \n",
        "    return vectorstore\n",
        "\n",
        "def initialize_reranker(reranker_model_name=reranker_model_name):\n",
        "    \"\"\"Initialize a cross-encoder reranker model.\"\"\"\n",
        "    reranker = CrossEncoder(reranker_model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return reranker\n",
        "\n",
        "def rerank_retrieved_documents(query, similar_pairs, reranker, top_k=3):\n",
        "    \"\"\"Rerank the retrieved documents using a cross-encoder model.\"\"\"\n",
        "    # Prepare pairs for reranking\n",
        "    pairs = [(query, pair['question'] + \"\\n\" + pair['answer']) for pair in similar_pairs]\n",
        "    \n",
        "    # Get scores from reranker\n",
        "    scores = reranker.predict(pairs)\n",
        "    \n",
        "    # Combine with original documents and sort by new scores\n",
        "    for i, pair in enumerate(similar_pairs):\n",
        "        pair['rerank_score'] = float(scores[i])\n",
        "    \n",
        "    # Sort by reranker score (descending)\n",
        "    reranked_pairs = sorted(similar_pairs, key=lambda x: x['rerank_score'], reverse=True)\n",
        "    \n",
        "    # Return top k results\n",
        "    return reranked_pairs[:top_k]\n",
        "\n",
        "def get_similar_qa_pairs(query, vectorstore, top_k=5):\n",
        "    \"\"\"Get the most similar Q&A pairs using FAISS.\"\"\"\n",
        "    # Search for similar documents\n",
        "    similar_docs = vectorstore.similarity_search_with_score(query, k=top_k)\n",
        "    \n",
        "    # Format results\n",
        "    similar_pairs = []\n",
        "    for doc, score in similar_docs:\n",
        "        similar_pairs.append({\n",
        "            'question': doc.page_content,\n",
        "            'answer': doc.metadata['answer'],\n",
        "            'similarity': 1 - score  # Convert distance to similarity score\n",
        "        })\n",
        "    \n",
        "    return similar_pairs\n",
        "\n",
        "def check_context_relevance(query, similar_pairs, judge_model, openai_api_key):\n",
        "    \"\"\"Check if the retrieved context is relevant enough to use for RAG.\"\"\"\n",
        "\n",
        "    # query=\"What is the weather in Rotterdam now?\" #With this query the context is not relevant\n",
        "\n",
        "    # Construct prompt for the judge\n",
        "    prompt = f\"\"\"Given a user question and retrieved similar Q&A pairs, determine if the context is relevant enough to be used for answering the question.\n",
        "    Consider:\n",
        "    1. Semantic similarity between the question and retrieved pairs\n",
        "    2. Whether the context provides useful information for answering\n",
        "    3. If using no context might be better than using potentially misleading context\n",
        "\n",
        "    User Question: {query}\n",
        "\n",
        "    Retrieved Q&A Pairs:\n",
        "    {similar_pairs}\n",
        "\n",
        "    Should this context be used for answering the question? Respond with only 'Yes' or 'No'.\n",
        "    \"\"\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that determines context relevance.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    # Use OpenAI to judge relevance\n",
        "    import openai\n",
        "    from langsmith.wrappers import wrap_openai\n",
        "    client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        model=\"/\".join(judge_model.split('/')[1:]),\n",
        "        seed=42\n",
        "    )\n",
        " \n",
        "    print(\"Use context/RAG:\",response.choices[0].message.content.strip().lower()) #Rotterdam query above returns 'no'\n",
        "    \n",
        "    return response.choices[0].message.content.strip().lower() == 'yes'\n",
        "\n",
        "def format_context(similar_pairs):\n",
        "    \"\"\"Format the similar Q&A pairs into a context string.\"\"\"\n",
        "    context = \"Here are some relevant previous Q&A pairs:\\n\\n\"\n",
        "    for pair in similar_pairs:\n",
        "        context += f\"Question: {pair['question']}\\n\"\n",
        "        context += f\"Answer: {pair['answer']}\\n\\n\"\n",
        "    return context.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(inputs: dict) -> dict:\n",
        "    \"\"\"Given a question, return the answer from the model, optionally using tools if tool_usage is True\"\"\"\n",
        "    \n",
        "    # Get these variables from the global scope\n",
        "    global model_name, generate_max_tokens, generation_max_tokens_thinking, vectorstore, reranker, tool_usage, use_smolagents\n",
        "\n",
        "    # Configure token limits based on model type - Reasoning model with CoT should have longer max_tokens to include the reasoning steps\n",
        "    if 'deepseek' in model_name or 'thinking' in model_name or '/o1' in model_name or '/o3' in model_name or 'gemini-2.5-pro' in model_name or 'QwQ-32B' in model_name or 'o4' in model_name or 'Qwen3' in model_name:\n",
        "        generate_max_tokens = generation_max_tokens_thinking #For 'DeepSeek-R1-Distill-Llama-70B-free' limit is 8193\n",
        "        print(\"Generation limit increased due to reasoning model:\", model_name, \"to:\", generate_max_tokens)\n",
        "    else:\n",
        "        generate_max_tokens = 1000\n",
        "\n",
        "    # Standard generation arguments\n",
        "    generation_args = { \n",
        "        \"max_new_tokens\": generate_max_tokens,\n",
        "        \"return_full_text\": False, \n",
        "        \"temperature\": 0.05, #Has to be positive number - not considered from model when do_sample is False (reproducible results)\n",
        "        \"do_sample\": True, #Selects highest probability token if sets to False\n",
        "        \"num_beams\": 5, #3 can also work if computationally intensive - more info on https://huggingface.co/blog/how-to-generate\n",
        "         #Warnings will be raised by some models\n",
        "\n",
        "#         #If we only set temp!=0 or if we also set do_sample=False then warning: `do_sample` is set to `False`. However, `temperature` is set to `1e-08` \n",
        "#         # -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
        "#         # That means that the temperature is probably ignored\n",
        "#         # Sometimes, results not reproducible if only temp is set\n",
        "#         # A temparature of 0.01 or lower results in: \"Error running target function: probability tensor contains either `inf`, `nan` or element < 0\"\n",
        "    }\n",
        "   \n",
        "    # API call handlers for different providers\n",
        "    def call_openai_api(messages):\n",
        "        try:\n",
        "            import openai\n",
        "            from langsmith.wrappers import wrap_openai\n",
        "            \n",
        "            openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "            \n",
        "            if '/o1' not in model_name and '/o3' not in model_name and '/o4' not in model_name:\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    messages=messages, \n",
        "                    temperature=0, \n",
        "                    model=\"/\".join(model_name.split('/')[1:]), \n",
        "                    max_tokens=generate_max_tokens, \n",
        "                    seed=42\n",
        "                ) \n",
        "            else:  # For thinking models\n",
        "                print(\"Thinking....\")\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    messages=messages, \n",
        "                    model=\"/\".join(model_name.split('/')[1:]), \n",
        "                    max_completion_tokens=generate_max_tokens, \n",
        "                    seed=42\n",
        "                ) \n",
        "\n",
        "            result = response.choices[0].message.content\n",
        "            print(\"Response from OpenAI:\", result)\n",
        "            time.sleep(5)  # To avoid rate limit\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            print(\"OpenAI Model ID:\", model_name)\n",
        "            print(\"Traceback:\", traceback.format_exc())\n",
        "            return f\"Error with OpenAI API: {str(e)}\"\n",
        "\n",
        "    def call_groq_api(messages):\n",
        "        try:\n",
        "            from groq import Groq\n",
        "            client = Groq(api_key=groq_api_key)\n",
        "            actual_model_name = \"/\".join(model_name.split('/')[1:])\n",
        "            response = client.chat.completions.create(\n",
        "                model=actual_model_name,\n",
        "                max_tokens=generate_max_tokens,\n",
        "                temperature=0,\n",
        "                messages=messages\n",
        "            )\n",
        "            \n",
        "            result = response.choices[0].message.content\n",
        "            print(\"Response from Groq:\", result)\n",
        "            time.sleep(5)  # To avoid rate limit\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            print(\"Groq Model ID:\", model_name)\n",
        "            print(\"Traceback:\", traceback.format_exc())\n",
        "            return f\"Error with Groq API: {str(e)}\"\n",
        "\n",
        "    def call_anthropic_api(messages):\n",
        "        try:\n",
        "            import anthropic\n",
        "            client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
        "            response = client.messages.create(\n",
        "                model=\"/\".join(model_name.split('/')[1:]),\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=generate_max_tokens,\n",
        "            )\n",
        "            result = response.content[0].text\n",
        "            print(\"Response from Anthropic:\", result)\n",
        "            time.sleep(5)  # To avoid rate limit\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            print(\"Anthropic Model ID:\", model_name)\n",
        "            print(\"Traceback:\", traceback.format_exc())\n",
        "            return f\"Error with Anthropic API: {str(e)}\"\n",
        "\n",
        "    def call_together_api(messages):\n",
        "        try:\n",
        "            from together import Together\n",
        "            client = Together(api_key=together_api_key)\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"/\".join(model_name.split(\"/\")[1:]),\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=generate_max_tokens\n",
        "            )\n",
        "            result = response.choices[0].message.content\n",
        "            print(\"Response from Together:\", result)\n",
        "            time.sleep(5)  # To avoid rate limit\n",
        "            if \"<think>\" in result:\n",
        "                time.sleep(180)  # To avoid rate limit need to wait 3 minutes\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            print(\"Together Model ID:\", model_name)\n",
        "            print(\"Traceback:\", traceback.format_exc())\n",
        "            return f\"Error with Together API: {str(e)}\"\n",
        "\n",
        "    def call_openrouter_api(messages):\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            client = OpenAI(\n",
        "                base_url=\"https://openrouter.ai/api/v1\",\n",
        "                api_key=open_router_api_key,\n",
        "            )\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"/\".join(model_name.split(\"/\")[1:]),\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=generate_max_tokens,\n",
        "            )\n",
        "            result = response.choices[0].message.content\n",
        "            print(\"Response from OpenRouter:\", result)\n",
        "            time.sleep(5)  # To avoid rate limit\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            print(\"OpenRouter Model ID:\", model_name)\n",
        "            print(\"Traceback:\", traceback.format_exc())\n",
        "            return f\"Error with OpenRouter API: {str(e)}\"\n",
        "\n",
        "    def call_gemini_api(messages):\n",
        "        try:\n",
        "            if 'thinking' in model_name or 'gemini-2.5-pro' in model_name:  # Thinking model has different call\n",
        "                from google.generativeai.types import GenerationConfig\n",
        "                import google.generativeai as genai\n",
        "                genai.configure(api_key=gemini_api_key)\n",
        "                model = genai.GenerativeModel(\"/\".join(model_name.split('/')[1:]))\n",
        "                response = model.generate_content(\n",
        "                    contents=messages,\n",
        "                    generation_config=GenerationConfig(\n",
        "                        temperature=0,\n",
        "                        max_output_tokens=generate_max_tokens,\n",
        "                    )\n",
        "                )\n",
        "                result = response.text\n",
        "                print(\"Response from Gemini ('thinking') model:\", result)\n",
        "                time.sleep(13)  # To avoid rate limit\n",
        "                return result\n",
        "            else:  # for the rest of the models\n",
        "                from google import genai\n",
        "                from google.genai import types\n",
        "                client = genai.Client(api_key=gemini_api_key)\n",
        "                response = client.models.generate_content(\n",
        "                    model=\"/\".join(model_name.split('/')[1:]),\n",
        "                    contents=messages,\n",
        "                    config=types.GenerateContentConfig(\n",
        "                        temperature=0,\n",
        "                        max_output_tokens=generate_max_tokens,\n",
        "                    )\n",
        "                )\n",
        "                print(\"Full response from Gemini model:\", response)\n",
        "                result = response.text\n",
        "                print(\"Response from Gemini:\", result)\n",
        "                time.sleep(10)  # To avoid rate limit\n",
        "                return result\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            print(\"Gemini Model ID:\", model_name)\n",
        "            print(\"Traceback:\", traceback.format_exc())\n",
        "            return f\"Error with Gemini API: {str(e)}\"\n",
        "\n",
        "    def call_huggingface_api(messages):\n",
        "        response = pipeline(messages, **generation_args)[0]['generated_text']\n",
        "        print(\"HF model\", model_name, ':', response)\n",
        "        return response\n",
        "\n",
        "    # Main API caller function\n",
        "    def get_model_response(messages):\n",
        "        # Use the original commercial_api_providers list\n",
        "        if not any(provider in model_name for provider in commercial_api_providers):\n",
        "            print(\"Using HuggingFace model...\")\n",
        "            return call_huggingface_api(messages)\n",
        "        \n",
        "        if 'openai' in model_name:\n",
        "            print(\"Using OpenAI model...\")\n",
        "            return call_openai_api(messages)\n",
        "        elif 'groq_website' in model_name:\n",
        "            print(\"Using Groq model...\")\n",
        "            return call_groq_api(messages)\n",
        "        elif 'anthropic' in model_name:\n",
        "            print(\"Using Anthropic model...\")\n",
        "            return call_anthropic_api(messages)\n",
        "        elif 'together' in model_name:\n",
        "            print(\"Using Together AI model...\")\n",
        "            return call_together_api(messages)\n",
        "        elif 'openrouter' in model_name:\n",
        "            print(\"Using OpenRouter model...\")\n",
        "            return call_openrouter_api(messages)\n",
        "        elif 'gemini' in model_name:\n",
        "            print(\"Using Gemini model...\")\n",
        "            return call_gemini_api(messages)\n",
        "        else:\n",
        "            print(\"Error: Not known model provider\")\n",
        "            return \"Unknown model provider\"\n",
        "\n",
        "    # Function to handle code extraction and execution\n",
        "    def handle_code_extraction(code_result, model_name, user_question, use_smolagents=False):\n",
        "        \"\"\" \n",
        "        Handle code extraction and execution.\n",
        "        \n",
        "        Args:\n",
        "            code_result (str): The code result to be processed\n",
        "            model_name (str): The name of the model to be used\n",
        "            user_question (str): The user question to be processed\n",
        "            use_smolagents (bool): Whether to use SmolAgents for code execution\n",
        "\n",
        "        Returns:\n",
        "            tuple: (final_answer, code_result)\n",
        "            final_answer (str): The final answer to be returned\n",
        "            code_result (str): The code result to be returned\n",
        "        \"\"\"\n",
        "\n",
        "        if use_smolagents==True:\n",
        "            print(\"Input question:\", user_question)\n",
        "\n",
        "            # Create the sandbox\n",
        "            sandbox = Sandbox()\n",
        "\n",
        "            # Install required packages\n",
        "            sandbox.commands.run(\"pip install smolagents\")\n",
        "            sandbox.commands.run(\"pip install 'smolagents[openai]'\") #to use openai model\n",
        "\n",
        "            def run_code_raise_errors(sandbox, code: str, verbose: bool = False) -> str:\n",
        "                execution = sandbox.run_code(\n",
        "                    code,\n",
        "                    envs={'HF_TOKEN': os.getenv('HF_TOKEN'),\n",
        "                        'OPENAI_API_KEY': openai_api_key,\n",
        "                        'GEMINI_API_KEY': gemini_api_key,\n",
        "                        'TOGETHER_API_KEY': together_api_key\n",
        "                        }\n",
        "                )\n",
        "                if execution.error:\n",
        "                    execution_logs = \"\\n\".join([str(log) for log in execution.logs.stdout])\n",
        "                    logs = execution_logs\n",
        "                    logs += execution.error.traceback\n",
        "                    raise ValueError(logs)\n",
        "                return \"\\n\".join([str(log) for log in execution.logs.stdout]), execution\n",
        "            \n",
        "        elif use_smolagents==False:\n",
        "            print(\"Resulting code:\", code_result)\n",
        "\n",
        "        # Log outputs\n",
        "        model_name_log = \"_\".join(model_name.split('/')[1:])\n",
        "        with open(\"code_log_initial_llm_response_\"+model_name_log+\".txt\", \"a\") as log:\n",
        "            if use_smolagents==False:\n",
        "                log.write(f\"\\n{code_result}\\n \\n\") \n",
        "            elif use_smolagents==True:\n",
        "                log.write(f\"\\n {user_question} \\n \\n\")\n",
        "            log.write('.................................. \\n')\n",
        "\n",
        "        if use_smolagents==True:\n",
        "            # Define your agent application \"gpt-4o-mini\"\n",
        "            agent_code = f\"\"\"\n",
        "            import os\n",
        "            from smolagents import CodeAgent, InferenceClientModel\n",
        "            from smolagents import OpenAIServerModel\n",
        "\n",
        "            model_name=\"{model_name}\"\n",
        "\n",
        "            if 'openai' in model_name:\n",
        "                model = OpenAIServerModel(\n",
        "                    model_id=\"/\".join(model_name.split('/')[1:]),\n",
        "                    api_base=\"https://api.openai.com/v1\",\n",
        "                    api_key=os.getenv('OPENAI_API_KEY'),\n",
        "                )\n",
        "\n",
        "\n",
        "            elif 'together' in model_name:\n",
        "                model = OpenAIServerModel(\n",
        "                    model_id=\"/\".join(model_name.split('/')[1:]),\n",
        "                    api_base=\"https://api.together.xyz/v1\",\n",
        "                    api_key=os.getenv('TOGETHER_API_KEY'),\n",
        "                    # api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "                    # base_url=\"https://api.together.xyz/v1\",\n",
        "                )\n",
        "\n",
        "            elif 'gemini' in model_name:\n",
        "                model = OpenAIServerModel(\n",
        "                    model_id=\"/\".join(model_name.split('/')[1:]),\n",
        "                    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "                    api_key=os.getenv('GEMINI_API_KEY'),\n",
        "                )\n",
        "\n",
        "            agent = CodeAgent(\n",
        "                model=model,\n",
        "                tools=[],\n",
        "                name=\"coder_agent\",\n",
        "                description=\"This agent takes care of your difficult algorithmic problems using code.\",\n",
        "                additional_authorized_imports=['json', 'sys']\n",
        "            )\n",
        "\n",
        "            manager_agent = CodeAgent(\n",
        "                model=model,\n",
        "                tools=[],\n",
        "                managed_agents=[agent],\n",
        "                additional_authorized_imports=['json', 'sys']\n",
        "            )\n",
        "            \n",
        "            # Run the agent\n",
        "            response = manager_agent.run({user_question!r})\n",
        "\n",
        "            print(response)\n",
        "            \"\"\" #Example prompt: 'Create a DataFrame with one column and 60 rows containing the first 60 Fibonacci numbers. Return the file contents as base64 so I can download it.'\n",
        "\n",
        "            # Run the agent code in the sandbox\n",
        "            execution_logs, execution_result = run_code_raise_errors(sandbox, agent_code)\n",
        "            print(\"Logs:\",execution_logs) #here is the string with the dict\n",
        "            print(\"Results of execution:\", execution_result)\n",
        "\n",
        "            with open(f\"code_log_final_answer_{model_name_log}.txt\", \"a\") as log:\n",
        "                log.write(f\"Final answer {execution_logs} \\n\")\n",
        "                log.write(f\"\\nFinal code logs \\n {execution_result} \\n\")\n",
        "                log.write('..........................\\n')\n",
        "            \n",
        "            return locals().get('execution_logs', '-'), locals().get('execution_result', '-')\n",
        "\n",
        "        elif use_smolagents==False:\n",
        "            # Save code_result as a py file to be used in comparisons below\n",
        "            with open(\"code_result.py\", \"w\") as file:\n",
        "                file.write(code_result)\n",
        "\n",
        "            try:\n",
        "                # Execute the command and capture the output\n",
        "                command, result = run_python_script('code_result.py')\n",
        "                stdout, stderr = result.communicate()\n",
        "                with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                    log.write(f\"Command to run script: {command} \\n \\n\")\n",
        "                final_answer = stdout\n",
        "                print(\"Execution Result:\", stdout)\n",
        "                print(\"Execution stderr\", stderr)\n",
        "\n",
        "                if stderr:\n",
        "                    print(\"Stderr detected:\", stderr)\n",
        "                    with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                        log.write(f\"Stdout: {stdout}\\n\")\n",
        "                        log.write(f\"Stderr: {stderr}\\n\")\n",
        "                    \n",
        "                    # If there's stderr, treat it like an error and attempt correction\n",
        "                    max_attempts = 3  # Original + 2 retries\n",
        "                    attempt = 1\n",
        "                    current_code = code_result\n",
        "                    current_result = f\"Error in execution: {stderr}\"\n",
        "\n",
        "                    while stderr and attempt < max_attempts:\n",
        "                        print(f\"\\nAttempt {attempt} failed, trying correction...\")\n",
        "                        with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                            log.write(f\"\\nAttempt {attempt} failed, trying correction...\\n\")\n",
        "                        \n",
        "                        # Send error and code to LLM for correction\n",
        "                        error_prompt = f\"\"\"\n",
        "                            The following code resulted in an error:\n",
        "                            \n",
        "                            {current_code}\n",
        "                            \n",
        "                            Error message:\n",
        "                            {current_result}\n",
        "                            \n",
        "                            Please correct the code to fix this error. Return only the code from the following message to be directly copy pasted in a py file. \\\n",
        "                            Do not return it in quotes, just plain code.\n",
        " \n",
        "                            \"\"\"\n",
        "                        \n",
        "                        # Get corrected code from LLM\n",
        "                        # Create error correction messages\n",
        "                        error_messages = [{\"role\": \"user\", \"content\": error_prompt}]                 \n",
        "                        try:\n",
        "                            # Get corrected code\n",
        "                            import openai\n",
        "                            from langsmith.wrappers import wrap_openai\n",
        "                            \n",
        "                            openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "                            response = openai_client.chat.completions.create(\n",
        "                                messages=error_messages, \n",
        "                                temperature=0, \n",
        "                                model=\"_\".join(judge_model.split('/')[1:]), \n",
        "                                max_tokens=generate_max_tokens, \n",
        "                                seed=42\n",
        "                            ) \n",
        "                            current_code=response.choices[0].message.content\n",
        "                            \n",
        "                            # Try running corrected code\n",
        "                            with open(\"code_result.py\", \"w\") as file:\n",
        "                                file.write(current_code)\n",
        "                                \n",
        "                            try:\n",
        "                                command, result = run_python_script('code_result.py')\n",
        "                                stdout, stderr = result.communicate()\n",
        "                                print(f\"Execution with corrected code (attempt {attempt}):\", stdout)\n",
        "                                \n",
        "                                with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                                    log.write(f\"Execution with corrected code (attempt {attempt}):\\n{stdout}\\n\")\n",
        "                                    \n",
        "                                if not stderr:\n",
        "                                    code_result = current_code\n",
        "                                    final_answer = stdout\n",
        "                                    with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                                        log.write(f\"It worked!\\n\")\n",
        "                                    break\n",
        "                            except subprocess.CalledProcessError as e:\n",
        "                                current_result = f\"Error in execution: {e.output}\"\n",
        "                                print(current_result)\n",
        "                                \n",
        "                                with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                                    log.write(f\"\\n{current_result}\\n\")\n",
        "\n",
        "                            with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                                log.write(\"\\n \\n\")\n",
        "                                    \n",
        "                            print(\"\\n\")\n",
        "                            \n",
        "                        except Exception as e:\n",
        "                            print(f\"Error getting correction: {e}\")\n",
        "                            \n",
        "                            with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                                log.write(f\"Error getting correction: {e}\\n\")\n",
        "                                \n",
        "                            break\n",
        "                            \n",
        "                        attempt += 1\n",
        "                        \n",
        "                        if stderr:\n",
        "                            print(\"\\nError still persists after maximum correction attempts\")\n",
        "                            \n",
        "                            with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                                log.write(\"\\nError still persists after maximum correction attempts\\n\")\n",
        "\n",
        "                        with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                            log.write(\"....................................................................................\\n\")\n",
        "                            \n",
        "                        with open(f\"code_log_final_answer_{model_name_log}.txt\", \"a\") as log:\n",
        "                            if 'final_answer' in locals():\n",
        "                                log.write(f\"Final answer {final_answer}\\n\")\n",
        "                            else:\n",
        "                                log.write(\"Final answer not exist\\n\")\n",
        "\n",
        "                            if 'code_result' in locals():\n",
        "                                log.write(f\"\\nFinal code: \\n {code_result}\\n\")\n",
        "                                print(\"Code output:\", code_result)\n",
        "                            else:\n",
        "                                log.write(\"\\nFinal code not exist\\n\")\n",
        "\n",
        "                            log.write('..........................\\n')\n",
        "\n",
        "                        return locals().get('final_answer', '-'), locals().get('code_result', '-')\n",
        "\n",
        "                #We get into here when no code and no answer is produced\n",
        "                with open(f\"code_log_final_answer_zero_shot_{model_name_log}.txt\", \"a\") as log:\n",
        "                    if 'final_answer' in locals():\n",
        "                        log.write(f\"Final answer {final_answer}\\n\")\n",
        "                    else:\n",
        "                        log.write(\"Final answer not exist\\n\")\n",
        "\n",
        "                    if 'code_result' in locals():\n",
        "                        log.write(f\"\\nFinal code: \\n {code_result}\\n\")\n",
        "                        print(\"Code output:\", code_result)\n",
        "                    else:\n",
        "                        log.write(\"\\nFinal code not exist\\n\")\n",
        "\n",
        "                    log.write('..........................\\n')\n",
        "                    \n",
        "                return final_answer, code_result\n",
        "            \n",
        "            except subprocess.CalledProcessError as e:\n",
        "                result = f\"Error in execution: {e.output}\"\n",
        "                print(\"Output error was:\",result)\n",
        "                \n",
        "                with open(f\"code_log_iterations_{model_name_log}.txt\", \"a\") as log:\n",
        "                    log.write(f\"Output error was: {result}\\n\")\n",
        "\n",
        "                return None, None\n",
        "        \n",
        "    def text_for_simulation(response, model_name):\n",
        "        \"\"\"\n",
        "        Handle simulation execution with the provided INP file content.\n",
        "        Attempts to run the simulation and corrects errors if needed.\n",
        "        \n",
        "        Args:\n",
        "            response (str): The INP file content to be executed\n",
        "            model_name (str): The name of the model to be used\n",
        "            \n",
        "        Returns:\n",
        "            tuple: (final_answer, inp_content) where final_answer is the execution result\n",
        "                  and inp_content is the final INP file content\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Running simulation with provided INP file content...\")\n",
        "        with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "            log.write(\"Running simulation with provided INP file content...\\n\")\n",
        "\n",
        "        def extract_inp_section(inp_text):\n",
        "            lines = inp_text.strip().splitlines()\n",
        "            extracting = False\n",
        "            extracted_lines = []\n",
        "\n",
        "            for line in lines:\n",
        "                if '[TITLE]' in line:\n",
        "                    extracting = True\n",
        "                if extracting:\n",
        "                    extracted_lines.append(line)\n",
        "                if '[END]' in line:\n",
        "                    break  # Stop after reaching [END]\n",
        "\n",
        "            return '\\n'.join(extracted_lines)\n",
        "        \n",
        "        # Initialize variables\n",
        "        max_attempts = 3\n",
        "        attempt = 1\n",
        "        final_answer = None\n",
        "        inp_content = extract_inp_section(response)\n",
        "        current_result = \"Error in execution: Initial attempt\"\n",
        "        \n",
        "        # Write the initial INP file\n",
        "        with open(\"simulation.inp\", \"w\") as file:\n",
        "            file.write(inp_content)\n",
        "\n",
        "        # Load an inp file content to be used as an example for the corrected INP file in the prompt\n",
        "        try:\n",
        "            with open(\"network_test.inp\", \"r\") as benchmark_file:\n",
        "                example_inp = benchmark_file.read()\n",
        "                print(\"Successfully loaded benchmark.inp file\")\n",
        "                with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                    log.write(\"Successfully loaded benchmark.inp file\\n\")\n",
        "                    \n",
        "        except FileNotFoundError:\n",
        "            print(\"Warning: benchmark.inp file not found\")\n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(\"Warning: benchmark.inp file not found\\n\")\n",
        "            example_inp = \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading benchmark.inp file: {e}\")\n",
        "            \n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(f\"Error loading benchmark.inp file: {e}\\n\")\n",
        "            example_inp = \"\"\n",
        "\n",
        "        while attempt <= max_attempts:\n",
        "            print(f\"\\nAttempt {attempt} to run simulation:\")\n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(f\"\\nAttempt {attempt} to run simulation:\\n\")\n",
        "            \n",
        "            try:\n",
        "                # Run the simulation\n",
        "                command, current_result = run_python_script('compare_networks.py')\n",
        "                stdout, stderr = current_result.communicate()\n",
        "\n",
        "                print(\"Script output:\\n\", stdout)\n",
        "                print(\"Script error:\\n\", stderr) \n",
        "                print(\"Continue....\")\n",
        "                with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                    log.write(f\"Script output:\\n{stdout}\\n\")\n",
        "                    log.write(f\"Script error:\\n{stderr}\\n\")\n",
        "                    log.write(\"Continue.........\\n\")\n",
        "                \n",
        "                # Check if the simulation was successful\n",
        "                if \"All unit tests passed\" in stdout:\n",
        "                    final_answer = stdout\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Trying to fix error....\")\n",
        "                    \n",
        "                    with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                        log.write(\"Trying to fix error....\\n\")\n",
        "                        \n",
        "                    # If there's an error, prepare error correction prompt\n",
        "                    error_prompt = f\"\"\"I tried to run a simulation with the following INP file content, but it failed. The content of it was:\n",
        "\n",
        "                        {inp_content}\n",
        "\n",
        "                        The output was:\n",
        "                        {stdout}\n",
        "\n",
        "                        An example of the format of a random INP file is: {example_inp}\n",
        "\n",
        "                        Please provide a corrected version of the content of the ORIGINAL INP file that will pass all unit tests. Make sure all numbers and values are aligned, as in the example.\n",
        "                        Return only the corrected INP file content without any explanations or quotes. \n",
        "                        Keep the same sections and format as in the example INP file. Do not add any additional sections and do not change their order. \n",
        "                        Make sure everything has the same alignment as in the example file.\n",
        "                        \n",
        "                        DO NOT use quotes,  or things like e.g. ```plaintext! All the columns should be aligned.\n",
        "                    \"\"\" #I have also tried to give the specific sections as content in the prompt, but it didn't work\n",
        "\n",
        "                    # Create error correction messages\n",
        "                    error_messages = [{\"role\": \"user\", \"content\": error_prompt}]\n",
        "                    \n",
        "                    try:\n",
        "                        # Get corrected INP file content\n",
        "                        import openai\n",
        "                        from langsmith.wrappers import wrap_openai\n",
        "                        \n",
        "                        openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "                        response = openai_client.chat.completions.create(\n",
        "                            messages=error_messages, \n",
        "                            temperature=0, \n",
        "                            model=\"_\".join(judge_model.split('/')[1:]), \n",
        "                            max_tokens=generate_max_tokens, \n",
        "                            seed=42\n",
        "                        ) \n",
        "                        \n",
        "                        # Extract the corrected INP file content\n",
        "                        inp_content = extract_inp_section(response.choices[0].message.content)\n",
        "                        print(\"Extracted inp content:\", inp_content)\n",
        "                        with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                            log.write(f\"Extracted inp content:\\n{inp_content}\\n\")\n",
        "                        \n",
        "                        # Try running with corrected INP file\n",
        "                        with open(\"simulation.inp\", \"w\") as file:\n",
        "                            file.write(inp_content)\n",
        "                            \n",
        "                        try:\n",
        "                            command, current_result = run_python_script('compare_networks.py')\n",
        "                            stdout, stderr = current_result.communicate()\n",
        "\n",
        "                            # current_result = current_result.stdout.decode('utf-8')\n",
        "                            print(f\"Execution with corrected INP file (attempt {attempt}):\", stdout)\n",
        "                            print(\"Error of the current execution was:\", stderr)\n",
        "                            \n",
        "                            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                                log.write(f\"Error of the current execution was:\\n{stderr}\\n\")\n",
        "                                log.write(f\"Execution with corrected INP file (attempt {attempt}):\\n{stdout}\\n\")\n",
        "                                \n",
        "                            if \"All unit tests passed\" in stdout:\n",
        "                                final_answer = stdout\n",
        "                                with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                                    log.write(f\"....................................................................................\\n\")\n",
        "                                break\n",
        "\n",
        "                        except subprocess.CalledProcessError as e:\n",
        "                            print(\"Error running script:\\n\", e.stdout, \"\\n\", e.stderr)\n",
        "                            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                                log.write(f\"Error running script:\\n{e.stdout}\\n{e.stderr}\\n\")\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"Error getting correction: {e}\")\n",
        "                        with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                            log.write(f\"Error getting correction: {e}\\n\")\n",
        "                        break\n",
        "                        \n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(\"Major Error running script:\\n\", e.stdout, \"\\n\", e.stderr)\n",
        "                with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                    log.write(f\"Major Error running script:\\n{e.stdout}\\n{e.stderr}\\n\")\n",
        "            \n",
        "            attempt += 1\n",
        "            \n",
        "        if not final_answer or \"All unit tests passed\" not in stdout:\n",
        "            print(\"\\nError still persists after maximum correction attempts\")\n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(\"\\nError still persists after maximum correction attempts\\n\")\n",
        "\n",
        "        # Delete simulation.inp file\n",
        "        try:\n",
        "            os.remove(\"simulation.inp\")\n",
        "            print(\"Deleted simulation.inp file\")\n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(\"Deleted simulation.inp file\\n\")\n",
        "                log.write(\"....................................................................................\\n\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"simulation.inp file not found\")\n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(\"simulation.inp file not found\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting simulation.inp file: {e}\")\n",
        "            with open(f\"simulation_log_{model_name}.txt\", \"a\") as log:\n",
        "                log.write(f\"Error deleting simulation.inp file: {e}\\n\")\n",
        "                \n",
        "        return locals().get('final_answer', '-'), locals().get('inp_content', '-')\n",
        "\n",
        "\n",
        "    # MAIN LOGIC STARTS HERE\n",
        "    # Get the question from inputs\n",
        "    question = inputs['question']\n",
        "\n",
        "    if use_RAG==True:\n",
        "        # Get similar Q&A pairs\n",
        "        similar_pairs = get_similar_qa_pairs(\n",
        "            question,\n",
        "            vectorstore,\n",
        "            top_k=10 #Retrieve more candidates for reranking\n",
        "        )\n",
        "\n",
        "        # Rerank the retrieved documents\n",
        "        reranked_pairs = rerank_retrieved_documents(question, similar_pairs, reranker, top_k=5)\n",
        "        \n",
        "        # Check if context should be used\n",
        "        use_context = check_context_relevance(question, reranked_pairs, judge_model, openai_api_key)\n",
        "        \n",
        "        # Prepare context if it should be used\n",
        "        if use_context:\n",
        "            context = format_context(reranked_pairs)\n",
        "            user_content = f\"Context:\\n{context}\\n\\n ANSWER THE FOLLOWING QUESTION: {question}\"\n",
        "        else:\n",
        "            user_content = question\n",
        "    \n",
        "    elif use_RAG==False:\n",
        "        user_content=question\n",
        "        print(\"RAG is disabled\")\n",
        "\n",
        "\n",
        "    # If tool_usage is enabled, check if we should use a tool for this question\n",
        "    if tool_usage:\n",
        "        model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "        tool_name = decide_tool_usage(inputs['question'])\n",
        "\n",
        "        if tool_name[0]!='no_tool_needed':\n",
        "            print(f\"Using tool: {tool_name}\")\n",
        "            with open(f\"main_log_{model_parameter}.txt\", \"a\") as log:\n",
        "                log.write(\"......................................................................\\n\")\n",
        "                log.write(f\"Using tool: {tool_name}\\n\")\n",
        "                log.write(f\"Question is: {user_content}\\n\")\n",
        "            \n",
        "            # Start with just the question\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": user_content},\n",
        "            ]\n",
        "\n",
        "            # Set the appropriate system message based on tool type\n",
        "            if tool_name[0] == 'extract_code':\n",
        "                system_message = \"Return only the code from the following message to be directly copy pasted in a py file. Do not return it in quotes, just plain code. \\\n",
        "                    Correct any typos, errors and undefined variables. Example of errors are KeyErrors of variable not defined or not properly accessed. \\\n",
        "                    Some other steps that might be needed: Add checks for edge direction using .has_edge(u, v) because some pipes in the loops might be defined in the opposite direction \\\n",
        "                    in our graph. When a pipe is flowing in the opposite direction of how it's defined in the graph, we need to Use negative flow value (e.g. -G[v][u]['Q']). The message is:\"\n",
        "            elif tool_name[0] == 'run_simulation':\n",
        "                system_message = \"Return only the text corresponding to the content of the INP file to run a simulation. Do not return it in quotes, just plain text.  \\\n",
        "                     Make sure all the columns are aligned. The message is:\"\n",
        "            else:\n",
        "                print(\"ERROR! With current tools, we shouldn't be here! \\n\")\n",
        "                with open(f\"main_log_{model_parameter}.txt\", \"a\") as log:\n",
        "                    log.write(f\"ERROR! With current tools, we shouldn't be here!\\n\")\n",
        "\n",
        "                system_message = \"Return only the text from the following message to be directly copy pasted into a file. Do not return it in quotes, just plain text. The message is:\"\n",
        "            \n",
        "            with open(f\"main_log_{model_parameter}.txt\", \"a\") as log:\n",
        "                log.write(f\"System message initially was: {system_message}\\n\\n\")\n",
        "\n",
        "            # Add system message based on model type\n",
        "            if 'gemma' not in model_name and 'anthropic' not in model_name and 'openrouter' not in model_name and 'gemini' not in model_name and '/o1' not in model_name:\n",
        "                messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n",
        "            elif 'gemini' in model_name:\n",
        "                messages = {\"role\": \"user\", \"parts\": [{\"text\": system_message + \" \" + messages[0]['content']}]}\n",
        "            else:  # For gemma add system prompt in user message\n",
        "                messages[0]['content'] = system_message + \" \" + messages[0]['content']\n",
        "            \n",
        "            # Get response from API\n",
        "            response = get_model_response(messages)\n",
        "\n",
        "            # Process based on tool type\n",
        "            if tool_name[0] == 'extract_code': #output saved within the function execution\n",
        "                if use_smolagents==False:\n",
        "                    final_answer, output_code = handle_code_extraction(response, model_name, user_question='', use_smolagents=False)\n",
        "                elif use_smolagents==True:\n",
        "                    response=''\n",
        "                    final_answer, output_code = handle_code_extraction(response, model_name, user_question=question, use_smolagents=True)\n",
        "\n",
        "                if os.path.exists(\"code_result.py\"):\n",
        "                    os.remove(\"code_result.py\")\n",
        "\n",
        "                return {\"output\": final_answer} #we return '-' if didn't work, even if correct code with 'final answer'\n",
        "            \n",
        "                # if text_code_evaluation: #this evaluates based on the actual code text/inp simulation file content\n",
        "                #     return {\"output\": output_code}\n",
        "                # else: #this evaluates based on code execution/simulation output\n",
        "                #     return {\"output\": final_answer}\n",
        "            \n",
        "            elif tool_name[0] == 'run_simulation':\n",
        "                final_answer, inp_content = text_for_simulation(response, model_name=model_parameter)\n",
        "\n",
        "                print(\"Printing responses....\")\n",
        "                print(\"Detailed output for simulation:\", response)\n",
        "                print(\"Code output for simulation:\", inp_content)\n",
        "                with open(f\"main_log_{model_parameter}.txt\", \"a\") as log:\n",
        "                    log.write(f\"Printing responses....\\n\")\n",
        "                    log.write(f\"Detailed output for simulation: \\n {response}\\n \\n\")\n",
        "                    log.write(f\"Code output for simulation: \\n {inp_content}\\n \\n\")\n",
        "                print(\"Final answer for simulation:\", final_answer, '\\n')\n",
        "                with open(f\"inp_final_answer_output_log_{model_parameter}.txt\", \"a\") as log_file:\n",
        "                    log_file.write(f\"Final answer output:\\n{final_answer}\\n\\n\")\n",
        "                    log_file.write(\"...........\")\n",
        "\n",
        "                return {\"output\": inp_content} #inp_content if we want the text to be fed in the simulation software to be evaluated instead\n",
        "            \n",
        "                # if text_code_evaluation: #this evaluates based on the actual code text/inp simulation file content\n",
        "                #     return {\"output\": inp_content}\n",
        "                # else: #this evaluates based on code execution/simulation output\n",
        "                #     return {\"output\": final_answer}\n",
        "            \n",
        "            else:  # For other tools\n",
        "                print(\"ERROR! We shouldn't be here! Returned response from\", model_name, ':', response)\n",
        "                \n",
        "                with open(f\"main_log_{model_parameter}.txt\", \"a\") as log:\n",
        "                    log.write(f\"ERROR! We shouldn't be here! Returned response from {model_name}:\\n{response}\\n \\n\")\n",
        "                    log.write(\"**********\")\n",
        "                    \n",
        "                return {\"output\": response}\n",
        "            \n",
        "        else:\n",
        "            print(\"No tool will be used\")\n",
        "            with open(f\"main_log_{model_parameter}.txt\", \"a\") as log:\n",
        "                log.write(\"No tool will be used\\n\")\n",
        "                log.write(f\"Question is {user_content}\\n\\n\")\n",
        "                \n",
        "             # Default case when not using tools - use the original message format\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": user_content},\n",
        "            ]\n",
        "\n",
        "            # Add system message based on model type - same as original\n",
        "            if 'gemma' not in model_name and 'anthropic' not in model_name and 'openrouter' not in model_name and 'gemini' not in model_name and '/o1' not in model_name:\n",
        "                messages.insert(0, {\"role\": \"system\", \"content\": \"You are a language model specialized in \"+ domain + \" engineering. Answer the following question:\"})\n",
        "            elif 'gemini' in model_name:\n",
        "                messages = {\"role\": \"user\", \"parts\": [{\"text\": \"You are a language model specialized in \"+ domain + \" engineering. Answer the following question: \" + messages[0]['content']}]}\n",
        "            else:  # For gemma add system prompt in user message\n",
        "                messages[0]['content'] = \"You are a language model specialized in \"+ domain + \" engineering. Answer the following question: \" + messages[0]['content']\n",
        "            \n",
        "            response = get_model_response(messages)\n",
        "            \n",
        "            return {\"output\": response}\n",
        "    \n",
        "    else: # Default case when tool_usage is False - use the original message format\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ]\n",
        "\n",
        "        # Add system message based on model type - same as original\n",
        "        if 'gemma' not in model_name and 'anthropic' not in model_name and 'openrouter' not in model_name and 'gemini' not in model_name and '/o1' not in model_name:\n",
        "            messages.insert(0, {\"role\": \"system\", \"content\": \"You are a language model specialized in \"+ domain + \" engineering. Answer the following question:\"})\n",
        "        elif 'gemini' in model_name:\n",
        "            messages = {\"role\": \"user\", \"parts\": [{\"text\": \"You are a language model specialized in \"+ domain + \" engineering. Answer the following question: \" + messages[0]['content']}]}\n",
        "        else:  # For gemma add system prompt in user message\n",
        "            messages[0]['content'] = \"You are a language model specialized in \"+ domain + \" engineering. Answer the following question: \" + messages[0]['content']\n",
        "        \n",
        "        response = get_model_response(messages)\n",
        "        \n",
        "        return {\"output\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate statistics and create plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_statistics(values):\n",
        "    \"\"\"Calculate mean, standard error, and confidence intervals.\"\"\"\n",
        "    mean_value = np.mean(values)  # Mean of the metric over single run and over single metric (but over all questions)\n",
        "    std_error = np.std(values, ddof=1) / np.sqrt(len(values))  # ddof=1 to divide by n-1 to calculate the sample sd\n",
        "    \n",
        "    assert np.std(values, ddof=1) == np.sqrt(np.sum((values-mean_value)**2)/(len(values)-1)), \"Standard deviation calculation mismatch\"\n",
        "    \n",
        "    margin_of_error = 1.96 * std_error  # didn't use t_critical=t.ppf(0.975, df=len(values)-1) since we're using sample standard deviation\n",
        "\n",
        "    return {\n",
        "        'mean': mean_value,\n",
        "        'std_error': std_error,\n",
        "        'ci_low': mean_value - margin_of_error,\n",
        "        'ci_high': mean_value + margin_of_error\n",
        "    }\n",
        "\n",
        "def plot_metric_distributions(metric_values, axes, colors, bin_edges, metric_names):\n",
        "    \"\"\"Plot individual metric distributions with error bars.\"\"\"\n",
        "    error_bars = []\n",
        "    run_stats = {}\n",
        "    \n",
        "    for metric_idx, (metric_name, values) in enumerate(metric_values.items()):  # Loop over runs' metric names and values\n",
        "        clean_metric_name = metric_name.replace('_descr', '')  # This is over one run and over one metric (but over all questions)\n",
        "        metric_name = metric_names[metric_idx]\n",
        "        assert clean_metric_name == metric_name, \"Metric name mismatch\"\n",
        "        \n",
        "        stats = calculate_statistics(values)\n",
        "        sns.histplot(values, bins=bin_edges, color=colors[metric_idx], ax=axes[metric_idx], kde=False)\n",
        "        \n",
        "        #Store error bars\n",
        "        if metric_idx == 0:\n",
        "            error_bars = []\n",
        "        error_bars.append((stats['mean'], axes[metric_idx].get_ylim()[1]/2, stats['ci_high'] - stats['mean']))\n",
        "        \n",
        "        run_stats[metric_name] = stats\n",
        "\n",
        "        axes[metric_idx].set_title(f'{metric_name} (Mean: {stats[\"mean\"]:.2f} ± {stats[\"std_error\"]:.2f} SE, CI: {stats[\"ci_low\"]:.2f}-{stats[\"ci_high\"]:.2f})')\n",
        "        axes[metric_idx].set_xlim(0, 5.5)  # Keep 0 in case of errors\n",
        "        axes[metric_idx].set_ylabel('Frequency')\n",
        "        axes[metric_idx].set_xlabel('Values' if metric_idx == len(metric_values)-1 else '')\n",
        "        \n",
        "    return error_bars, run_stats\n",
        "\n",
        "def plot_question_scores(metric_names, grouped_values, colors):\n",
        "    \"\"\"Plot scores for each question across metrics.\"\"\"\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Define colors for each metric\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(metric_names)))\n",
        "\n",
        "    # First count all frequencies per score (1-5) per metric for one run over all questions\n",
        "    question_scores_by_metric = {metric: [] for metric in metric_names}\n",
        "    score_metric_counts = {}\n",
        "\n",
        "    #Plot each metric's values and store question scores\n",
        "    for i, (metric, question_scores) in enumerate(zip(metric_names, grouped_values)):\n",
        "        width = 0.8 / len(question_scores)  # Width of each metric's bar\n",
        "        \n",
        "        for j, val in enumerate(question_scores): #Create a bar for each question's score\n",
        "            plt.bar(i + j * width, val, width=width, color=colors[i], alpha=0.5, \n",
        "                    label=metric if j == 0 else \"\")\n",
        "                    # i is the index of metric and determines the base position of a group of bars corresponding to that metric.\n",
        "                    # j*width adds an offset to the base position to separate individual bars within the same group (metric). \n",
        "                    # Each j corresponds to a different value in question_scores, creating distinct bars for the values of question_scores for the same metric.\n",
        "                    # By combining the above two, we get the exact x-position of a specific bar     \n",
        "            question_scores_by_metric[metric].append((j, val))\n",
        "\n",
        "        counts = Counter(question_scores)  # Count frequency of each score in question_scores (e.g. {4: 1, 3: 2, 2: 2, 1: 1, 0: 1}, where key is score)\n",
        "        for score, freq in counts.items():\n",
        "            if score not in score_metric_counts:\n",
        "                score_metric_counts[score] = {}\n",
        "            score_metric_counts[score][metric] = freq  #Keeps track of how many times each metric gets a specific score over all questions (for one run)\n",
        "            # {4: {'completeness': 1, 'confidence': 1, 'factuality': 1, 'judgement': 1}, 3: {'completeness': 1, 'relevance': 2, 'conciseness': 2, ....}\n",
        "\n",
        "    return question_scores_by_metric, score_metric_counts\n",
        "\n",
        "def plot_ordered_scores(metric_names, question_scores_by_metric, colors):\n",
        "    \"\"\"Plot metrics ordered by score values.\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    for i, metric in enumerate(metric_names):\n",
        "        plt.subplot(len(metric_names), 1, i+1)\n",
        "        sorted_questions = sorted(question_scores_by_metric[metric], key=lambda x: x[1]) #Sort questions by score\n",
        "        \n",
        "        #Plot bars\n",
        "        x_pos = range(len(sorted_questions))\n",
        "        scores = [q[1] for q in sorted_questions] #q[1] is the score, q[0] is the index\n",
        "        plt.bar(x_pos, scores, color=colors[i], alpha=0.5)\n",
        "\n",
        "        #Add question indices as x-axis labels\n",
        "        plt.xticks(x_pos, [str(q[0]) for q in sorted_questions])\n",
        "        \n",
        "        plt.ylabel(metric)\n",
        "        plt.ylim(0, 5.5)\n",
        "        plt.yticks(range(6))  # Set y-axis ticks from 0 to 5\n",
        "\n",
        "        if i == len(metric_names)-1:\n",
        "            plt.xlabel('Question number (ordered by score)')\n",
        "\n",
        "def plot_accumulated_distributions(score_metric_counts, metric_names, colors):\n",
        "    \"\"\"Plot accumulated distribution of scores by metric.\"\"\"\n",
        "    legend_added = set()\n",
        "\n",
        "    #For each score, plot metrics in order of frequency (highest frequency at bottom)\n",
        "    for score in sorted(score_metric_counts.keys()):\n",
        "        #Sort metrics by frequency for this score\n",
        "        sorted_metrics = sorted(score_metric_counts[score].items(),\n",
        "                            key=lambda x: x[1], #Use the frequency (second element of each tuple) as the sorting key\n",
        "                            reverse=True)  # highest frequency first\n",
        "        bottom = 0\n",
        "        for metric, freq in sorted_metrics:\n",
        "            i = metric_names.index(metric) #get index for color\n",
        "            plt.bar(score, freq,\n",
        "                    width=0.4,\n",
        "                    color=colors[i],\n",
        "                    alpha=0.5,\n",
        "                    label=metric if metric not in legend_added else \"\",\n",
        "                    bottom=bottom)\n",
        "            bottom += freq\n",
        "            legend_added.add(metric)\n",
        "\n",
        "           \n",
        "def plot_figures_metrics(all_runs_model_metrics, metric_names, model_name, judge_model):\n",
        "    \"\"\"\n",
        "    Creates visualizations and calculates statistics for evaluation metrics across multiple runs.\n",
        "\n",
        "    Args:\n",
        "        all_runs_model_metrics (dict): Nested dictionary containing evaluation metrics for each model and run.\n",
        "            Structure: {model_id: [{metric1_descr_run1: [q1_score, q2_score, ...], \n",
        "                                  metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "                                 {metric1_descr_run2: [q1_score, q2_score, ...],\n",
        "                                  metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "                                 ...num_runs]}\n",
        "            Example: {'model1': [{'completeness_descr_run1': [4.5, 3.0, 4.0], \n",
        "                                'relevance_descr_run1': [3.5, 4.0, 3.0]}, ...,\n",
        "                               {'completeness_descr_run2': [4.0, 3.5, 4.5],\n",
        "                                'relevance_descr_run2': [3.0, 4.5, 3.5], ...},\n",
        "                               ...num_runs]}\n",
        "            Where each inner dictionary represents one run containing scores for each metric across all questions\n",
        "        metric_names (list): Names of metrics to analyze and plot (e.g. ['completeness', 'relevance'])\n",
        "        model_name (str): Name/identifier of the model being evaluated\n",
        "        judge_model (str): Name/identifier of the model used for judging the evaluations\n",
        "\n",
        "    Returns:\n",
        "        dict: Summary statistics for each model, run and metric.\n",
        "            Structure: {model_name: {run_idx: {metric_name: {\n",
        "                'mean': float,\n",
        "                'std_error': float, \n",
        "                'ci_low': float,\n",
        "                'ci_high': float\n",
        "            }}}}\n",
        "            Example: {'anthropic/claude-3-5-sonnet': {\n",
        "                '0': {'completeness': {'mean': 4.5, 'std_error': 0.5, \n",
        "                                     'ci_low': 3.52, 'ci_high': 5.48},\n",
        "                      'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "                                  'ci_low': 2.52, 'ci_high': 4.48} , ...},\n",
        "                '1': {'completeness': {'mean': 4.5, 'std_error': 0.5,\n",
        "                                     'ci_low': 3.52, 'ci_high': 5.48},\n",
        "                      'relevance': {'mean': 3.5, 'std_error': 0.5,\n",
        "                                  'ci_low': 2.52, 'ci_high': 4.48}, ...},\n",
        "                ...num_runs}}\n",
        "\n",
        "    The function generates several visualization types:\n",
        "    - Individual histograms for each metric showing score distributions\n",
        "    - Error bars indicating means and confidence intervals\n",
        "    - Overlapping bar plots comparing metrics\n",
        "    - Stacked distribution plots showing relative frequencies of scores\n",
        "\n",
        "    All plots are saved as PNG files with names indicating the judge model,\n",
        "    evaluated model, run index, and plot type.\n",
        "    \"\"\"\n",
        "\n",
        "    summary_stats_all_runs = {}  # Keep track of summary statistics over all runs\n",
        "\n",
        "    for run_idx, metric_values_run in enumerate(all_runs_model_metrics[model_name]): #Loop over runs\n",
        "\n",
        "        colors = sns.color_palette(\"Set3\", len(metric_names))\n",
        "        \n",
        "        # Create two figures - one with separate subplots and one overlaid\n",
        "        fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 18))\n",
        "        plt.subplots_adjust(hspace=0.6, top=0.94)\n",
        "        fig.suptitle(f'Metric Distributions for {model_name} (Run {run_idx})', fontsize=16)\n",
        "        \n",
        "        bin_edges = np.arange(0.0, 5.6, 0.2)  # Bins for range 0-5\n",
        "        metric_names = [name.replace('_descr', '') for name in metric_values_run]\n",
        "        \n",
        "        error_bars, run_stats = plot_metric_distributions(metric_values_run, axes, colors, bin_edges, metric_names)\n",
        "        \n",
        "        # Save version without error bars\n",
        "        plt.figure(fig.number)\n",
        "        judge_name = \"_\".join(judge_model.split('/')[1:])\n",
        "        plt.savefig(f\"{judge_name}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions_no_error_bars.png\")\n",
        "        \n",
        "        # Add error bars and save updated version\n",
        "        for i, (mean, ylim, margin) in enumerate(error_bars):\n",
        "            axes[i].errorbar(mean, ylim, xerr=margin, color='black', capsize=5, \n",
        "                           capthick=1, elinewidth=2, marker='o')\n",
        "        \n",
        "        plt.savefig(f\"{judge_name}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_metric_distributions.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Print summary statistics - Can also be seen in txt file. \n",
        "        print(f\"\\nSummary Statistics over run {run_idx}:\")\n",
        "        print(\"-\" * 50)\n",
        "        for metric, stats in run_stats.items():\n",
        "            print(f\"{metric}:\")\n",
        "            for key, value in stats.items():\n",
        "                print(f\"  {key}: {value:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        summary_stats_all_runs[run_idx] = run_stats #For one run\n",
        "\n",
        "        grouped_values=list(metric_values_run.values()) #Values of all metrics for one run over all questions. There are num_metrics lists in that list. \n",
        "        values = [val for sublist in grouped_values for val in sublist] #Flatten the list - Size is num_questions*num_metrics (1st metric questions, 2nd metric questions, etc)\n",
        "        \n",
        "        question_scores_by_metric, score_metric_counts = plot_question_scores(metric_names, grouped_values, colors)\n",
        "        plt.xlabel('Metrics')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Per-Metric Question Scores Distribution')\n",
        "        plt.xticks(np.arange(len(metric_names)) + 0.1, metric_names)\n",
        "        plt.yticks(range(6)) #Set y-ticks to 0-5\n",
        "        plt.savefig(f\"{judge_name}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_per_metric_question_scores.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Plot ordered scores\n",
        "        plot_ordered_scores(metric_names, question_scores_by_metric, colors)\n",
        "        plt.suptitle('Question indices ordered by metric value')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{judge_name}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_question_indices_ordered_by_metric_value.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "        # Plot accumulated distributions\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plot_accumulated_distributions(score_metric_counts, metric_names, colors)\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Score Distribution Histogram by Metric') \n",
        "        plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.xticks(np.arange(0, 6))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{judge_name}_judge_with_{model_name.replace('/', '_')}_run_{run_idx}_score_distribution_histogram_by_metric.png\")\n",
        "        plt.close('all')\n",
        "\n",
        "    return summary_stats_all_runs\n",
        "\n",
        "def plot_model_comparison(models, metrics, metric_means, metric_stds, save_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Plot comparison charts for multiple models across different metrics.\n",
        "    \n",
        "    Args:\n",
        "        models (list): List of model names to compare\n",
        "        metrics (list): List of metric names to display\n",
        "        metric_means (dict): Dictionary mapping metrics to lists of mean values for each model\n",
        "        metric_stds (dict): Dictionary mapping metrics to lists of standard deviation values for each model\n",
        "        save_prefix (str, optional): Prefix for saved image filenames\n",
        "    \n",
        "    Returns:\n",
        "        None: Plots are displayed and saved to files\n",
        "    \"\"\"\n",
        "    # Your preferred colors first\n",
        "    base_colors = ['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b','#e377c2']\n",
        "\n",
        "    # If we need more colors than in the base list\n",
        "    if len(models) > len(base_colors):\n",
        "        # Generate the additional colors needed\n",
        "        extra_needed = len(models) - len(base_colors)\n",
        "        hsv_colors = [(i/extra_needed, 0.8, 0.8) for i in range(extra_needed)]\n",
        "        rgb_colors = [mcolors.hsv_to_rgb(hsv) for hsv in hsv_colors]\n",
        "        extra_colors = [mcolors.to_hex(rgb) for rgb in rgb_colors]\n",
        "        \n",
        "        # Combine base colors with extra colors\n",
        "        model_colors = base_colors + extra_colors\n",
        "    else:\n",
        "        # Use just the base colors up to the number needed\n",
        "        model_colors = base_colors[:len(models)]\n",
        "\n",
        "    # Plot 1: Grid of metrics\n",
        "    # Calculate the number of rows and columns needed for the subplots\n",
        "    num_metrics = len(metrics)\n",
        "    num_cols = 3  # Keep 3 columns as in original\n",
        "    num_rows = (num_metrics + num_cols - 1) // num_cols  # Ceiling division to ensure enough subplots\n",
        "    \n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(16, 5 * num_rows))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        means = metric_means[metric]\n",
        "        stds = metric_stds[metric]\n",
        "        \n",
        "        # Filter out zero values\n",
        "        valid_indices = [j for j, mean in enumerate(means) if mean > 0]\n",
        "        valid_means = [means[j] for j in valid_indices]\n",
        "        valid_stds = [stds[j] for j in valid_indices]\n",
        "        valid_colors = [model_colors[j] for j in valid_indices]\n",
        "        valid_model_labels = [models[j] for j in valid_indices]\n",
        "        \n",
        "        # Create new x positions without gaps\n",
        "        valid_x = np.arange(len(valid_indices))\n",
        "        \n",
        "        bars = axs[i].bar(valid_x, valid_means, yerr=valid_stds, capsize=5, color=valid_colors)\n",
        "        axs[i].set_title(metric)\n",
        "        axs[i].set_xticks(valid_x)\n",
        "        axs[i].set_xticklabels(valid_model_labels, rotation=45, ha='right')\n",
        "        axs[i].set_ylim(0, 6.2)  # higher to accommodate error bar labels\n",
        "        axs[i].set_yticks(np.arange(0, 6, 1))\n",
        "        axs[i].grid(axis='y', linestyle='dotted', color='gray', linewidth=0.8)\n",
        "        if i in [0, 3]:\n",
        "            axs[i].set_ylabel(\"Score\")\n",
        "        if i >= 3:\n",
        "            axs[i].set_xlabel(\"LLM\")\n",
        "        for j, bar in enumerate(bars):\n",
        "            top = valid_means[j] + valid_stds[j]\n",
        "            axs[i].text(bar.get_x() + bar.get_width() / 2, top + 0.05, f\"{valid_means[j]:.2f}\",\n",
        "                        ha='center', va='bottom', fontsize=9, rotation=90)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(num_metrics, len(axs)):\n",
        "        axs[i].set_visible(False)\n",
        "\n",
        "    fig.suptitle(\"Metric Comparison Across LLMs (± std dev)\", fontsize=16)\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(f\"metric_comparison_grid_judge_{save_prefix}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot 2: Grouped bar chart\n",
        "    width = 0.12\n",
        "    fig, ax = plt.subplots(figsize=(18, 7))\n",
        "\n",
        "    # Ensure we have enough colors for all metrics\n",
        "    metric_colors = ['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b']\n",
        "    if len(metrics) > len(metric_colors):\n",
        "        # Generate additional colors if needed\n",
        "        extra_needed = len(metrics) - len(metric_colors)\n",
        "        hsv_colors = [(i/extra_needed, 0.8, 0.8) for i in range(extra_needed)]\n",
        "        rgb_colors = [mcolors.hsv_to_rgb(hsv) for hsv in hsv_colors]\n",
        "        extra_colors = [mcolors.to_hex(rgb) for rgb in rgb_colors]\n",
        "        metric_colors = metric_colors + extra_colors\n",
        "    else:\n",
        "        metric_colors = metric_colors[:len(metrics)]\n",
        "        \n",
        "    max_y = 0\n",
        "\n",
        "    # Get all valid model indices (models with at least one non-zero metric)\n",
        "    all_valid_indices = set()\n",
        "    for metric in metrics:\n",
        "        for j, mean in enumerate(metric_means[metric]):\n",
        "            if mean > 0:\n",
        "                all_valid_indices.add(j)\n",
        "\n",
        "    all_valid_indices = sorted(list(all_valid_indices))\n",
        "    valid_models = [models[j] for j in all_valid_indices]\n",
        "\n",
        "    # Create new x positions without gaps\n",
        "    x = np.arange(len(all_valid_indices))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        means = metric_means[metric]\n",
        "        stds = metric_stds[metric]\n",
        "        \n",
        "        # Filter out zero values but maintain position for valid models\n",
        "        valid_means = []\n",
        "        valid_stds = []\n",
        "        valid_positions = []\n",
        "        \n",
        "        for idx, j in enumerate(all_valid_indices):\n",
        "            if means[j] > 0:\n",
        "                valid_means.append(means[j])\n",
        "                valid_stds.append(stds[j])\n",
        "                valid_positions.append(x[idx])\n",
        "        \n",
        "        # Skip if no valid data for this metric\n",
        "        if not valid_means:\n",
        "            continue\n",
        "            \n",
        "        offset = (i - len(metrics)/2 + 0.5) * width\n",
        "        positions = [pos + offset for pos in valid_positions]\n",
        "        \n",
        "        bars = ax.bar(positions, valid_means, width, yerr=valid_stds, label=metric, color=metric_colors[i], capsize=4)\n",
        "        for j, bar in enumerate(bars):\n",
        "            top = valid_means[j] + valid_stds[j]\n",
        "            max_y = max(max_y, top)\n",
        "            ax.text(bar.get_x() + bar.get_width() / 2, top + 0.1, f\"{valid_means[j]:.2f}\",\n",
        "                    ha='center', va='bottom', fontsize=9, rotation=90)\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('LLM Metric Comparison (Mean ± Std Dev)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(valid_models, rotation=45, ha='right')\n",
        "    ax.set_yticks(np.arange(0, 6, 1))\n",
        "    ax.set_ylim(0, max_y + 0.5)\n",
        "    ax.grid(axis='y', linestyle='dotted', color='gray', linewidth=0.8)\n",
        "    ax.legend(title='Metric', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "    plt.tight_layout(rect=[0, 0, 0.88, 0.97])\n",
        "    plt.savefig(f\"metric_comparison_summary_by_LLM_judge_{save_prefix}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_spider_chart(models, metrics, metric_means, save_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Plot a spider chart comparing multiple models across different metrics.\n",
        "    \n",
        "    Args:\n",
        "        models (list): List of model names to compare\n",
        "        metrics (list): List of metric names to display\n",
        "        metric_means (dict): Dictionary mapping metrics to lists of mean values for each model\n",
        "        save_prefix (str, optional): Prefix for saved image filenames\n",
        "    \"\"\"\n",
        "    # Filter out models with zero values\n",
        "    all_valid_indices = set()\n",
        "    for metric in metrics:\n",
        "        means = metric_means[metric]\n",
        "        for j, mean in enumerate(means):\n",
        "            if mean > 0:\n",
        "                all_valid_indices.add(j)\n",
        "\n",
        "    all_valid_indices = sorted(list(all_valid_indices))\n",
        "    valid_models = [models[j] for j in all_valid_indices]\n",
        "\n",
        "    N = len(metrics)\n",
        "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "    ax.set_theta_offset(np.pi / 2)\n",
        "    ax.set_theta_direction(-1)\n",
        "\n",
        "    plt.xticks(angles[:-1], metrics, fontsize=12)\n",
        "\n",
        "    ax.set_rlabel_position(0)\n",
        "    plt.yticks(np.arange(1, N), [str(i) for i in range(1, N)], color=\"grey\", size=8)\n",
        "    plt.ylim(0, N)\n",
        "\n",
        "    markers = ['o', 's', '^', 'D', 'v', 'p', 'h', '*', 'x', '+'][:len(valid_models)]\n",
        "    \n",
        "    for i, model_idx in enumerate(all_valid_indices):\n",
        "        values = []\n",
        "        for metric in metrics:\n",
        "            values.append(metric_means[metric][model_idx])\n",
        "        values += values[:1]  # repeat first value to close the loop\n",
        "        ax.plot(angles, values, linewidth=2, linestyle='solid', marker=markers[i], label=valid_models[i])\n",
        "        ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "    plt.title('LLM Performance Comparison', size=16, y=1.08)\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "\n",
        "    plt.savefig(f\"spider_chart_judge_{save_prefix}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform the Evaluation over all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
        "def load_model_stats(judge_model): #In case we had to restart the loop - some models didn't run - Keep track of all model stats\n",
        "    \"\"\"Load existing stats from files or initialize empty dictionaries.\"\"\"\n",
        "    judge_name = \"_\".join(judge_model.split('/')[1:])\n",
        "    print(f\"Checking if we can load stats for {judge_name}\")\n",
        "    try:\n",
        "        with open(f'stats_{judge_name}.json', 'r') as f:\n",
        "            all_models_stats = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_models_stats = {}  # Used in comparison between models\n",
        "\n",
        "    try: # a dict with num_models keys, each having a list with one dict. The dict has num_metrics keys,\n",
        "        # and each key has a list with number_questions values like so: {\"openai/o1-mini\": [{\"completeness_descr\": [5, 0, 0],...\n",
        "        with open(f'all_runs_model_metrics_{judge_name}.json', 'r') as f:\n",
        "            all_runs_model_metrics = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_runs_model_metrics = {}  # Used in plotting metrics\n",
        "        \n",
        "    return all_models_stats, all_runs_model_metrics\n",
        "\n",
        "def perform_evaluation(model_id, judge_model, n_resamples, example_inputs, factor_evaluator, langsmith_api_key, use_RAG=False, use_smolagents=False):\n",
        "    \"\"\"Perform evaluation runs and collect results.\"\"\"\n",
        "    global vectorstore, reranker\n",
        "    dataset_name = get_dataset_name(model_id, judge_model, use_RAG=use_RAG, use_smolagents=use_smolagents) #How the dataset will be named in Langsmith\n",
        "    dataset_langsmith = create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key)\n",
        "    results_df, list_of_questions, vectorstore, reranker = process_evaluation_results(langsmith_api_key, dataset_langsmith, use_RAG=use_RAG)\n",
        "    print(\"Vectorstore:\",vectorstore)\n",
        "    print(\"List of questions:\",list_of_questions)\n",
        "    model_name = \"_\".join(model_id.split('/')[1:])\n",
        "    judge_name = \"_\".join(judge_model.split('/')[1:])\n",
        "    with open('vectorstore_'+str(model_name)+'_judge_'+str(judge_name)+'.txt', 'a') as f:\n",
        "        f.write(str(vectorstore))\n",
        "        try:\n",
        "            f.write(f\"actual variable {vectorstore()}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    evaluation_all_resamples = [] #Used below to obtain the unique questions/answers and also the results of each resample\n",
        "    \n",
        "    begin = time.time()\n",
        "    for resample_idx in range(n_resamples):\n",
        "        print(f\"\\nPerforming evaluation of resample {resample_idx+1}/{n_resamples} of {model_id}\")\n",
        "\n",
        "        max_retries = 1 #try only once if connection issues\n",
        "        backoff_factor = 5\n",
        "        attempt_langsmith = 0\n",
        "        while True:\n",
        "            try:\n",
        "                evaluation_results = evaluate(\n",
        "                    predict, #Function that call our LLM and returns its output\n",
        "                    data=dataset_langsmith.name, #Just using dataset_langsmith doesn't work \n",
        "                    evaluators=[factor_evaluator], #Evaluators to use\n",
        "                    max_concurrency=1, #Run one question through langsmith each time - Other values will give errors in resulting excels\n",
        "                    # metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
        "                    experiment_prefix=str(judge_model) + '_judge_with_' + str(model_id) + '_resample_' + str(resample_idx) # A prefix for your experiment names to easily identify them\n",
        "                )\n",
        "                break\n",
        "            except (LangSmithConnectionError, requests.exceptions.ConnectionError) as e:\n",
        "                if attempt_langsmith >= max_retries:\n",
        "                    raise\n",
        "                wait_time = backoff_factor * (2 ** attempt_langsmith)\n",
        "                print(f\"[Retry {attempt_langsmith+1}/{max_retries}] LangSmith connection failed: {e}. Retrying in {wait_time}s...\")\n",
        "                with open(\"retry_log.txt\", \"a\") as log:\n",
        "                    log.write(f\"[Retry {attempt_langsmith+1}/{max_retries}] LangSmith connection failed: {e}. Retrying in {wait_time}s...\")\n",
        "                    log.write(\"\\n **********\")\n",
        "                with open(\"retry_eval_log.txt\", \"a\") as log:\n",
        "                    try:\n",
        "                        log.write(f\"Evaluation results: \\n {evaluation_results}\")\n",
        "                    except Exception as e:\n",
        "                        log.write(f\"Unable to write evaluation results to log file: {e}\")\n",
        "                    log.write(\"\\n **********\")\n",
        "                time.sleep(wait_time)\n",
        "                attempt_langsmith += 1\n",
        "    \n",
        "        evaluation_all_resamples.extend(evaluation_results) #Used below to get unique questions/answers and to select the predicted answers\n",
        "        #After the loop, we get a list with n_resamples*num_questions elements, for just one model (and only for main judge)\n",
        "\n",
        "    with open('evaluation_all_resamples_'+str(model_name)+'_judge_'+str(judge_name)+'.txt', 'w') as f:\n",
        "        f.write(str(evaluation_all_resamples))\n",
        "\n",
        "    assert len(evaluation_all_resamples)==n_resamples*len(example_inputs), f\"Number of evaluation results not matching num_resamples*num_questions. \\\n",
        "        Got {len(evaluation_all_resamples)} evaluation results but expected {n_resamples*len(example_inputs)}\"\n",
        "    \n",
        "    print(f\"Total time for evaluation: {time.time() - begin}\")\n",
        "\n",
        "    return evaluation_all_resamples, dataset_langsmith, results_df, list_of_questions, vectorstore, reranker\n",
        "\n",
        "def process_evaluation_results(langsmith_api_key, dataset_langsmith, use_RAG=False):\n",
        "    \"\"\"Extract questions and answers from evaluation results.\"\"\"\n",
        "    #https://docs.smith.langchain.com/tutorials/Developers/evaluation\n",
        "\n",
        "    # Get unique questions/answers\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "    questions_answers=[x for x in client.list_examples(dataset_id=dataset_langsmith.id)]\n",
        "    list_of_questions=[x.inputs['question'] for x in questions_answers]\n",
        "    list_of_answers=[x.outputs['answer'] for x in questions_answers]\n",
        "        \n",
        "    # with open('list_of_questions.txt', 'w') as f:\n",
        "    #     f.write(str(list_of_questions))\n",
        "    # with open('list_of_answers.txt', 'w') as f:\n",
        "    #     f.write(str(list_of_answers))\n",
        "\n",
        "    if use_RAG==True:\n",
        "        # Initialize vectorstore\n",
        "        vectorstore = initialize_vectorstore(list_of_questions, list_of_answers, embedding_model)\n",
        "        # Initialize reranker\n",
        "        reranker = initialize_reranker(reranker_model_name)\n",
        "    \n",
        "    results_df = pd.DataFrame({\n",
        "        'questions': list_of_questions,\n",
        "        'answers': list_of_answers\n",
        "    })\n",
        "\n",
        "    if use_RAG:\n",
        "        return results_df, list_of_questions, vectorstore, reranker\n",
        "    else:\n",
        "        return results_df, list_of_questions, None, None\n",
        "\n",
        "def process_metrics(resample_results, list_of_metrics, list_of_questions, resample_idx, results_df, model_name, indices_to_reorder):\n",
        "    \"\"\"\n",
        "    Process metrics for a single resample and update results DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        resample_results: Results from current resample\n",
        "        list_of_metrics: List of metrics to process\n",
        "        resample_idx: Current resample index\n",
        "        results_df: DataFrame to update with metrics\n",
        "        model_name: Name of the model being evaluated\n",
        "        indices_to_reorder: Indices to reorder the metrics\n",
        "        \n",
        "    Returns:\n",
        "        individual_run_metric_scores, metrics, results_df\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = [] #This should be the same as resample_results (list) except when there are 'traceback' errors where it will be 0.\n",
        "    # metrics format will be:[[EvaluationResult(key='completeness', score=4, value='To evaluate the .... - It has num_questions sublists, each with num_metrics values\n",
        "\n",
        "    model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "\n",
        "    for result in resample_results:\n",
        "        if result['run'].outputs['output'] is None or not result['evaluation_results']['results']: #or result['run'].error is not None - Same as first condition\n",
        "            metrics.append(0)  # Use 0 to indicate failed evaluation - We might even get in here when LangSmith API connection issues\n",
        "            print(\"Error: No metric value found!\")\n",
        "            #Also print which condition is true\n",
        "            print(\"result['run'].outputs['output'] is None\",result['run'].outputs['output'] is None)\n",
        "            print(\"not result['evaluation_results']['results']\",not result['evaluation_results']['results'])\n",
        "            # Log the error conditions to a file\n",
        "            with open('error_conditions_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                f.write(\"Error: No metric value found! \\n\")\n",
        "                f.write(f\"result['run'].outputs['output'] is None: {result['run'].outputs['output'] is None}\\n\")\n",
        "                f.write(f\"not result['evaluation_results']['results']: {not result['evaluation_results']['results']}\\n\")\n",
        "                f.write(f\"result['evaluation_results']['results'] {result['evaluation_results']['results']}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "        else:\n",
        "            metrics.append(result['evaluation_results']['results'])\n",
        "    \n",
        "    # Reorder metrics based on indices_to_reorder\n",
        "    reordered_metrics = [metrics[i] for i in indices_to_reorder]\n",
        "    metrics = reordered_metrics\n",
        "\n",
        "    assert len(resample_results)==len(list_of_questions), f\"Number of resample results not matching num_questions. Got {len(resample_results)} resample \\\n",
        "        results but expected {len(list_of_questions)}\"\n",
        "    #Format is [{'run': RunTree(id=UUID('b7aea73... and there are num_questions runs. There are multiple files, one for each model and one for each resample (for main judge only)\n",
        "    \n",
        "    #A list with num_questions sublists, each with num_metrics values in the format:\n",
        "    #[EvaluationResult(key='completeness', score=5, value='To evaluate\n",
        "    with open('process_metrics_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'w') as f:\n",
        "        f.write(str(metrics))\n",
        "\n",
        "    assert len(metrics)==len(list_of_questions), f\"Number of metrics not matching num_questions. Got {len(metrics)} metrics but expected {len(list_of_questions)}\"\n",
        "    \n",
        "    #This is at the end a dict with num_metrics keys and each key has num_questions values.\n",
        "    #Example: {'completeness_descr': [4, 3, 3, 5, 5, 4, 3], 'relevance_descr': [4, 3, 3, 3, 4, 3, 1], ....} assuming 7 questions\n",
        "    individual_run_metric_scores = {} #Keep track of scores of all metrics over all questions for one resample\n",
        "\n",
        "    for metric_idx, metric_name in enumerate(list_of_metrics): #Get specific metric name and values over all questions for the current resample\n",
        "\n",
        "        clean_metric_names, metric_scores, metric_prompts = [], [], [] #Metric scores and prompts for all questions for a given resample - Should be num_questions elements each time\n",
        "        \n",
        "        #Get all metric keys for the current resample over all questions, handling potential missing keys (values set to 0 for those - they are errors)\n",
        "        for m in metrics:\n",
        "            if m == 0: #If there is an error\n",
        "                key = metric_name.replace('_descr','')\n",
        "                score = 0\n",
        "                prompt=\"\"\n",
        "            else:\n",
        "                try:\n",
        "                    key = m[metric_idx].key #Metric name\n",
        "                    score = m[metric_idx].score ##Scores of a given metric over all questions for a given resample\n",
        "                    prompt = m[metric_idx].value #Prompt used for the evaluation\n",
        "                except:\n",
        "                    print(\"Error: Metric not found - Shouldn't get here\")\n",
        "                    with open('error_conditions_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                        f.write(\"Error: Metric not found - Shouldn't get here \\n\")\n",
        "                    key = metric_name.replace('_descr','')\n",
        "                    score = 0\n",
        "                    prompt = \"\"\n",
        "                \n",
        "            clean_metric_names.append(key)\n",
        "            metric_scores.append(score)\n",
        "            metric_prompts.append(prompt)\n",
        "            \n",
        "        assert all(name == metric_name.replace('_descr','') for name in clean_metric_names), f\"Metric keys not matching: clean_metric_names={clean_metric_names}, \\\n",
        "            expected={metric_name.replace('_descr','')} and their values: {metric_scores}\"\n",
        "            \n",
        "        assert len(metric_scores)==len(list_of_questions), f\"Number of metric scores not matching num_questions. Got {len(metric_scores)} metric scores \\\n",
        "            but expected {len(list_of_questions)}\"\n",
        "            \n",
        "        assert len(metric_prompts)==len(list_of_questions), f\"Number of metric prompts not matching num_questions. Got {len(metric_prompts)} metric prompts \\\n",
        "            but expected {len(list_of_questions)}\"\n",
        "\n",
        "        # Update results DataFrame\n",
        "        clean_metric_name = clean_metric_names[0] #Just one metric name without the _descr\n",
        "        results_df[f'metric_{clean_metric_name}_{resample_idx+1}'] = metric_scores\n",
        "        results_df[f'prompt_{clean_metric_name}_{resample_idx+1}'] = metric_prompts\n",
        "        \n",
        "        # Store scores for return\n",
        "        individual_run_metric_scores[metric_name] = metric_scores #len is num_metrics\n",
        "\n",
        "    return individual_run_metric_scores, metrics, results_df\n",
        "\n",
        "def process_metrics_second_judge(excel_path, list_of_metrics, n_resamples, model_name, question_col=\"questions\", judge_model_2=judge_model):\n",
        "    \"\"\"\n",
        "    Loads the Excel file and processes metrics and prompts for the second judge model.\n",
        "    Returns:\n",
        "        - all_run_metric_scores: list of dicts, one per resample, {metric: [scores for all questions]}\n",
        "        - all_run_metric_prompts: list of dicts, one per resample, {metric: [prompts for all questions]}\n",
        "    \"\"\"\n",
        "    if judge_model_2!='openai/gpt4o-mini':\n",
        "        df = pd.read_excel(excel_path)\n",
        "        all_run_metric_scores = []\n",
        "        all_run_metric_prompts = []\n",
        "        num_questions = len(df[question_col])\n",
        "\n",
        "        at_least_one_nan = False\n",
        "\n",
        "        for resample_idx in range(n_resamples):\n",
        "            run_scores = {}\n",
        "            run_prompts = {}\n",
        "            for metric_name in list_of_metrics:\n",
        "                clean_name = metric_name.replace('_descr', '')\n",
        "                judge_name = \"_\".join(judge_model_2.split('/')[1:])\n",
        "                score_col = f\"metric_{clean_name}_{resample_idx+1}_{judge_name}\"\n",
        "                prompt_col = f\"prompt_{clean_name}_{resample_idx+1}_{judge_name}\"\n",
        "                if score_col in df.columns:\n",
        "                    flag_nan = False\n",
        "                    if df[score_col].isna().any():\n",
        "                        flag_nan = True\n",
        "                        at_least_one_nan = True\n",
        "                        # with open(f\"Column_scores_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                        #     col_file.write(f\"Found NaN values in {score_col} before filling: {df[score_col][df[score_col].isna()].index.tolist()}\\n\")\n",
        "                        with open(f\"Column_scores_nan_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                            col_file.write(f\"Score col: {score_col} and values are \\n {df[score_col]} \\n \\n\")\n",
        "                    \n",
        "                    df[score_col] = df[score_col].fillna(0)\n",
        "                    run_scores[metric_name] = df[score_col].astype(int).tolist()\n",
        "\n",
        "                    # if flag_nan: #Final output of this function saved elsewhere\n",
        "                    #     with open(f\"Column_scores_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                    #         col_file.write(f\"After conversion to int Score col: {score_col} and values are \\n {df[score_col]} \\n \\n\")\n",
        "   \n",
        "                else:\n",
        "                    run_scores[metric_name] = [None] * num_questions\n",
        "                    print(f\"Metric {metric_name} not found in column {score_col}\")\n",
        "                    with open(f\"Column_scores_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                        col_file.write(f\"Metric {metric_name} not found in column {score_col} \\n \\n\")\n",
        "                if prompt_col in df.columns:\n",
        "                    run_prompts[metric_name] = df[prompt_col].tolist()\n",
        "                else:\n",
        "                    run_prompts[metric_name] = [None] * num_questions\n",
        "                    print(f\"Prompt {metric_name} not found in column {prompt_col}\")\n",
        "                    with open(f\"Column_scores_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                        col_file.write(f\"Prompt {metric_name} not found in column {prompt_col} \\n \\n\")\n",
        "\n",
        "            all_run_metric_scores.append(run_scores)\n",
        "            all_run_metric_prompts.append(run_prompts)\n",
        "        \n",
        "        if at_least_one_nan:\n",
        "            model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "            with open(f\"Column_scores_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                col_file.write(f\"####### Judge model: {model_parameter}...... \\n \\n\")\n",
        "\n",
        "        return all_run_metric_scores, all_run_metric_prompts\n",
        "    else:\n",
        "        return [], []\n",
        "\n",
        "def calculate_metric_statistics(all_runs_metric_scores, list_of_metrics, num_questions, model_name, judge_model):\n",
        "    \"\"\"Calculate statistical metrics across resamples (reduce variance - step 3.1).\"\"\"\n",
        "    metric_stats_resampling = {} # Calculate mean and standard error for each metric and question across K resamples\n",
        "    #The above dict will have num_metrics elements, each with metric keys (e.g. mean, std, etc), that will have num_questions values\n",
        "    #Example: {'completeness': {'mean':[4, 3, 3, 5, 5, 4, 3]}, #here for a dataset with 7 questions\n",
        "    #          'relevance': {'mean':[4, 3, 3, 3, 4, 3, 2]}, ...}\n",
        "    \n",
        "    for metric in list_of_metrics:\n",
        "        metric_stats_resampling[metric] = {\n",
        "            'means': [],  # Mean score across K resamples for each question\n",
        "            'standard_errors': [],  # Standard error of the mean for each question\n",
        "            'conditional_vars': []  # Conditional variance reduced by factor of K\n",
        "        }\n",
        "        \n",
        "        # For each question\n",
        "        for q in range(num_questions):\n",
        "            # Get K scores for this metric/question across all resamples (num_resamples elements each time in that list)\n",
        "            scores = [run[metric][q] for run in all_runs_metric_scores]\n",
        "            K = len(scores)  # Number of resamples\n",
        "            assert len(scores)==n_resamples, f\"Number of scores not matching num_resamples. Got {len(scores)} scores but expected {n_resamples}\"\n",
        "            \n",
        "            # Calculate statistics\n",
        "            mean = np.mean(scores) #Average score of each question for a given metric over all resamples\n",
        "            var = np.var(scores) #Variance of the scores of each question for a given metric over all resamples\n",
        "            # Calculate conditional variance reduced by factor of K. Var(mean) = σ²/K where σ² is the variance of individual scores\n",
        "            conditional_var = var / K if K > 0 else 0\n",
        "            standard_error = np.sqrt(conditional_var)\n",
        "            \n",
        "            # Store results\n",
        "            metric_stats_resampling[metric]['means'].append(mean)\n",
        "            metric_stats_resampling[metric]['standard_errors'].append(standard_error)\n",
        "            metric_stats_resampling[metric]['conditional_vars'].append(conditional_var)\n",
        "\n",
        "    model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "    try:\n",
        "        judge_name = \"_\".join(judge_model.split('/')[1:]) if '/' in judge_model else judge_model\n",
        "        with open('metric_stats_resampling_'+str(model_parameter)+'_judge_'+str(judge_name)+'.txt', 'w') as f:\n",
        "            f.write(f\"judge is: {judge_name} \\n\")\n",
        "            f.write(str(metric_stats_resampling))\n",
        "    except:\n",
        "        print(f\"ERROR: Metric stats resampling not found for {model_parameter} and judge\")\n",
        "        print(\"metric_stats_resampling is\",metric_stats_resampling)\n",
        "        with open(f\"ERROR_Calculate_metric_statistics_{model_parameter}.txt\", \"a\") as col_file:\n",
        "            col_file.write(f\"ERROR: Metric stats resampling not found for {model_parameter} and judge \\n\")\n",
        "        with open('metric_stats_resampling_'+str(model_parameter)+'.txt', 'w') as f:\n",
        "            f.write(f\"judge in the except is (and that's why error): {judge_model} \\n\")\n",
        "            f.write(\"try to write metric_stats_resampling as simple string \\n\")\n",
        "            f.write(str(metric_stats_resampling))\n",
        "\n",
        "    assert len(metric_stats_resampling)==len(list_of_metrics), f\"Number of metric_stats_resampling not matching num_metrics. \\\n",
        "        Got {len(metric_stats_resampling)} metric_stats_resampling but expected {len(list_of_metrics)}\"\n",
        "    \n",
        "    for metric in list_of_metrics:\n",
        "        assert len(metric_stats_resampling[metric]['means']) == num_questions, f\"Number of values for metric '{metric}' ({len(metric_stats_resampling[metric]['means'])}) \\\n",
        "            not matching expected number of questions ({num_questions})\"\n",
        "\n",
        "    return metric_stats_resampling\n",
        "\n",
        "def handle_zero_values(results_df, n_resamples, list_of_metrics, model_name): #Need to be changed for second judge\n",
        "    \"\"\"\n",
        "    Handle zero values in results.\n",
        "    \n",
        "    Args:\n",
        "        results_df (pd.DataFrame): DataFrame containing results\n",
        "        n_resamples (int): Number of resamples\n",
        "        list_of_metrics (list): List of metrics to check\n",
        "        model_name (str): Name of the model being evaluated\n",
        "        \n",
        "    Returns:\n",
        "        dict: Indices of rows containing zero values for each metric\n",
        "    \"\"\"\n",
        "    zero_rows_columns = {}\n",
        "    \n",
        "    try:\n",
        "        # Handle 0 values across all resamples - These are errors\n",
        "        for resample_idx in range(n_resamples):\n",
        "            for metric in list_of_metrics:\n",
        "                try:\n",
        "                    simple_metric_name = metric.replace('_descr','')\n",
        "                    metric_col = f'metric_{simple_metric_name}_{resample_idx+1}'\n",
        "                    \n",
        "                    # Check if column exists\n",
        "                    if metric_col not in results_df.columns:\n",
        "                        print(colored(f\"Warning: Column {metric_col} not found in DataFrame\", 'yellow'))\n",
        "                        continue\n",
        "                    \n",
        "                    zero_indices = results_df[metric_col] == 0 #series with True/False\n",
        "                    \n",
        "                    if zero_indices.any(): #If any of the values of that column are 0\n",
        "                        zero_rows_columns[metric_col] = []\n",
        "                        for idx in zero_indices[zero_indices].index: #Loop over True indices (rows with 0s)\n",
        "                            try:\n",
        "                                print(colored(f\"Missing value for metric '{simple_metric_name}' in resample {resample_idx+1} of model {'_'.join(model_name.split('/')[1:])}\", 'red'))\n",
        "                                print(colored(f\"Question: {results_df.loc[idx, 'questions']}\", 'green'))\n",
        "                                zero_rows_columns[metric_col].append(idx) #Keep track of columns and rows with zero values\n",
        "\n",
        "                                with open(f\"Handle_zero_values.txt\", \"a\") as col_file:\n",
        "                                    # Write all zero value information to file\n",
        "                                    col_file.write(f\"\\nMissing value for metric '{simple_metric_name}' in resample {resample_idx+1} of model {'_'.join(model_name.split('/')[1:])}\\n\")\n",
        "                                    col_file.write(f\"Question: {results_df.loc[idx, 'questions']}\\n\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(colored(f\"Unexpected error processing zero value at row {idx} of model {'_'.join(model_name.split('/')[1:])}: {e}\", 'red'))\n",
        "                                with open(f\"Handle_zero_values.txt\", \"a\") as col_file:\n",
        "                                    col_file.write(f\"Unexpected error processing zero value at row {idx} of model {'_'.join(model_name.split('/')[1:])}: {e}\\n\")\n",
        "                \n",
        "                except Exception as e:\n",
        "                    print(colored(f\"Error processing metric {metric} in resample {resample_idx} of model {'_'.join(model_name.split('/')[1:])}: {e}\", 'red'))\n",
        "                    with open(f\"Handle_zero_values.txt\", \"a\") as col_file:\n",
        "                        col_file.write(f\"Error processing metric {metric} in resample {resample_idx} of model {'_'.join(model_name.split('/')[1:])}: {e}\\n\")\n",
        "        \n",
        "        return zero_rows_columns # Return column names and rows with zero values\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(colored(f\"Critical error in handle_zero_values for model {'_'.join(model_name.split('/')[1:])}: {e}\", 'red'))\n",
        "        with open(f\"Handle_zero_values.txt\", \"a\") as col_file:\n",
        "            col_file.write(f\"Critical error in handle_zero_values for model {'_'.join(model_name.split('/')[1:])}: {e}\\n\")\n",
        "        traceback.print_exc()\n",
        "        return {}  # Return empty dict in case of critical error\n",
        "\n",
        "def process_zero_values(results_df, zero_rows_columns, list_of_metrics, model_name): #TO BE ACTIVATED - Need to be changed for second judge\n",
        "    \"\"\"Process and optionally replace zero values in results.\"\"\"\n",
        "    row_zero_counts = {}\n",
        "    col_zero_counts = {}\n",
        "\n",
        "    # Force a copy to ensure changes are applied properly below when replace with mean value\n",
        "    results_df_copy = results_df.copy()\n",
        "    model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "\n",
        "    for column_name, row_indices in zero_rows_columns.items():\n",
        "        for row_idx in row_indices:\n",
        "                \n",
        "            # Get values for this metric for this row and column (one resample per time)\n",
        "            values = results_df.loc[row_idx, column_name]\n",
        "\n",
        "            assert values==0, \"Values should be 0\"\n",
        "            \n",
        "            if values != 0: #We should never get here\n",
        "                with open('values_'+str(model_parameter)+'_'+str(column_name)+'_'+str(row_idx)+'.txt', 'w') as f:\n",
        "                    f.write(str(values))\n",
        "                \n",
        "            #Given that values are 0, replace with mean of non-zero values\n",
        "            df_values=results_df.loc[:, column_name].values\n",
        "            non_zero_values = [x for x in df_values if x != 0]\n",
        "\n",
        "            if len(non_zero_values) > 0:\n",
        "                mean_value = np.mean(non_zero_values)\n",
        "\n",
        "                if results_df.loc[row_idx, column_name] == 0 and mean_value != 0:\n",
        "                    print(colored(f\"0 value in row {row_idx}, column {column_name} should be replaced with mean {mean_value:.2f}\", 'yellow'))\n",
        "                    # Uncomment to actually replace values:\n",
        "                    # results_df.loc[row_idx, column_name] = mean_value#round(mean_value, 1)\n",
        "\n",
        "                    row_zero_counts[row_idx] = row_zero_counts.get(row_idx, 0) + 1\n",
        "                    col_zero_counts[column_name] = col_zero_counts.get(column_name, 0) + 1\n",
        "\n",
        "    print(\"\\nZero values replaced per row:\")\n",
        "    for row in sorted(row_zero_counts):\n",
        "        print(f\"Row/question {row}: {row_zero_counts[row]} replacements\")\n",
        "        with open(f\"process_zero_values_{model_parameter}.txt\", \"a\") as col_file:\n",
        "            col_file.write(f\"Row/question {row}: {row_zero_counts[row]} replacements \\n\")\n",
        "\n",
        "    print(\"\\nZero values replaced per column:\")\n",
        "    for col in sorted(col_zero_counts):\n",
        "        print(f\"Column/metric {col}: {col_zero_counts[col]} replacements\")\n",
        "        with open(f\"process_zero_values_{model_parameter}.txt\", \"a\") as col_file:\n",
        "            col_file.write(f\"Column/metric {col}: {col_zero_counts[col]} replacements \\n\")\n",
        "\n",
        "def reorganize_evaluation_metrics(all_resamples_metrics, list_of_metrics, model_name, list_of_questions, n_resamples, judge_model):\n",
        "    \"\"\"    \n",
        "    This function takes evaluation metrics from multiple resampling runs and reorganizes them into\n",
        "    a structured dictionary where each metric's scores are grouped together. It handles cases where\n",
        "    some evaluations may have failed (represented by 0s).\n",
        "    \n",
        "    Args:\n",
        "        all_resamples_metrics (list): List of evaluation results for each resample. Each resample contains\n",
        "                                     scores for multiple questions and metrics.\n",
        "        list_of_metrics (list): List of metric names to process (e.g., ['completeness_descr', 'relevance_descr']).\n",
        "        model_name (str): Name of the model being evaluated, used for logging.\n",
        "        list_of_questions (list): List of questions that were evaluated.\n",
        "        n_resamples (int): Number of resampling iterations performed.\n",
        "        judge_model (str): Name of the judge model being used.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary where keys are metric names (without '_descr' suffix) and values are lists\n",
        "              containing all scores for that metric across all resamples and questions.\n",
        "              \n",
        "    Note:\n",
        "        The function assumes each resample has scores for all questions and metrics.\n",
        "    \"\"\"\n",
        "    metric_scores_all_resamples = {metric.replace('_descr', ''): [] for metric in list_of_metrics}\n",
        "    #The above dict will have num_metrics elements, with their value for each question, over each run (first num_questions for first run, then next num_questions for next run, etc)\n",
        "    #Example: {'completeness': {'mean':[4, 3, 3, 5, 5, 4, 3, 4, 3, 3, 5, 5, 0, 3, 5, 3, 3, 5, 5, 4, 3]},  #assuming 3 runs and 7 questions\n",
        "    #          'relevance': {'mean':[4, 3, 3, 3, 4, 3, 2, 4, 3, 3, 3, 3, 0, 2, 4, 3, 3, 3, 3, 3, 2]}, ...}\n",
        "    #In case of error, there will be num_questions less elements in the sublist for which there was an error\n",
        "    \n",
        "    for metric_name in list_of_metrics:\n",
        "        clean_name = metric_name.replace('_descr', '')\n",
        "    \n",
        "        #Each resample_metrics (num_resamples in total) has a list of num_questions lists, each having num_metrics values\n",
        "        #format of each sublist: [EvaluationResult(key='completeness', score=4, value='To evaluate the ...\n",
        "        #If error, instead of the above list we have just a 0.\n",
        "        for resample_idx, resample_metrics in enumerate(all_resamples_metrics):\n",
        "\n",
        "            judge_name=\"_\".join(judge_model.split('/')[1:])\n",
        "            # #Information exists in all_resamples_metrics_main\n",
        "            # with open('resample_metrics_'+str(resample_idx)+'_'+str(metric_name)+'_'+str(model_name.split('/')[1])+\"_with_judge_\"+judge_name+'.txt', 'w') as f:\n",
        "            #     f.write(str(resample_metrics))\n",
        "\n",
        "            metric_idx = list_of_metrics.index(metric_name) #0-num_metrics the range of values of this. \n",
        "\n",
        "            scores = [m[metric_idx].score if m!=0 and m!=[] else 0 \n",
        "                     for m in resample_metrics] #num_questions elements each time\n",
        "            assert len(scores)==len(list_of_questions), \"Scores length not matching num_questions\"\n",
        "\n",
        "            metric_scores_all_resamples[clean_name].extend(scores) #Every time we add one metric for one resample (num_questions elements)\n",
        "\n",
        "    assert [len(x) for x in metric_scores_all_resamples.values()]==[len(list_of_questions)*n_resamples]*len(list_of_metrics), \"Metric stats length not matching\"\n",
        "\n",
        "    return metric_scores_all_resamples\n",
        "\n",
        "def reorganize_evaluation_metrics_second_judge(\n",
        "    excel_path, list_of_metrics, n_resamples, question_col=\"questions\", judge_model_2=judge_model):\n",
        "    \"\"\"\n",
        "    Loads the Excel file and reorganizes metrics for the second judge model.\n",
        "    Returns a dict: {metric: [all scores for that metric across all resamples/questions]}\n",
        "    \"\"\"\n",
        "\n",
        "    if judge_model_2!='openai/gpt-4o-mini':\n",
        "        df = pd.read_excel(excel_path)\n",
        "        metric_scores_all_resamples = {m.replace('_descr', ''): [] for m in list_of_metrics}\n",
        "        questions = df[question_col].tolist()\n",
        "        num_questions = len(questions)\n",
        "\n",
        "        for resample_idx in range(n_resamples):\n",
        "            for metric_name in list_of_metrics:\n",
        "                clean_name = metric_name.replace('_descr', '')\n",
        "                judge_name = \"_\".join(judge_model_2.split('/')[1:])\n",
        "                col = f\"metric_{clean_name}_{resample_idx+1}_{judge_name}\"\n",
        "                if col not in df.columns:\n",
        "                    with open(f\"warning_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                        col_file.write(f\"Warning excel: {col} not found in DataFrame \\n\")\n",
        "                    print(f\"Warning excel: {col} not found in DataFrame\")\n",
        "                    continue\n",
        "                scores = df[col].tolist()\n",
        "                initial_scores = scores\n",
        "                assert len(scores) == num_questions, f\"Scores length not matching num_questions for {col}\"\n",
        "                scores=[0 if value is None or (isinstance(value, float) and np.isnan(value)) else value for value in scores]\n",
        "                final_scores = scores\n",
        "                metric_scores_all_resamples[clean_name].extend(scores)\n",
        "\n",
        "                if final_scores!=initial_scores:\n",
        "                    with open(f\"warning_{'_'.join(judge_model_2.split('/')[1:])}.txt\", \"a\") as col_file:\n",
        "                        col_file.write(f\"Initial scores!!! for col {col} are {initial_scores} \\n\")\n",
        "                        col_file.write(f\"Final scores!!! for col {col} are {scores} \\n\")\n",
        "\n",
        "        return metric_scores_all_resamples\n",
        "\n",
        "def save_results(results_df, judge_model, model_id):\n",
        "    \"\"\"Save results DataFrame to Excel.\"\"\"\n",
        "\n",
        "    filename = (f\"results_{'_'.join(judge_model.split('/')[1:])}_judge_with_\"\n",
        "                    f\"{model_id.replace('/','_')}.xlsx\")\n",
        "    try:\n",
        "        #Extract reasoning traces and final answers from predicted answers\n",
        "        # Check for <think> tags in predicted answer columns and split them if found\n",
        "        for col in results_df.columns:\n",
        "            if 'predicted_answer' in col:\n",
        "                # Create new column names\n",
        "                run_number = col.split('_')[-1]\n",
        "                reasoning_col = f'reasoning_trace_{run_number}'\n",
        "                \n",
        "                # Check if we need to split this column\n",
        "                has_think_tags = results_df[col].astype(str).str.contains('</think>', na=False).any()\n",
        "                has_think_start_tags = results_df[col].astype(str).str.contains('<think>', na=False).any()\n",
        "                \n",
        "                if has_think_tags or has_think_start_tags: \n",
        "                    print(\"Has </think> or <think> in answer\")\n",
        "                    # Extract reasoning traces and final answers\n",
        "                    reasoning_traces = []\n",
        "                    final_answers = []\n",
        "                    \n",
        "                    for answer in results_df[col]:\n",
        "                        if isinstance(answer, str):\n",
        "                            if '<think>' in answer and '</think>' in answer:\n",
        "                                # Extract the reasoning trace between <think> and </think>\n",
        "                                think_start = answer.find('<think>') + len('<think>')\n",
        "                                think_end = answer.find('</think>')\n",
        "                                \n",
        "                                if think_start >= 0 and think_end >= 0:\n",
        "                                    reasoning = answer[think_start:think_end].strip()\n",
        "                                    print(\"Reasoning:\",reasoning)\n",
        "                                    final_answer = answer[think_end + len('</think>'):].strip()\n",
        "                                    print(\"Final answer:\",final_answer)\n",
        "                                    reasoning_traces.append(reasoning)\n",
        "                                    final_answers.append(final_answer)\n",
        "                            elif '</think>' in answer:\n",
        "                                print(\"Only </think> in answer\")\n",
        "                                # Handle case where only </think> is present\n",
        "                                think_end = answer.find('</think>')\n",
        "                                reasoning = answer[:think_end].strip()\n",
        "                                print(\"Reasoning:\",reasoning)\n",
        "                                final_answer = answer[think_end + len('</think>'):].strip()\n",
        "                                print(\"Final answer:\",final_answer)\n",
        "                                reasoning_traces.append(reasoning)\n",
        "                                final_answers.append(final_answer)\n",
        "                            elif '<think>' in answer:\n",
        "                                print(\"Only <think> in answer\")\n",
        "                                # Handle case where only <think> is present\n",
        "                                think_start = answer.find('<think>') + len('<think>')\n",
        "                                reasoning = answer[think_start:].strip()\n",
        "                                print(\"Reasoning:\",reasoning)\n",
        "                                final_answer = \"\"  # No final answer if only <think> tag is present\n",
        "                                print(\"Final answer: (empty)\")\n",
        "                                reasoning_traces.append(reasoning)\n",
        "                                final_answers.append(final_answer)\n",
        "                            else:\n",
        "                                reasoning_traces.append('')\n",
        "                                final_answers.append(answer)\n",
        "                    \n",
        "                    if any(trace.strip() for trace in reasoning_traces): #If there is any reasoning trace\n",
        "                        # Add the new columns - insert reasoning column right after the predicted answer column\n",
        "                        col_idx = results_df.columns.get_loc(col)\n",
        "                        results_df.insert(col_idx + 1, reasoning_col, reasoning_traces)\n",
        "                        results_df[col] = final_answers\n",
        "                        print(\"Added reasoning traces and final answers to dataframe\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error in saving trace results:\", e)\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    results_df.to_excel(filename, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Try to load already saved data (if some models have already been evaluated), otherwise initialize empty dicts\n",
        "all_models_stats, all_runs_model_metrics = load_model_stats(judge_model) \n",
        "all_models_stats_judge2, all_runs_model_metrics_judge2 = load_model_stats(judge_model_2)\n",
        "\n",
        "for model_id in models:\n",
        "    global model_name, model, tokenizer, pipeline, generate_max_tokens, vectorstore\n",
        "    model_name = model_id #Since model_name defined as global variable\n",
        "    model_parameter = \"_\".join(model_name.split('/')[1:])\n",
        "    model, tokenizer, pipeline = get_model(model_parameter)\n",
        "    \n",
        "    try: #Sometimes some errors with the evaluation\n",
        "        evaluation_all_resamples, dataset_langsmith, results_df, list_of_questions, vectorstore, reranker = perform_evaluation(model_id, judge_model, n_resamples, example_inputs, \n",
        "                                                                                                                               factor_evaluator, langsmith_api_key, use_RAG=use_RAG,\n",
        "                                                                                                                               use_smolagents=use_smolagents)\n",
        "        chunk_size = len(example_inputs) #Number of questions\n",
        "        \n",
        "        all_resamples_metrics = [] #Keep track of all metrics over all resamples and all questions\n",
        "        #There will be n_resamples lists, each with num_questions sublists (each having num_metrics sublists) (so num_questions*num_metrics elements in those in total)\n",
        "        #Each question will have 6 metric values like this: [EvaluationResult(key='completeness', score=4, value='To evaluate the ....\n",
        "        all_runs_metric_scores = [] #This will be appended to the input that plots metrics at the end. \n",
        "        #The format of it is [{metric1_descr_run1: [q1_score, q2_score, ...], metric2_descr_run1: [q1_score, q2_score, ...], ...}, \n",
        "        #                     {metric1_descr_run2: [q1_score, q2_score, ...], metric2_descr_run2: [q1_score, q2_score, ...], ...},\n",
        "        #                     ...num_runs]\n",
        "        \n",
        "        # Process each resample\n",
        "        for resample_idx in range(n_resamples):\n",
        "            start_idx = resample_idx * chunk_size #start index of current resample (chunk size is the number of questions of each resample)\n",
        "            #Resample_results saved in the process_metrics function\n",
        "            resample_results = evaluation_all_resamples[start_idx:start_idx + chunk_size] #Get results of a particular resample\n",
        "            assert len(resample_results)==chunk_size, f\"Number of resample results not matching num_questions. Got {len(resample_results)} resample results but expected {chunk_size}\"\n",
        "            predicted_answers = [x['run'].outputs['output'] for x in resample_results] #None if error\n",
        "            assert len(predicted_answers)==chunk_size, f\"Number of predicted answers not matching num_questions. Got {len(predicted_answers)} predicted answers but expected {chunk_size}\"\n",
        "\n",
        "            #We check below if there is any none ('') in predicted_answers and if so, we reorder the questions\n",
        "            with open('predicted_answers_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                f.write(str(predicted_answers))\n",
        "                f.write(\"\\n \\n\")\n",
        "                print(\"Total num of predicted answers:\",len(predicted_answers))\n",
        "                f.write(f\"Total num of predicted answers: {len(predicted_answers)}\")\n",
        "                f.write(\"............ \\n \\n\")\n",
        "\n",
        "            run_questions=[x['run'].inputs['inputs']['question'] for x in resample_results]\n",
        "            with open('run_questions_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                f.write(str(run_questions))\n",
        "                f.write(\"\\n \\n\")\n",
        "                print(\"Total num of run questions:\",len(run_questions))\n",
        "                f.write(f\"Total num of run questions: {len(run_questions)}\")\n",
        "                f.write(\"............ \\n \\n\")\n",
        "\n",
        "            # Get indices to reorder run_questions to match list_of_questions\n",
        "            reorder_indices = []\n",
        "            used_indices = set()\n",
        "\n",
        "            existing_questions = set(run_questions) - {'-'} - {'--'}\n",
        "            with open('existing_questions_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:  \n",
        "                f.write(str(existing_questions))\n",
        "                f.write(\"\\n \\n\")\n",
        "                print(\"Total num of existing questions:\",len(existing_questions))\n",
        "                f.write(f\"Total num of existing questions: {len(existing_questions)}\")\n",
        "                f.write(\"............ \\n \\n\")\n",
        "            missing_questions = [q for q in list_of_questions if q not in existing_questions]\n",
        "            with open('missing_questions_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                f.write(str(missing_questions))\n",
        "                f.write(\"\\n \\n\")\n",
        "                print(\"Total num of missing questions:\",len(missing_questions))\n",
        "                f.write(f\"Total num of missing questions: {len(missing_questions)}\")\n",
        "                f.write(\"............ \\n \\n\")\n",
        "            remaining_indices = [list_of_questions.index(val) for i, val in enumerate(missing_questions)]\n",
        "            with open('remaining_indices_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                f.write(str(remaining_indices))\n",
        "                f.write(\"\\n \\n\")\n",
        "                print(\"Total num of remaining indices:\",len(remaining_indices))\n",
        "                f.write(f\"Total num of remaining indices: {len(remaining_indices)}\")\n",
        "                f.write(\"............ \\n \\n\")\n",
        "\n",
        "            for q in run_questions:\n",
        "                if q in list_of_questions:\n",
        "                    idx = list_of_questions.index(q)\n",
        "                    reorder_indices.append(idx)\n",
        "                    used_indices.add(idx)\n",
        "                else:\n",
        "                    reorder_indices.append(None)\n",
        "                    with open('warning_run_questions_reordering_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                        f.write(f\"Warning: Question '{q}' not found in list_of_questions \\n\")\n",
        "\n",
        "            all_indices = set(range(len(list_of_questions)))\n",
        "            remaining_indices = list(all_indices - used_indices)\n",
        "            with open('remaining_indices_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                f.write(str(remaining_indices))\n",
        "                f.write(\"\\n \\n\")\n",
        "                print(\"Total num of remaining indices:\",len(remaining_indices))\n",
        "                f.write(f\"Total num of remaining indices: {len(remaining_indices)}\")\n",
        "                f.write(\"............ \\n \\n\")\n",
        "\n",
        "            # Replace None with remaining indices\n",
        "            ri_iter = iter(remaining_indices)\n",
        "            reorder_indices = [i if i is not None else next(ri_iter) for i in reorder_indices]\n",
        "            # Append any leftover indices not already used\n",
        "            reorder_indices += list(ri_iter)\n",
        "\n",
        "            if reorder_indices!=range(len(list_of_questions)):\n",
        "                with open('reorder_indices_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                    f.write(f\"Indices were reordered for model {model_parameter} and judge {judge_model} \\n\")\n",
        "                    f.write(str(reorder_indices))\n",
        "                    f.write(\"\\n \\n\")\n",
        "                    print(\"Total num of reorder indices:\",len(reorder_indices))\n",
        "                    f.write(f\"Total num of reorder indices: {len(reorder_indices)}\")\n",
        "                    f.write(\"............ \\n \\n\")\n",
        "\n",
        "            # If reorder_indices length doesn't match questions length, use sequential indices\n",
        "            if len(reorder_indices) != len(list_of_questions):\n",
        "                print(f\"Warning: Reorder indices length ({len(reorder_indices)}) doesn't match questions length ({len(list_of_questions)}). \\\n",
        "                      These indices are {reorder_indices}. Using sequential indices.\")\n",
        "                reorder_indices = list(range(len(list_of_questions)))\n",
        "            \n",
        "            # Reorder run_questions and predicted_answers using the indices\n",
        "            run_questions = [run_questions[i] for i in reorder_indices]\n",
        "            predicted_answers2 = [predicted_answers[i] for i in reorder_indices]\n",
        "            \n",
        "            if reorder_indices!=range(len(list_of_questions)):\n",
        "                # Save reordered questions and answers\n",
        "                with open('run_questions_reordered_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                    f.write(str(run_questions))\n",
        "                    f.write(\"\\n \\n\")\n",
        "                    print(\"Total num of reordered questions\",len(run_questions))\n",
        "                    f.write(f\"Total num of reordered questions {len(run_questions)}\")\n",
        "                    f.write(\"............ \\n \\n\")\n",
        "                with open('predicted_answers_reordered_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                    f.write(str(predicted_answers2))\n",
        "                    f.write(\"\\n \\n\")\n",
        "                    print(\"Total num of reordered answers\",len(predicted_answers))\n",
        "                    f.write(f\"Total num of reordered answers {len(predicted_answers2)}\")\n",
        "                    f.write(\"............ \\n \\n\")\n",
        "\n",
        "            # Verify the reordering worked\n",
        "            try:\n",
        "                assert len(run_questions) == len(list_of_questions), \"Questions reordering failed - orders don't match\"\n",
        "                predicted_answers = predicted_answers2\n",
        "            except AssertionError:\n",
        "                print(\"Questions reordering failed - using sequential indices\")\n",
        "                with open('warning_questions_reordering_'+str(resample_idx)+'_'+str(model_parameter)+'.txt', 'a') as f:\n",
        "                    f.write(f\"Questions reordering failed - using sequential indices. Original indices: {reorder_indices}\\n\")\n",
        "                reorder_indices = list(range(len(list_of_questions)))\n",
        "\n",
        "            #Add predicted answers to df\n",
        "            results_df[f'predicted_answer_{resample_idx+1}'] = predicted_answers\n",
        "\n",
        "            individual_run_metric_scores, metrics, results_df = process_metrics(\n",
        "                    resample_results, \n",
        "                    list_of_metrics, \n",
        "                    list_of_questions,\n",
        "                    resample_idx,\n",
        "                    results_df,\n",
        "                    model_name,\n",
        "                    reorder_indices\n",
        "                )           \n",
        "            \n",
        "            # Handle zero values\n",
        "            zero_rows_columns = handle_zero_values(results_df, n_resamples, list_of_metrics, model_name)\n",
        "            print(\"model id and judge\", model_id, judge_model)\n",
        "            print(\"scores from first judge\", individual_run_metric_scores)\n",
        "            print(\"metrics from first judge\", metrics)\n",
        "\n",
        "            with open(f\"individual_run_metric_scores_{model_name.split('/')[1]}.txt\", \"a\") as col_file: #Also saved in all_runs_metric_scores below\n",
        "                col_file.write(f\"Model id and judge: {model_id} and {judge_model} \\n\")\n",
        "                col_file.write(f\"Scores from first judge {individual_run_metric_scores} \\n\")\n",
        "                col_file.write(f\"Metrics from first judge {metrics} \\n\")\n",
        "\n",
        "            if zero_rows_columns: #Only keeps tracks of missing values if there are any - NOT ACTIVATED YET\n",
        "                unique_zero_rows_columns = len(set([x for sublist in list(zero_rows_columns.values()) for x in sublist]))\n",
        "                print(colored(f\"ERROR: Found missing values in {unique_zero_rows_columns} rows out of {len(results_df)}\", 'red'))\n",
        "                with open(f\"missing_values_log_{model_parameter}.txt\", \"a\") as col_file:\n",
        "                    col_file.write(f\"ERROR: Found missing values in {unique_zero_rows_columns} rows out of {len(results_df)}. These are the rows: {zero_rows_columns}, \\\n",
        "                                   where the values of dict are the indices of the rows with missing values. Model is {model_name} and judge is {judge_model}\\n\")\n",
        "                process_zero_values(results_df, zero_rows_columns, list_of_metrics, model_name) #Replace 0s with mean of non-zero values    \n",
        "            \n",
        "            #In each iteration we append the metrics (6 in total) of one resample for all questions - n at the end, one for each resample\n",
        "            #If there is an error, the metrics will be 0 (there will be n_errors*num_metrics less 'EvaluationResult' objects in that case)\n",
        "            all_resamples_metrics.append(metrics)\n",
        "\n",
        "            #Has n_resamples lists, each with num_metrics sublists (each sublist has scores over all questions of one metric) \n",
        "            all_runs_metric_scores.append(individual_run_metric_scores)\n",
        "        \n",
        "        assert len(all_runs_metric_scores)==n_resamples, f\"Number of all_runs_metric_scores not matching num_resamples. \\\n",
        "            Got {len(all_runs_metric_scores)} all_runs_metric_scores but expected {n_resamples}\"\n",
        "        \n",
        "        for i in range(n_resamples):\n",
        "            assert len(all_runs_metric_scores[i])==len(list_of_metrics), f\"Number of all_runs_metric_scores[{i}] not matching num_metrics. \\\n",
        "                Got {len(all_runs_metric_scores[i])} all_runs_metric_scores[{i}] but expected {len(list_of_metrics)}\"\n",
        "\n",
        "        # #A list with num resamples dicts, each having num metrics keys. Each key has num_questions values. - Information already in all_runs_metric_scores\n",
        "        # # example: [{'completeness_descr': [5, 5, 5, 3, 1], .....'general_descr': [5, 4, 5, 0, 2]}, {'completeness_descr': [5, 5, 5, 4, 1], .....}]\n",
        "        # with open('all_runs_metric_scores_main_'+str(model_parameter)+'.txt', 'w') as f:\n",
        "        #     f.write(str(all_runs_metric_scores))\n",
        "\n",
        "        #A list with num_resamples sublists, each sublist having num_questions sublists. For each of those num_questions sublists,\n",
        "        #  each sub-sublist having num_metrics elements, each like the following:\n",
        "        #[EvaluationResult(key='completeness', score=3, value=\"To evaluate the completeness...\n",
        "        #for example, for 2 resamples, there would be 2 sublists, each with num_questions sublists. For each of those num_questions sublists, ...\n",
        "        with open('all_resamples_metrics_main_'+str(model_parameter)+\"_\"+str(\"_\".join(judge_model.split('/')[1:]))+'.txt', 'w') as f:\n",
        "            f.write(str(all_resamples_metrics))\n",
        "\n",
        "        assert len(all_resamples_metrics)==n_resamples, f\"Number of all_resamples_metrics not matching num_resamples. \\\n",
        "            Got {len(all_resamples_metrics)} all_resamples_metrics but expected {n_resamples}\"\n",
        "        \n",
        "        for i in range(n_resamples): #Each one will have num_questions elements, each with num_metrics sublists (or 0 if error)\n",
        "            assert len(all_resamples_metrics[i])==len(list_of_questions), f\"Number of all_resamples_metrics[{i}] not matching num_questions. \\\n",
        "                Got {len(all_resamples_metrics[i])} all_resamples_metrics[{i}] but expected {len(list_of_questions)}\" #Each all_ressamples_metrics[i] should have num_questions elements\n",
        "\n",
        "        # Calculate statistics\n",
        "        metric_stats_resampling = calculate_metric_statistics(\n",
        "            all_runs_metric_scores, \n",
        "            list_of_metrics, \n",
        "            len(list_of_questions),\n",
        "            model_name,\n",
        "            judge_model\n",
        "        )\n",
        "        \n",
        "        # Save initial results\n",
        "        save_results(results_df, judge_model, model_id)\n",
        "\n",
        "\n",
        "    #Second judge - Order of indices should be the same as for main judge\n",
        "        if judge_model_2:\n",
        "            judge_name = \"_\".join(judge_model_2.split('/')[1:])\n",
        "            with open(f\"non_existing_cols_{judge_name}.txt\", \"a\") as f:\n",
        "                f.write(f\"Model used: {model_id}\\n \\n\")\n",
        "\n",
        "            filename_excel = (f\"results_{'_'.join(judge_model.split('/')[1:])}_judge_with_\"\n",
        "                    f\"{model_id.replace('/','_')}.xlsx\") \n",
        "            \n",
        "            apply_second_judge(\n",
        "                input_excel=filename_excel,\n",
        "                list_of_metrics=list_of_metrics,  # e.g. ['completeness_descr', ...]\n",
        "                num_resamples=n_resamples,  \n",
        "                model_name=model_name,\n",
        "                judge_model_2=judge_model_2 if judge_model_2 else judge_model,\n",
        "            )\n",
        "\n",
        "            excel_path=(f\"results_{'_'.join(judge_model_2.split('/')[1:])}_judge_with_\"\n",
        "                    f\"{model_id.replace('/','_')}.xlsx\")\n",
        "\n",
        "            #A list with num of judges dicts, each with num of metrics keys and num of questions scores/prompts\n",
        "            all_runs_metric_scores_second_judge, all_run_metric_prompts_second_judge = process_metrics_second_judge( #all_run_metric_prompts_second_judge is not used\n",
        "                excel_path, list_of_metrics, n_resamples, model_name=model_name, judge_model_2=judge_model_2 if judge_model_2 else judge_model)\n",
        "            \n",
        "            with open('all_runs_metric_scores_second_judge_{judge_name}.txt', 'w') as f:\n",
        "                f.write(str(all_runs_metric_scores_second_judge))\n",
        "                f.write(\"\\n \\n\")\n",
        "                initial_all_runs_metric_scores_second_judge = all_runs_metric_scores_second_judge\n",
        "\n",
        "            len_questions=len(results_df)\n",
        "            for resample in range(n_resamples):\n",
        "                for metric in all_runs_metric_scores_second_judge[resample]:\n",
        "                        all_runs_metric_scores_second_judge[resample][metric] = [0 if value is None or (isinstance(value, float) and np.isnan(value)) else value for value in all_runs_metric_scores_second_judge[resample][metric]]\n",
        "\n",
        "            if initial_all_runs_metric_scores_second_judge != all_runs_metric_scores_second_judge:\n",
        "                with open(f\"all_runs_scores_second_judge_changed_{model_parameter}.txt\", \"a\") as log_file:\n",
        "                    log_file.write(f\"changed_metrics:\\n{all_runs_metric_scores_second_judge}\\n\\n\")\n",
        "\n",
        "            calculate_metric_statistics(\n",
        "                        all_runs_metric_scores_second_judge, \n",
        "                        list_of_metrics, \n",
        "                        len(list_of_questions),\n",
        "                        model_name,\n",
        "                        judge_name\n",
        "                    )\n",
        "\n",
        "            with open(f\"warning_{judge_name}.txt\", \"a\") as col_file: #For main questions this just notes model!\n",
        "                col_file.write(f\"\\n \\n Model used: {model_id} \\n\")\n",
        "            \n",
        "            # Get reorganized metrics\n",
        "            metric_scores_all_resamples_second_judge = reorganize_evaluation_metrics_second_judge(\n",
        "                excel_path, list_of_metrics, n_resamples, judge_model_2=judge_model_2 if judge_model_2 else judge_model\n",
        "            )\n",
        "            \n",
        "            #A dict with num_metrics keys, each with num_questions*num_resamples values (as a list - first num_questions values are for first resample, \n",
        "            # second num_questions values are for second resample, etc.)\n",
        "            with open('metric_scores_all_resamples_'+str(model_parameter)+'_judge_'+str(judge_name)+'.txt', 'w') as f:\n",
        "                f.write(str(metric_scores_all_resamples_second_judge))\n",
        "\n",
        "            assert len(metric_scores_all_resamples_second_judge)==len(list_of_metrics), f\"Number of metric_scores_all_resamples_second_judge not matching num_metrics. \\\n",
        "                Got {len(metric_scores_all_resamples_second_judge)} metric_scores_all_resamples_second_judge but expected {len(list_of_metrics)}\"\n",
        "            \n",
        "            for i in range(len(list_of_metrics)):\n",
        "                name_of_metric=list_of_metrics[i].replace('_descr','')\n",
        "                assert len(metric_scores_all_resamples_second_judge[name_of_metric])==len(list_of_questions)*n_resamples, f\"Number of metric_scores_all_resamples_second_judge[{name_of_metric}] not matching \\\n",
        "                    num_questions*num_resamples. Got {len(metric_scores_all_resamples_second_judge[name_of_metric])} metric_scores_all_resamples_second_judge[{name_of_metric}] but \\\n",
        "                    expected {len(list_of_questions)*n_resamples}\"\n",
        "\n",
        "            metric_names_second_judge = list(metric_scores_all_resamples_second_judge.keys()) #Final list of metrics for plotting\n",
        "            # Verify metric names\n",
        "            metrics_names_loop_second_judge = [metric.replace('_descr','') for metric in list_of_metrics]\n",
        "            assert metrics_names_loop_second_judge == metric_names_second_judge, \"Metric names mismatch\"\n",
        "            \n",
        "            # Save results\n",
        "            all_runs_model_metrics_judge2[model_id] = all_runs_metric_scores_second_judge #Used in plotting metrics\n",
        "            #Dictionary in format {model_id:[{metric_1_run_1:[values], metric_2_run_1:[values], ...}, {metric_1_run_2:[values]....}]\n",
        "\n",
        "            all_models_stats_judge2[model_id] = plot_figures_metrics(\n",
        "                all_runs_model_metrics_judge2,\n",
        "                metric_names_second_judge,\n",
        "                model_id,\n",
        "                judge_model_2\n",
        "            ) #Stats like mean, std, etc. per metric and per run over all questions\n",
        "            \n",
        "            # Save to files\n",
        "            with open(f'stats_{judge_name}.json', 'w') as f:\n",
        "                json.dump(all_models_stats_judge2, f, indent=4)\n",
        "            with open(f'all_runs_model_metrics_{judge_name}.json', 'w') as f:\n",
        "                json.dump(all_runs_model_metrics_judge2, f, indent=4)\n",
        "\n",
        "            print(\"Model\",model_id,\"saved with judge\", judge_model_2)\n",
        "            print(\"Models saved so far:\",list(all_models_stats_judge2.keys()))\n",
        "\n",
        "\n",
        "        #Continue with main judge below\n",
        "        # Reorganize metrics - Has num_metrics keys, each with num_questions*num_resamples values (as a list)\n",
        "        metric_scores_all_resamples = reorganize_evaluation_metrics(all_resamples_metrics, list_of_metrics, model_name, list_of_questions, n_resamples, judge_model)\n",
        "\n",
        "        #A dict with num_metrics keys, each with num_questions*num_resamples values (as a list - first num_questions values are for first resample, \n",
        "        # second num_questions values are for second resample, etc.)\n",
        "        judge_name_main = \"_\".join(judge_model.split('/')[1:])\n",
        "        with open('metric_scores_all_resamples_'+str(model_parameter)+\"_judge_\"+str(judge_name_main)+'.txt', 'w') as f:\n",
        "            f.write(str(metric_scores_all_resamples))\n",
        "\n",
        "        assert len(metric_scores_all_resamples)==len(list_of_metrics), f\"Number of metric_scores_all_resamples not matching num_metrics. \\\n",
        "            Got {len(metric_scores_all_resamples)} metric_scores_all_resamples but expected {len(list_of_metrics)}\"\n",
        "        \n",
        "        for i in range(len(list_of_metrics)):\n",
        "            name_of_metric=list_of_metrics[i].replace('_descr','')\n",
        "            assert len(metric_scores_all_resamples[name_of_metric])==len(list_of_questions)*n_resamples, f\"Number of metric_scores_all_resamples[{name_of_metric}] not matching \\\n",
        "                num_questions*num_resamples. Got {len(metric_scores_all_resamples[name_of_metric])} metric_scores_all_resamples[{name_of_metric}] but \\\n",
        "                expected {len(list_of_questions)*n_resamples}\"\n",
        "\n",
        "        metric_names = list(metric_scores_all_resamples.keys()) #Final list of metrics for plotting\n",
        "        \n",
        "        # Verify metric names\n",
        "        metrics_names_loop = [metric.replace('_descr','') for metric in list_of_metrics]\n",
        "        assert metrics_names_loop == metric_names, \"Metric names mismatch\"\n",
        "        \n",
        "        # Save results\n",
        "        all_runs_model_metrics[model_id] = all_runs_metric_scores #Used in plotting metrics\n",
        "        #Dictionary in format {model_id:[{metric_1_run_1:[values], metric_2_run_1:[values], ...}, {metric_1_run_2:[values]....}]\n",
        "\n",
        "        all_models_stats[model_id] = plot_figures_metrics(\n",
        "            all_runs_model_metrics,\n",
        "            metric_names,\n",
        "            model_id,\n",
        "            judge_model\n",
        "        ) #Stats like mean, std, etc. per metric and per run over all questions\n",
        "        \n",
        "        # Save to files\n",
        "        judge_name = \"_\".join(judge_model.split('/')[1:])\n",
        "        with open(f'stats_{judge_name}.json', 'w') as f:\n",
        "            json.dump(all_models_stats, f, indent=4)\n",
        "        with open(f'all_runs_model_metrics_{judge_name}.json', 'w') as f:\n",
        "            json.dump(all_runs_model_metrics, f, indent=4)\n",
        "\n",
        "        print(\"Model\",model_id,\"saved\")\n",
        "        print(\"Models saved so far:\",list(all_models_stats.keys()))\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(\"An error occurred in evaluating model\",model_id)\n",
        "        print(\"Error Details:\", e)\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    finally:\n",
        "        # Clear VRAM\n",
        "        del model, tokenizer, pipeline\n",
        "        torch.cuda.empty_cache()\n",
        "        print('-'*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparison plots between LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_metrics_by_model(all_runs_model_metrics):\n",
        "    \"\"\"\n",
        "    Calculate mean and std of each metric over all runs for each model\n",
        "    \n",
        "    Args:\n",
        "        all_runs_model_metrics (dict): Dictionary containing metrics for all model runs\n",
        "        \n",
        "    Returns:\n",
        "        dict: Aggregated metrics by model\n",
        "    \"\"\"\n",
        "    aggregated_metrics_by_model = {}\n",
        "\n",
        "    for model, model_data in all_runs_model_metrics.items():\n",
        "        if model not in aggregated_metrics_by_model:\n",
        "            aggregated_metrics_by_model[model] = {}\n",
        "        \n",
        "        for run_data in model_data:\n",
        "            for metric_name, metric_values in run_data.items():\n",
        "                if metric_name not in aggregated_metrics_by_model[model]:\n",
        "                    aggregated_metrics_by_model[model][metric_name] = []\n",
        "                \n",
        "                if isinstance(metric_values, list) and all(isinstance(x, (int, float)) for x in metric_values):\n",
        "                    aggregated_metrics_by_model[model][metric_name].extend(metric_values)\n",
        "                else:\n",
        "                    print(metric_values)\n",
        "                    with open(f\"aggregated_metrics_by_model_{model}.txt\", \"a\") as f:\n",
        "                        f.write(f\"metric_values: {metric_values}\\n\")\n",
        "                    \n",
        "    return aggregated_metrics_by_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aggregated_metrics=aggregate_metrics_by_model(all_runs_model_metrics)\n",
        "aggregated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aggregated_metrics_by_model_second_judge=aggregate_metrics_by_model(all_runs_model_metrics_judge2)\n",
        "aggregated_metrics_by_model_second_judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_aggregated_metrics(aggregated_metrics, judge_model):\n",
        "    for model, metrics in aggregated_metrics.items():\n",
        "        print(f\"\\nModel: {model}\")\n",
        "        print(\"-\" * (len(model) + 8))\n",
        "        \n",
        "        # Create file first\n",
        "        model_name=\"_\".join(model.split(\"/\")[1:])\n",
        "        judge_name = \"_\".join(judge_model.split(\"/\")[1:])\n",
        "        with open(f\"aggregated_metrics_{model_name}_{judge_name}.txt\", \"w\") as f:\n",
        "            pass\n",
        "            \n",
        "        with open(f\"aggregated_metrics_{model_name}_{judge_name}.txt\", \"a\") as f:\n",
        "            f.write(f\"Model: {model_name}\\n\")\n",
        "            f.write(\"-\" * (len(model_name) + 8) + \"\\n\")\n",
        "        \n",
        "        for metric_name, values in metrics.items():\n",
        "            if values:\n",
        "                mean_value = np.mean(values)\n",
        "                std_value = np.std(values)\n",
        "                print(f\"{metric_name}:\")\n",
        "                print(f\"  Mean: {mean_value:.4f}\")\n",
        "                print(f\"  Std:  {std_value:.4f}\")\n",
        "                with open(f\"aggregated_metrics_{model_name}_{judge_name}.txt\", \"a\") as f:\n",
        "                    f.write(f\"{metric_name}: {mean_value:.4f} {std_value:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_aggregated_metrics(aggregated_metrics, judge_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_aggregated_metrics(aggregated_metrics_by_model_second_judge, judge_model_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_of_metric_names=[name.removesuffix('_descr') for name in list_of_metrics]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_model_metrics(list_of_metric_names, aggregated_metrics):\n",
        "    metric_means = {m: [] for m in list_of_metric_names}\n",
        "    metric_stds = {m: [] for m in list_of_metric_names}\n",
        "    model_names = []\n",
        "\n",
        "    for model, model_metrics in aggregated_metrics.items():\n",
        "        model_names.append(model.split('/')[-1].replace('-descr', '').replace('_descr', ''))\n",
        "        for m in list_of_metric_names:\n",
        "            key = f\"{m}_descr\"\n",
        "            if key in model_metrics and model_metrics[key]:\n",
        "                values = model_metrics[key]\n",
        "                metric_means[m].append(np.mean(values))\n",
        "                metric_stds[m].append(np.std(values))\n",
        "            else:\n",
        "                metric_means[m].append(0.0)\n",
        "                metric_stds[m].append(0.0)\n",
        "    return model_names, metric_means, metric_stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_names, metric_means, metric_stds=calculate_model_metrics(list_of_metric_names, aggregated_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_names_second_judge, metric_means_second_judge, metric_stds_second_judge=calculate_model_metrics(list_of_metric_names, aggregated_metrics_by_model_second_judge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model_comparison(model_names, list_of_metric_names, metric_means, metric_stds, save_prefix=\"_\".join(judge_model.split('/')[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model_comparison(model_names_second_judge, list_of_metric_names, metric_means_second_judge, metric_stds_second_judge, save_prefix=\"_\".join(judge_model_2.split('/')[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_spider_chart(model_names, list_of_metric_names, metric_means, save_prefix=\"_\".join(judge_model.split('/')[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_spider_chart(model_names_second_judge, list_of_metric_names, metric_means_second_judge, save_prefix=\"_\".join(judge_model_2.split('/')[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistical comparison between models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_model_performances(all_runs_model_metrics, judge_model): \n",
        "    \"\"\"\n",
        "    Performs statistical comparison between models using paired differences, standard errors,\n",
        "    and Pearson correlation coefficients following section 4.2 methodology.\n",
        "    \n",
        "    Args:\n",
        "        all_runs_model_metrics (dict): Dictionary containing raw metrics for each model/run/question\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary containing pairwise comparison results\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy import stats\n",
        "    import itertools\n",
        "    \n",
        "    # Get all model pairs for comparison\n",
        "    models = list(all_runs_model_metrics.keys())\n",
        "    model_pairs = list(itertools.combinations(models, 2))\n",
        "    \n",
        "    # Store results\n",
        "    comparison_results = {}\n",
        "    \n",
        "    for model1, model2 in model_pairs:\n",
        "        judge_name = \"_\".join(judge_model.split('/')[1:])\n",
        "        comparison_key = f\"{model1.split('/')[-1]}_vs_{model2.split('/')[-1]}_with_{judge_name}\"\n",
        "        comparison_results[comparison_key] = {}\n",
        "        \n",
        "        # Get metrics (removing '_descr' suffix)\n",
        "        metrics = [metric.replace('_descr', '') for metric in list(all_runs_model_metrics[model1][0].keys())]\n",
        "        \n",
        "        # Create file for this model comparison\n",
        "        variance_results_text = f\"\\n=== Variance Analysis Results for {comparison_key} ===\\n\"\n",
        "        \n",
        "        for metric in metrics:\n",
        "            # Calculate differences and correlations for each resample\n",
        "            resample_differences = []\n",
        "            resample_ses = []\n",
        "            correlations = []\n",
        "            model1_variances = []  # Initialize list\n",
        "            model2_variances = []  # Initialize list\n",
        "            \n",
        "            # Iterate through resamples - Same number for both models\n",
        "            for resample_idx in range(len(all_runs_model_metrics[model1])):\n",
        "                # Get scores for both models for this resample\n",
        "                scores1 = all_runs_model_metrics[model1][resample_idx][f'{metric}_descr']\n",
        "                scores2 = all_runs_model_metrics[model2][resample_idx][f'{metric}_descr']\n",
        "                \n",
        "                # Calculate differences for each question\n",
        "                question_differences = np.array(scores1) - np.array(scores2)\n",
        "                \n",
        "                # Calculate mean difference for this resample\n",
        "                mean_diff = np.mean(question_differences) #Same as the formula in the paper since mean(a-b)=mean(a)-mean(b)\n",
        "                \n",
        "                # Calculate standard error for this resample - Paired analysis (section 4.2)\n",
        "                n = len(question_differences)\n",
        "                se = np.sqrt(np.sum((question_differences - mean_diff)**2) / (n * (n-1))) if n > 1 else np.nan\n",
        "\n",
        "                # # Calculate standard errors for each model - Unpaired analysis (section 4.1)\n",
        "                # n = len(scores1)\n",
        "                # sea = np.sqrt(np.sum((scores1 - np.mean(scores1))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "                # seb = np.sqrt(np.sum((scores2 - np.mean(scores2))**2) / (n * (n - 1))) if n > 1 else np.nan\n",
        "\n",
        "                # # Calculate the combined standard error as sqrt(sea^2 + seb^2)\n",
        "                # se = np.sqrt(sea**2 + seb**2)\n",
        "\n",
        "                # Calculate variances for each model\n",
        "                var1 = np.var(scores1, ddof=1)  # Using ddof=1 for sample variance\n",
        "                var2 = np.var(scores2, ddof=1)\n",
        "                model1_variances.append(var1)\n",
        "                model2_variances.append(var2)\n",
        "                \n",
        "                # Calculate Pearson correlation\n",
        "                correlation, _ = stats.pearsonr(scores1, scores2)\n",
        "                \n",
        "                resample_differences.append(mean_diff)\n",
        "                resample_ses.append(se)\n",
        "                correlations.append(correlation)\n",
        "            \n",
        "            # Convert to numpy arrays\n",
        "            resample_differences = np.array(resample_differences)\n",
        "            resample_ses = np.array(resample_ses)\n",
        "            correlations = np.array(correlations)\n",
        "            model1_variances = np.array(model1_variances)\n",
        "            model2_variances = np.array(model2_variances)\n",
        "            print(\"resample_differences\",resample_differences)\n",
        "            print(\"resample_ses\",resample_ses)\n",
        "            print(\"correlations\",correlations)\n",
        "            print(f\"Model 1 variances: {model1_variances}\")\n",
        "            print(f\"Model 2 variances: {model2_variances}\")\n",
        "            with open(f\"model_variances_{comparison_key}.txt\", \"a\") as f:\n",
        "                f.write(f\"Model 1 variances: {model1_variances}\\n\")\n",
        "                f.write(f\"Model 2 variances: {model2_variances}\\n\")\n",
        "                f.write(f\"resample_differences: {resample_differences}\\n\")\n",
        "                f.write(f\"resample_ses: {resample_ses}\\n\")\n",
        "                f.write(f\"correlations: {correlations}\\n\")\n",
        "          \n",
        "            # Calculate overall mean difference over all resamples\n",
        "            overall_mean_diff = np.mean(resample_differences)\n",
        "            print(\"overall_mean_diff\",overall_mean_diff)\n",
        "            with open(f\"model_variances_{comparison_key}.txt\", \"a\") as f:\n",
        "                f.write(f\"overall_mean_diff: {overall_mean_diff}\\n\")\n",
        "            \n",
        "            #We want an aggregated SE across all resamples for the same questions (same paired differences)\n",
        "            #This approach accounts for the fact that each resampling provides a different estimate of the variance of the same underlying distribution, \n",
        "            # and averaging these estimates gives a better representation of the overall uncertainty.\n",
        "\n",
        "            # Calculate pooled standard error across resamples\n",
        "            R = len(resample_differences)\n",
        "            pooled_se = np.sqrt(np.sum(resample_ses**2) / (R**2))\n",
        "            print(\"pooled_se\",pooled_se)\n",
        "            with open(f\"model_variances_{comparison_key}.txt\", \"a\") as f:\n",
        "                f.write(f\"pooled_se: {pooled_se}\\n\")\n",
        "            \n",
        "            # # If the resampling results are independent estimates of variance (i.e., combining uncertainty estimates from independent sources), the combined variance is\n",
        "            # # the sum of all individual variances, and the combined standard error is given below (goal to capture total variability)\n",
        "            # # Calculate the overall combined SE across all resamples\n",
        "            # combined_se = np.sqrt(np.nansum(np.array(resample_ses)**2))\n",
        "\n",
        "            # Calculate overall variance reduction across all resamples\n",
        "            n = len(scores1)\n",
        "            \n",
        "            # Calculate mean variances across resamples\n",
        "            mean_var1 = np.mean(model1_variances)  # Var(sA)\n",
        "            mean_var2 = np.mean(model2_variances)  # Var(sB)\n",
        "            \n",
        "            # Calculate mean correlation across resamples\n",
        "            mean_correlation = np.mean(correlations)\n",
        "            \n",
        "            # Calculate covariance between model scores\n",
        "            mean_cov = mean_correlation * np.sqrt(mean_var1 * mean_var2)  # Cov(sA, sB)\n",
        "            \n",
        "            # Calculate variance for unpaired case: Var(μA-B,unpaired) = (Var(sA) + Var(sB))/n\n",
        "            var_unpaired = (mean_var1 + mean_var2) / n\n",
        "            \n",
        "            # Calculate variance for paired case: Var(μA-B,paired) = (Var(sA) + Var(sB) - 2Cov(sA,sB))/n\n",
        "            var_paired = (mean_var1 + mean_var2 - 2 * mean_cov) / n\n",
        "            \n",
        "            # The reduction in variance is: Var(μA-B,unpaired) - Var(μA-B,paired) = 2Cov(xA,xB)/n\n",
        "            variance_reduction = 2 * mean_cov / n  # This should equal var_unpaired - var_paired\n",
        "            \n",
        "            # Calculate percentage reduction in variance\n",
        "            percent_reduction = (variance_reduction / var_unpaired) * 100 if var_unpaired != 0 else 0\n",
        "\n",
        "            # Add results for this metric to the text\n",
        "            variance_results_text += f\"\\nMetric: {metric}\\n\"\n",
        "            variance_results_text += f\"Mean Model 1 variance (Var(sA)): {mean_var1:.6f}\\n\"\n",
        "            variance_results_text += f\"Mean Model 2 variance (Var(sB)): {mean_var2:.6f}\\n\"\n",
        "            variance_results_text += f\"Mean covariance (Cov(sA,sB)): {mean_cov:.6f}\\n\"\n",
        "            variance_results_text += f\"Unpaired variance: {var_unpaired:.6f}\\n\"\n",
        "            variance_results_text += f\"Paired variance: {var_paired:.6f}\\n\"\n",
        "            variance_results_text += f\"Variance reduction (2Cov(xA,xB)/n): {variance_reduction:.6f}\\n\"\n",
        "            variance_results_text += f\"Percent reduction: {percent_reduction:.1f}%\\n\"\n",
        "\n",
        "            # # Calculate t-statistic and p-value\n",
        "            # t_stat = overall_mean_diff / pooled_se if pooled_se != 0 else np.nan\n",
        "            # df = R - 1  # degrees of freedom\n",
        "            # p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if not np.isnan(t_stat) else np.nan\n",
        "            \n",
        "            # # Calculate confidence interval\n",
        "            # t_crit = stats.t.ppf(0.975, df)  # 95% CI\n",
        "            # ci_margin = t_crit * pooled_se\n",
        "\n",
        "            # Calculate z-statistic and CI using standard normal distribution\n",
        "            z_stat = overall_mean_diff / pooled_se if pooled_se != 0 else np.nan\n",
        "            \n",
        "            # Calculate confidence interval using 1.96 for 95% CI\n",
        "            ci_margin = 1.96 * pooled_se\n",
        "            \n",
        "            # Calculate p-value using standard normal distribution\n",
        "            #For a two-tailed test p = 2 × (1 − Φ(|z|)), where Φ(z) is the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat))) if not np.isnan(z_stat) else np.nan\n",
        "            \n",
        "            # # Calculate average Pearson correlation - not accurate when correlations close to 1 or -1, variances differences across resamples, sample size is small.\n",
        "            # avg_correlation = np.mean(correlations)\n",
        "\n",
        "            #Apply Fisher z-transformation\n",
        "            z_values = [0.5 * np.log((1 + r) / (1 - r)) for r in correlations]\n",
        "\n",
        "            # Compute the mean Fisher z-value\n",
        "            z_mean = np.mean(z_values)\n",
        "\n",
        "            #Back-transform to Pearson correlation scale\n",
        "            overall_correlation = (np.exp(2 * z_mean) - 1) / (np.exp(2 * z_mean) + 1)\n",
        "            \n",
        "            # Store results\n",
        "            comparison_results[comparison_key][metric] = {\n",
        "                \"mean_difference\": overall_mean_diff,\n",
        "                \"pooled_standard_error\": pooled_se,\n",
        "                \"ci_low\": overall_mean_diff - ci_margin,\n",
        "                \"ci_high\": overall_mean_diff + ci_margin,\n",
        "                # \"t_statistic\": t_stat,\n",
        "                \"z_statistic\": z_stat,\n",
        "                \"p_value\": p_value,\n",
        "                \"significant\": p_value < 0.05 if not np.isnan(p_value) else None,\n",
        "                \"better_model\": model1.split('/')[-1] if overall_mean_diff > 0 else model2.split('/')[-1],\n",
        "                \"pearson_correlation\": overall_correlation\n",
        "            }\n",
        "        \n",
        "        # Write all metrics results for this model comparison to a single file\n",
        "        with open(f'variance_results_{comparison_key}.txt', 'w') as f:\n",
        "            variance_results_text += f\"Overall Variance Reduction Analysis:\\n\"\n",
        "            f.write(variance_results_text)\n",
        "    \n",
        "    return comparison_results\n",
        "\n",
        "comparison_results = compare_model_performances(all_runs_model_metrics, judge_model) \n",
        "\n",
        "# Save results to file\n",
        "with open('comparison_result_'+\"_\".join(judge_model.split('/')[1:])+\".json\", 'w') as f:\n",
        "    # Convert numpy types to native Python types for JSON serialization\n",
        "    def convert_to_serializable(obj):\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
        "            np.int16, np.int32, np.int64, np.uint8,\n",
        "            np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.bool_)):\n",
        "            return bool(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        elif obj is None:\n",
        "            return None\n",
        "        return obj\n",
        "    \n",
        "    serializable_results = json.loads(\n",
        "        json.dumps(comparison_results, default=convert_to_serializable)\n",
        "    )\n",
        "    json.dump(serializable_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_results_second_judge = compare_model_performances(all_runs_model_metrics_judge2, judge_model_2)\n",
        "\n",
        "# Save results to file\n",
        "with open('comparison_result_'+'_'.join(judge_model_2.split('/')[1:])+'.json', 'w') as f:\n",
        "    # Convert numpy types to native Python types for JSON serialization\n",
        "    def convert_to_serializable(obj):\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
        "            np.int16, np.int32, np.int64, np.uint8,\n",
        "            np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.bool_)):\n",
        "            return bool(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        elif obj is None:\n",
        "            return None\n",
        "        return obj\n",
        "    \n",
        "    serializable_results_second_judge = json.loads(\n",
        "        json.dumps(comparison_results_second_judge, default=convert_to_serializable)\n",
        "    )\n",
        "    json.dump(serializable_results_second_judge, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_and_save_model_comparisons(comparison_results, list_of_metrics, suffix):\n",
        "    # Extract metrics and models from comparison_results\n",
        "    metrics = [metric.replace('_descr', '') for metric in list_of_metrics]\n",
        "    model_pairs = list(comparison_results.keys())\n",
        "\n",
        "    # Create figure with subplots for each metric\n",
        "    # Calculate number of rows needed based on number of metrics\n",
        "    num_metrics = len(metrics)\n",
        "    num_rows = (num_metrics + 2) // 3  # Using 3 columns, calculate rows needed (ceiling division)\n",
        "    fig, axes = plt.subplots(num_rows, 3, figsize=(25, 20 * num_rows / 2), dpi=600)  # Adjusted figsize proportionally\n",
        "    fig.suptitle('Model Comparison Results by Metric for judge model '+suffix, fontsize=16, y=1.05)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Extract data for this metric\n",
        "        means = []\n",
        "        cis = []\n",
        "        labels = []\n",
        "        \n",
        "        for pair in model_pairs:\n",
        "            metric_data = comparison_results[pair][metric]\n",
        "            means.append(metric_data['mean_difference'])\n",
        "            # ci_margin = metric_data['ci_margin']\n",
        "            cis.append([metric_data['ci_low'], \n",
        "                       metric_data['ci_high']])\n",
        "            labels.append(pair.split('with')[0].strip()) #Append only the model name comparisons\n",
        "\n",
        "        # Create bar plot\n",
        "        bars = ax.bar(range(len(means)), means)\n",
        "        \n",
        "        # Add error bars for confidence intervals\n",
        "        ax.errorbar(range(len(means)), means, \n",
        "                   yerr=[[m - ci[0] for m, ci in zip(means, cis)],\n",
        "                         [ci[1] - m for m, ci in zip(means, cis)]],\n",
        "                   fmt='none', color='black', capsize=5)\n",
        "        \n",
        "        # Add horizontal line at y=0\n",
        "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "        \n",
        "        # Customize plot\n",
        "        ax.set_title(f'{metric.capitalize()}')\n",
        "        ax.set_xticks(range(len(means)))\n",
        "        ax.set_xticklabels(labels, rotation=90) # Changed to vertical labels\n",
        "        ax.set_ylabel('Mean Difference')\n",
        "        \n",
        "        # Color bars based on statistical significance\n",
        "        for j, bar in enumerate(bars):\n",
        "            if comparison_results[model_pairs[j]][metric]['p_value'] < 0.05:\n",
        "                bar.set_color('darkred')\n",
        "            else:\n",
        "                bar.set_color('lightgray')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(num_metrics, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot before showing with high resolution\n",
        "    plt.savefig(f'model_comparisons_{suffix}.png', bbox_inches='tight', dpi=600)  # Increased DPI for higher resolution\n",
        "\n",
        "    # Show plot after saving\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_and_save_model_comparisons(comparison_results, list_of_metrics, \"_\".join(judge_model.split('/')[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_and_save_model_comparisons(comparison_results_second_judge, list_of_metrics, \"_\".join(judge_model_2.split('/')[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_table(comparison_results, metrics):\n",
        "    \"\"\"\n",
        "    Creates a formatted table from comparison results.\n",
        "    \n",
        "    Args:\n",
        "        comparison_results (dict): The comparison results dictionary\n",
        "        metrics (list): List of metrics to include\n",
        "        \n",
        "    Returns:\n",
        "        str: Formatted markdown table\n",
        "    \"\"\"\n",
        "    # Table header\n",
        "    table = \"| Metric | Model | Baseline | Model - Baseline | 95% Conf. Interval | Correlation |\\n\"\n",
        "    table += \"|--------|--------|-----------|-----------------|-------------------|-------------|\\n\"\n",
        "    \n",
        "    # Add rows for each comparison and metric\n",
        "    for pair in comparison_results:\n",
        "        model1, model2 = pair.split('_vs_')\n",
        "        for metric in metrics:\n",
        "            results = comparison_results[pair][metric]\n",
        "            \n",
        "            row = f\"| {metric} | {model1} | {model2} | \"\n",
        "            row += f\"{results['mean_difference']:.1%} | \"\n",
        "            row += f\"({results['ci_low']:.1%}, {results['ci_high']:.1%}) | \"\n",
        "            row += f\"{results['pearson_correlation']:.2f} |\\n\"\n",
        "            \n",
        "            table += row\n",
        "            \n",
        "    return table\n",
        "\n",
        "# Create and print the table\n",
        "metrics = [m.replace('_descr', '') for m in list_of_metrics]\n",
        "comparison_table = create_comparison_table(comparison_results, metrics)\n",
        "print(comparison_table)\n",
        "\n",
        "# Save table to file\n",
        "with open('comparison_table_'+'_'.join(judge_model.split('/')[1:])+'.txt', 'w') as f:\n",
        "    f.write(comparison_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_table_second_judge = create_comparison_table(comparison_results_second_judge, metrics)\n",
        "print(comparison_table_second_judge)\n",
        "\n",
        "# Save table to file\n",
        "with open('comparison_table_'+'_'.join(judge_model_2.split('/')[1:])+'.txt', 'w') as f:\n",
        "    f.write(comparison_table_second_judge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Power Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_power_analysis(effect_size=0.5, alpha=0.05, power=0.8):\n",
        "    \"\"\"\n",
        "    Perform power analysis to determine required sample size.\n",
        "    \n",
        "    Args:\n",
        "        effect_size (float): Expected effect size (Cohen's d)\n",
        "        alpha (float): Significance level\n",
        "        power (float): Desired statistical power\n",
        "        \n",
        "    Returns:\n",
        "        int: Required sample size per group\n",
        "    \"\"\"\n",
        "    analysis = TTestIndPower()\n",
        "    sample_size = analysis.solve_power(\n",
        "        effect_size=effect_size,\n",
        "        alpha=alpha,\n",
        "        power=power,\n",
        "        alternative='two-sided'\n",
        "    )\n",
        "    return int(np.ceil(sample_size))\n",
        "\n",
        "# First, determine required sample size\n",
        "required_samples = perform_power_analysis(effect_size=0.1254, alpha=0.05, power=0.8)  #These parameters result in a sample size of 1000\n",
        "print(f\"Required samples per model for statistical power: {required_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For real-time inference (below implementation only for meta-llama/Meta-Llama-3.1-8B-Instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# # del pipeline #Otherwise too much memory is used\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto')\n",
        "\n",
        "# #Example of real-time response generation\n",
        "# messages=[{\"role\": \"user\", \"content\": \"What is the chemical formula of water?\"}]\n",
        "\n",
        "# inputs_tokenized = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "#     return_dict=True,\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# input_ids = inputs_tokenized['input_ids']\n",
        "\n",
        "# # Generate tokens one by one\n",
        "# max_length = 256\n",
        "# output_ids = input_ids\n",
        "# for _ in range(256):\n",
        "#     outputs = model.generate(\n",
        "#         output_ids,\n",
        "#         max_new_tokens=1,\n",
        "#         do_sample=True,\n",
        "#         top_k=50,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "#     new_token_id = outputs[0, -1].item()\n",
        "#     if new_token_id == tokenizer.eos_token_id:\n",
        "#         break\n",
        "#     output_ids = torch.cat([output_ids, outputs[:, -1:]], dim=1)\n",
        "#     new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
        "#     print(new_token, end=\"\", flush=True)\n",
        "\n",
        "# print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other evaluators from Langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations\n",
        "# https://docs.smith.langchain.com/old/evaluation/quickstart\n",
        "\n",
        "# from langsmith.evaluation import LangChainStringEvaluator\n",
        "\n",
        "# eval_llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0.0, seed=42)\n",
        "\n",
        "# #Evaluators\n",
        "# qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm}) #LLM just gives 'correct' or 'incorrect' based on reference answer\n",
        "# context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm}) #Also uses reference context of example outputs to do the above\n",
        "# cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm}) #Same as above but with chain of thought 'reasoning'\n",
        "\n",
        "#Prompts Used internally:\n",
        "\n",
        "# 1) context_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. \n",
        "# It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 2) cot_qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, \n",
        "# based on the context.\n",
        "# Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# CONTEXT: context the question is about here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# EXPLANATION: step by step reasoning here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
        "\n",
        "\n",
        "# 3) qa_evaluator: You are a teacher grading a quiz.\n",
        "# You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
        "\n",
        "# Example Format:\n",
        "# QUESTION: question here\n",
        "# STUDENT ANSWER: student's answer here\n",
        "# TRUE ANSWER: true answer here\n",
        "# GRADE: CORRECT or INCORRECT here\n",
        "\n",
        "# Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer.\n",
        "#  It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, use custom prompts as shown below (and set {\"prompt\": PROMPT} as additional argument inside the config above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "# _PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in chemical engineering answers to questions.\n",
        "# You are grading the following question:\n",
        "# {query}\n",
        "# Here is the real answer:\n",
        "# {answer}\n",
        "# You are grading the following predicted answer:\n",
        "# {result}\n",
        "# Respond with CORRECT or INCORRECT:\n",
        "# \"\"\"\n",
        "\n",
        "# PROMPT = PromptTemplate(\n",
        "#     input_variables=[\"query\", \"result\", \"answer\"], template=_PROMPT_TEMPLATE\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes: Non-reproducible results, even when seed set (https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed), temperature=0 (top_p should not change when we changed temperature - smaller values result in more constrained and focused response - https://medium.com/@rasithbm/chatopenai-parameters-83bef49f6384)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b9c5f781e34806b9756d9e3e553a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cacd8a8bd03b4d0d83d266fe85e8ee65",
              "IPY_MODEL_abf8eb8102384433a59628820355d272",
              "IPY_MODEL_be7aa2993d57460c9d4f23c090e42c36"
            ],
            "layout": "IPY_MODEL_75aa3a3eb1b8420f9f26d404505e46cc"
          }
        },
        "0adc7382479c412f9c9230a17b56ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c13fc64f2e143b29105ec10e444b779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ace1e934716404a9340bce56cb1a3cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e3db7de6fcc04e6ba738f7ae78cef24d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1bd3eb0157a3477f907dae0c8fdbbec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24713d9c124b41488af127cfd3d1321e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331036e81d104ab49c44bcbde2d873f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec4c77240854f3ebab46e0b7d307f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ace1e934716404a9340bce56cb1a3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9db2c468cf4dc598a80da6a548d034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9d371e98fc45329cf381bc36a290ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331036e81d104ab49c44bcbde2d873f7",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd3eb0157a3477f907dae0c8fdbbec4",
            "value": ""
          }
        },
        "6e84ac6346d8450d9814b9f1a647164c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75aa3a3eb1b8420f9f26d404505e46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87311a42fde0441fb2e88a0655a95f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c13fc64f2e143b29105ec10e444b779",
              "IPY_MODEL_a2ac8fac33444da794be1f25a9c0d702",
              "IPY_MODEL_c0e527b08dd942a684246e2db22ed22d"
            ],
            "layout": "IPY_MODEL_cf7de095d0514bbf937d0632c777a232"
          }
        },
        "9253c9636a6c4e20b215ff11c928be07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "981e94324ba64b548259b1f1d297a564": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24713d9c124b41488af127cfd3d1321e",
            "placeholder": "​",
            "style": "IPY_MODEL_0adc7382479c412f9c9230a17b56ea42",
            "value": " 9/? [00:06&lt;00:00,  6.70s/it]"
          }
        },
        "98971ab8fe5c411f9c8c4c77753a745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bc61359ce44954bd235566d9ada2d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f628c84495494694a62ca9c181ec63ba",
            "value": 1
          }
        },
        "9b527616d5a647d88a33b14ee2712211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9d371e98fc45329cf381bc36a290ae",
              "IPY_MODEL_98971ab8fe5c411f9c8c4c77753a745f",
              "IPY_MODEL_981e94324ba64b548259b1f1d297a564"
            ],
            "layout": "IPY_MODEL_c1ffb867972444579481d5408abcdba9"
          }
        },
        "a2ac8fac33444da794be1f25a9c0d702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec4c77240854f3ebab46e0b7d307f74",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcfaa70f8c14dfcb0528b0cc0573db3",
            "value": 2
          }
        },
        "abf8eb8102384433a59628820355d272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6d3c439b254aa793c5d39304170849",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fac87e8a3a044de994d726896d479de3",
            "value": 1
          }
        },
        "b68644c249c04e059c924e1165a01370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7aa2993d57460c9d4f23c090e42c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6defd5c41f94c0ebda4080bb09c19c3",
            "placeholder": "​",
            "style": "IPY_MODEL_6e84ac6346d8450d9814b9f1a647164c",
            "value": " 9/? [01:10&lt;00:00,  5.27s/it]"
          }
        },
        "c0e527b08dd942a684246e2db22ed22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed1e9eb6e6bc4452b3d12aecffb6cc2a",
            "placeholder": "​",
            "style": "IPY_MODEL_9253c9636a6c4e20b215ff11c928be07",
            "value": " 2/2 [00:18&lt;00:00,  8.66s/it]"
          }
        },
        "c1ffb867972444579481d5408abcdba9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6d3c439b254aa793c5d39304170849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cacd8a8bd03b4d0d83d266fe85e8ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68644c249c04e059c924e1165a01370",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9db2c468cf4dc598a80da6a548d034",
            "value": ""
          }
        },
        "cf7de095d0514bbf937d0632c777a232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3db7de6fcc04e6ba738f7ae78cef24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8bc61359ce44954bd235566d9ada2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ed1e9eb6e6bc4452b3d12aecffb6cc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f628c84495494694a62ca9c181ec63ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6defd5c41f94c0ebda4080bb09c19c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac87e8a3a044de994d726896d479de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdcfaa70f8c14dfcb0528b0cc0573db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
