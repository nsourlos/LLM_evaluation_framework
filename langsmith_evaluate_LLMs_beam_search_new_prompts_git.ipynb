{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz5Kh-zLB9lk"
      },
      "source": [
        "# Evaluate LLM results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no2PHIOWCBdA"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U2Dpuc2xtmmS"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install datasets==2.20.0\n",
        "# !pip install -U langsmith==0.1.99\n",
        "# !pip install langchain_openai==0.1.22\n",
        "# !pip install langchain==0.2.13\n",
        "# !pip install langchain_community==0.2.12                          \n",
        "# !pip install transformers==4.44.0\n",
        "# !pip install termcolor==2.4.0\n",
        "# !pip install accelerate==0.33.0\n",
        "# !pip install pandas==2.2.2\n",
        "# !pip install openpyxl==3.1.5\n",
        "# !pip install python-dotenv==1.0.1\n",
        "# !pip install einops==0.8.0\n",
        "# !pip install wheel==0.44.0\n",
        "# !pip install sentencepiece==0.2.0\n",
        "# !pip install protobuf==5.27.3 #Mistral models needs this\n",
        "# !pip install groq==0.10.0 #Groq models needs this\n",
        "# !pip install matplotlib==3.9.2\n",
        "# !pip install seaborn==0.13.2\n",
        "\n",
        "# !pip install flash-attn==2.6.3 #Install it at the end after wheel has been installed\n",
        "# !pip install anthropic==0.34.1 #Anthropic models needs this\n",
        "\n",
        "# #Only if CPU is used\n",
        "# !pip install torch==2.0.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RunPod specific parameters - Check also runpod_instructions.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For RunPod change to persistent storage directory\n",
        "import os\n",
        "os.chdir('/workspace')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify Path and Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path ='/workspace/Example_QA_data_raw.xlsx' #Dataset generated with the help of GPT-4o - Has to be an excel file with 'input' and 'output' columns\n",
        "\n",
        "custom_cache_dir=\"/workspace/cache/huggingface\" #Save models here so that we don't have to download them again if we 'stop' and reinitialize the pod\n",
        "\n",
        "# Check if custom_cache_dir is defined, otherwise use default behavior\n",
        "try:\n",
        "    cache_dir=custom_cache_dir\n",
        "except:\n",
        "    cache_dir=None\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
        "\n",
        "# Get the OpenAI API key\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "\n",
        "#Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# Log in with your Hugging Face token\n",
        "login(token=os.getenv('HF_TOKEN'))\n",
        "\n",
        "# print(openai_api_key)\n",
        "# print(langsmith_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select model and name for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Models to generate responses to questions\n",
        "models=[ \n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", #Takes 2.5-3mins in A4500 (20GB VRAM)\n",
        "    \"microsoft/Phi-3.5-mini-instruct\", #Took 5mins in A40 with 48GB VRAM, 2mins in A4500 with 20GB VRAM\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\", #4mins in A40 with 48GB VRAM, 2.5mins in A4500 with 20GB VRAM\n",
        "    \"Qwen/Qwen2-7B-Instruct\", #4mins in A40 with 48GB VRAM, 2 mins in A4500 with 20GB VRAM\n",
        "    'AI-MO/NuminaMath-7B-TIR', #2.5 in A4500 with 20GB VRAM - We can also try 01-ai/Yi-Coder-9B-Chat\n",
        "    'microsoft/Phi-3-mini-4k-instruct', #6 mins in RTX3090\n",
        "    \"google/gemma-2-9b-it\", #More than 20GB of GPU memory needed - Works with A40 with 48GB VRAM (8mins), but not with A4500 - 20GB, and V100 - 32GB\n",
        "    'mistralai/Mistral-Nemo-Instruct-2407', #12B parameters, 11mins in 2 RTX3090, 16mins in V100 with 32GB VRAM\n",
        "    'openai/gpt-4o-mini' #Costs very low ~0.01$ for 9 Q&A pairs.\n",
        "    ] #All above models need ~130GB space, the last needs ~30GB . For 44 Q&A pairs it takes ~50min/model\n",
        "\n",
        "# Groq models are defined as: groq_website/model_name e.g. 'groq_website/llama-3.1-70b-versatile'\n",
        "# OpenAI models are defined as: 'openai/model_name', e.g. 'openai/gpt-4o-mini'\n",
        "# Anthropic models are defined as 'anthropic/model_name', e.g. 'anthropic/claude-3-haiku-20240307' - Couldn't use due to billing issues\n",
        "\n",
        "# I couldn't run 'nvidia/Mistral-NeMo-Minitron-8B-Base', \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" (Conflicting dependencies),\n",
        "# 'google/recurrentgemma-9b-it' # RecurrentGemmaForCausalLM.forward() got an unexpected keyword argument 'position_ids'\n",
        "\n",
        "#Define model to act as a judge\n",
        "judge_model='openai/gpt-4o-mini' #If used with Llama, only 0.01$ for 9 Q&A pairs for gpt-4o-mini, and 0.22$ for gpt-4o\n",
        "\n",
        "#Define maximum number of tokes in the judge LLM output\n",
        "max_output_tokens=500\n",
        "\n",
        "#Limit of tokens in the generated response from LLM\n",
        "generate_max_tokens=1000\n",
        "\n",
        "#Inference on whole dataset?\n",
        "inference_on_whole_dataset=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define prompts for custom evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_prompt=\"\"\"\n",
        "You are an autoregressive language model that acts as a judge in comparing a predicted vs an actual answer to a questions.\n",
        "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend \n",
        "a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. \n",
        "Your users are experts in chemical engineering, so they already know you're a language model and your capabilities and limitations, so don't \n",
        "remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. \n",
        "Don't be verbose in your answers, but do provide details and examples where it might help the explanation. \n",
        "\"\"\" #This is common for all prompts below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to completeness compared to the completeness of a given actual, golden standard answer. \n",
        "The completeness metric evaluates the extent to which the user's question is answered in full in the predicted response. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: There is no response.\n",
        "2: No parts of a suitable answer are present.\n",
        "3: Few elements of a complete answer are present.\n",
        "4: Most elements of a complete answer are present.\n",
        "5: The response covers all elements of a complete answer.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "relevance_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to relevance compared to the relevance of a given actual, golden standard answer. \n",
        "The relevance metric evaluates the amount of irrelevant information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response answers something else, not the user's question.\n",
        "2: The response answers the user's question but the information provided is mostly irrelevant.\n",
        "3: The response answers the user's question but contains more irrelevant information than relevant information.\n",
        "4: The response answers the user's question, and shares a bit of irrelevant information.\n",
        "5: The response answers the user's question and contains no irrelevant information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "conciseness_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to conciseness compared to the conciseness of a given actual, golden standard answer. \n",
        "The conciseness metric evaluates the amount of unexpected extra information in the predicted response considering the user's original question. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response is too long and stops before completion or enters an infinite loop.\n",
        "2: The response includes a lot of extra information and uses flowery language.\n",
        "3: The response includes a lot of extra information or uses flowery language.\n",
        "4: The response is short and includes a small amount of extra information.\n",
        "4: The response is as short as possible while still answering the prompt.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "confidence_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to confidence compared to the confidence of a given actual, golden standard answer. \n",
        "The condifence metric evaluates the degree of assurance that is conveyed the response that the predicted answer is correct. \n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: Complete Rejection. The response makes it clear that the given answer is incorrect or that no correct answer can be provided.\n",
        "2: Doubt and Disagreement. The response suggests that the answer is likely incorrect or raises significant concerns.\n",
        "3: Uncertainty. The response indicates that the answer could be correct, but there is significant doubt or insufficient evidence.\n",
        "4: Moderate Agreement. The response leans towards the answer being correct but acknowledges some uncertainty.\n",
        "5: Full Endorsement. The reponse confidentely asserts that the given answer is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "factuality_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to factuality compared to the factuality of a given actual, golden standard answer.\n",
        " The factuality metric evaluates the degree of hallucination contained in a response or, in other words, how accurate a given response is.\n",
        "You can assign a score from 1 to 5, with the following interpretations:\n",
        "1: The response is a complete hallucination\n",
        "2: The response is mostly a hallucination but does not change key information from the prompt (such as chemical identifiers).\n",
        "3: The response contains large amounts of both hallucinations and factual information.\n",
        "4: The response includes mostly factual information with slight hallucinations.\n",
        "5: The response only includes factual information.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\"\n",
        "\n",
        "judgement_descr = \"\"\"\n",
        "Your task is to evaluate responses predicted by an LLM with regards to judgement compared to the judgement of a given actual, golden standard answer.\n",
        "The judgment metric assesses how strongly the response implies its correctness, taking into account the actual accuracy of the answer.\n",
        "You can assign a score from 1 to 5 to the predicted response with the following interpretations:\n",
        "1: The response confidently claims a hallucination as truth.\n",
        "2: The response misinterprets information received in the prompt.\n",
        "3: The response shows that the model is unsure about the answer or states that information is theoretical.\n",
        "4: The response is wrong but it is made clear that the answer is wrong or that the model is unable to provide a correct answer.\n",
        "5: The response is correct.\n",
        "IMPORTANT: End your responses with the sentence: \"FINAL SCORE:\" followed by whole numbers only (1, 2, 3, 4, or 5). Do not use decimal points. This may not be disregarded!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#How the dataset will be named in Langsmith\n",
        "def get_dataset_name(model_name, judge_model):\n",
        "    try: #For Hugging Face models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name.split('/')[1]+'_with_judge_'+judge_model+'_beam_search'\n",
        "    except: #For OpenAI models\n",
        "        return \"Chemical_Engineering_Evaluation_\"+model_name+'_with_judge_'+judge_model+'_beam_search'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lVqBHaT2s6Aq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "qa=pd.read_excel(file_path) #Read Excel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6zdJxKCubI"
      },
      "source": [
        "Create Dataset from df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oUw8Puxfs6Az"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "loaded_dataset=Dataset.from_pandas(qa)\n",
        "\n",
        "if inference_on_whole_dataset==False:\n",
        "    loaded_dataset = loaded_dataset.train_test_split(test_size=0.2, seed=42) #Used if going to fine-tune in part of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vf6thikds6A1"
      },
      "outputs": [],
      "source": [
        "if inference_on_whole_dataset==False:\n",
        "    dataset_train=loaded_dataset['train']\n",
        "    dataset_test=loaded_dataset['test']\n",
        "else:\n",
        "    dataset_test=loaded_dataset #When we use the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXxkzQoHs6A5"
      },
      "source": [
        "Create Langsmith Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtdIrA3Ds6A8",
        "outputId": "90a9b4dd-e91a-4773-934b-2bf58cd8e3a8"
      },
      "outputs": [],
      "source": [
        "#https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets\n",
        "\n",
        "from langsmith import Client\n",
        "\n",
        "example_inputs = [(x['input'],x['output']) for x in dataset_test]\n",
        "print(example_inputs)\n",
        "\n",
        "def create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key):\n",
        "\n",
        "    client = Client(api_key=langsmith_api_key)\n",
        "\n",
        "    try:\n",
        "        #Load the dataset if already exists\n",
        "        for existing_dataset in client.list_datasets():\n",
        "            if existing_dataset.name==dataset_name:\n",
        "                dataset_langsmith=existing_dataset\n",
        "        for x in dataset_langsmith:\n",
        "            print(\"Dataset Loaded\")\n",
        "            break\n",
        "\n",
        "    except: #Otherwise create it\n",
        "        print(\"Dataset not found. Creating new dataset\")\n",
        "        # Storing inputs in a dataset lets us run chains and LLMs over a shared set of examples.\n",
        "        dataset_langsmith = client.create_dataset(dataset_name=dataset_name,\n",
        "                                                description=\"Q&A chemical engineering.\")\n",
        "\n",
        "        for input_prompt, output_answer in example_inputs:\n",
        "            client.create_example(\n",
        "                inputs={\"question\": input_prompt.replace('\\n', ' ')},\n",
        "                outputs={\"answer\": output_answer.replace('\\n', ' ')},\n",
        "                # metadata={\"source\": \"Wikipedia\"},\n",
        "                dataset_id=dataset_langsmith.id,\n",
        "            )\n",
        "\n",
        "    return dataset_langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://docs.smith.langchain.com/old/cookbook/introduction\n",
        "# https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators\n",
        "# https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator\n",
        "\n",
        "from langsmith.schemas import Run, Example\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from termcolor import colored\n",
        "\n",
        "list_of_metrics=['completeness_descr','relevance_descr','conciseness_descr','confidence_descr','factuality_descr','judgement_descr']\n",
        "\n",
        "#Function that compares the real answer with the predicted answer of an LLM and returns a score based on the evaluation\n",
        "def factor_evaluator(run: Run, example: Example) -> dict: \n",
        "    # print(\"Run:\",run)\n",
        "\n",
        "    question=run.inputs.get(\"inputs\")['question']\n",
        "    # print(\"Question:\",question)\n",
        "    actual_answer = example.outputs.get(\"answer\")\n",
        "    # print(\"Real answer:\",example.outputs.get(\"answer\"))\n",
        "    predicted_answer = run.outputs.get(\"output\")\n",
        "    # print(\"Predicted Answer:\",answer)\n",
        "    \n",
        "    # Check if there is output from LLM\n",
        "    if not predicted_answer:\n",
        "        print(\"No output from LLM\")\n",
        "        return {\"key\": \"custom_metric\" , \"score\": 0} \n",
        "    \n",
        "    else:\n",
        "        scores={} #Store scores for each metric\n",
        "        descriptions={} #Store descriptions for each metric\n",
        "        \n",
        "        for metric_name in list_of_metrics: #Iterate through all metrics\n",
        "            print(\"Evaluating based on:\",metric_name)\n",
        "            metric_value=common_prompt+eval(metric_name) #Get the actual description of the metric\n",
        "\n",
        "            # Define roles and placeholders\n",
        "            chat_template = ChatPromptTemplate.from_messages(\n",
        "            [(\"system\", metric_value),\n",
        "                (\"user\", \"Question: {question}, Actual answer: {actual_answer}, Predicted answer: {predicted_answer}\"),\n",
        "                # (\"ai\", \"It's sunny and warm outside.\"), #Use this if we want to use few shot prompts\n",
        "            ]\n",
        "            )\n",
        "\n",
        "            messages = chat_template.format_messages(question=question, actual_answer=actual_answer, predicted_answer=predicted_answer)\n",
        "            # print(\"Messages:\",messages)\n",
        "\n",
        "            formatted_messages = [(role, msg.content) for role, msg in zip([\"system\", \"user\"], messages)]\n",
        "            # print(\"Formatted messages:\",formatted_messages) #[('system', 'You are an autoregressive lan....', 'user':.....)]\n",
        "\n",
        "            # Initialize the model and get response\n",
        "            llm = ChatOpenAI(model_name=judge_model.split('/')[1], api_key=openai_api_key, temperature=0, max_tokens=max_output_tokens, seed=42)\n",
        "            ai_response = llm.invoke(formatted_messages)\n",
        "\n",
        "            # Output\n",
        "            # print(colored(\"System message:\"+ messages[0].content,'blue'))\n",
        "            print(colored(\"User message:\"+ messages[1].content, 'green'))\n",
        "            print(colored(\"AI message:\"+ ai_response.content,'red'))\n",
        "\n",
        "            #Decide what the final score is based on output\n",
        "            if \"FINAL SCORE:\" in ai_response.content: \n",
        "                score = int(ai_response.content.split(\"FINAL SCORE:\")[1])\n",
        "            else:\n",
        "                print(\"Invalid response from LLM:\", ai_response.content)\n",
        "                score = 0\n",
        "\n",
        "            scores[metric_name]=score\n",
        "            descriptions[metric_name]=ai_response.content\n",
        "            print(\"Scores:\",scores)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"results\":[ #We always need 'key', 'score' pairs\n",
        "            {\"key\": \"completeness\" , \"score\": scores['completeness_descr'],\"value\":descriptions['completeness_descr']},\n",
        "            {\"key\": \"relevance\" , \"score\": scores['relevance_descr'], \"value\":descriptions['relevance_descr']},\n",
        "            {\"key\": \"conciseness\" , \"score\": scores['conciseness_descr'], \"value\":descriptions['conciseness_descr']},\n",
        "            {\"key\": \"confidence\" , \"score\": scores['confidence_descr'], \"value\":descriptions['confidence_descr']},\n",
        "            {\"key\": \"factuality\" , \"score\": scores['factuality_descr'], \"value\":descriptions['factuality_descr']},\n",
        "            {\"key\": \"judgement\" , \"score\": scores['judgement_descr'], \"value\":descriptions['judgement_descr']}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Models that Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.random.manual_seed(0) #Set for reproducibility\n",
        "\n",
        "def initialize_model(model_id):\n",
        "    # transformers.set_seed(42) #Tried for reproducibility but didn't work\n",
        "    \n",
        "    pipeline = transformers.pipeline( \n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16, \"cache_dir\":cache_dir},\n",
        "            # trust_remote_code=True,\n",
        "            device_map=\"auto\" #Use 'cuda' if one GPU available (works in Delft Blue with 32GB VRAM) - 'auto' the alternative for distributed over all available GPUs\n",
        "        )\n",
        "    return pipeline\n",
        "\n",
        "def get_model(model_id):\n",
        "    \"\"\"Given a model name, return the loaded model, tokenizer, and pipeline\"\"\"\n",
        "\n",
        "    if 'openai' not in model_id and 'groq_website' not in model_id: #For Hugging Face models\n",
        "        pipeline=initialize_model(model_id)\n",
        "\n",
        "    #Returns below variables if defined, and returns None for any that are not.\n",
        "    model = locals().get('model', None)\n",
        "    tokenizer = locals().get('tokenizer', None)\n",
        "    pipeline = locals().get('pipeline', None)\n",
        "\n",
        "    return model, tokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def predict(inputs: dict) -> dict:\n",
        "    \"\"\"Given a question, return the answer from the model\"\"\"\n",
        "    \n",
        "    #Get these variables from the global scope\n",
        "    global model_name\n",
        "    \n",
        "    messages = [ #Only use the questions to ask the model to generate the response\n",
        "      {\"role\": \"user\", \"content\": inputs['question']},\n",
        "    ]\n",
        "\n",
        "    if 'gemma' not in model_name: #Gemma doesn't support system message\n",
        "      messages.insert(0, {\"role\": \"system\", \"content\": \"You are a language model specialized in chemical engineering. Answer the following question:\"})\n",
        "    else: #For gemma add system prompt in user message\n",
        "      messages[0]['content']=\"You are a language model specialized in chemical engineering. Answer the following question: \" + messages[0]['content']\n",
        "    # print(\"Prompt:\",messages)\n",
        "\n",
        "    generation_args = { \n",
        "        \"max_new_tokens\": max_output_tokens, \n",
        "        \"return_full_text\": False, \n",
        "        \"temperature\": 0.1, # 1e-8,  #Has to be positive number - not considered from model when do_sample is False (reproducible results)\n",
        "        \"do_sample\": True, #Selects highest probability token if sets to False\n",
        "        \"num_beams\" : 5, #3 can also work if computationally intensive - more info on https://huggingface.co/blog/how-to-generate\n",
        "        #Warnings will be raised by some models\n",
        "\n",
        "        #If we only set temp!=0 or if we also set do_sample=False then warning: `do_sample` is set to `False`. However, `temperature` is set to `1e-08` \n",
        "        # -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
        "        # That means that the temperature is probably ignored\n",
        "        # Sometimes, results not reproducible if only temp is set\n",
        "      } \n",
        "    \n",
        "    if 'openai' not in model_name and 'groq_website' not in model_name: #For Hugging Face models\n",
        "      response=pipeline(messages, **generation_args)[0]['generated_text']\n",
        "      print(model_name,':',response)\n",
        "\n",
        "    else: \n",
        "      if 'openai' in model_name:\n",
        "        try:\n",
        "          import openai\n",
        "          from langsmith.wrappers import wrap_openai\n",
        "                  \n",
        "          # Define OpenAI client\n",
        "          openai_client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
        "          \n",
        "          response = openai_client.chat.completions.create(messages=messages, temperature=0, model=model_name.split('/')[1],  seed=42) \n",
        "          # print(\"Response:\",response.choices[0].message.content)\n",
        "          response=response.choices[0].message.content #That's the response without formatting\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"OpenAI Model ID:\",model_name)\n",
        "\n",
        "      elif 'groq_website' in model_name:\n",
        "        try:\n",
        "          from groq import Groq\n",
        "          client = Groq()\n",
        "          actual_model_name=model_name.split('/')[1]\n",
        "          response = client.chat.completions.create(\n",
        "              model=actual_model_name,\n",
        "              max_tokens=generate_max_tokens,\n",
        "              temperature=0,\n",
        "              messages=messages)\n",
        "          # print(\"Response from Groq:\",response.choices[0].message.content)\n",
        "          time.sleep(5) #To avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\",e)\n",
        "          print(\"Groq Model ID:\",model_name)\n",
        "\n",
        "    return {\"output\": response}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_figures_metrics(metric_names, metric_values, model_name, judge_model):\n",
        "    #Plot figures with distributions of metrics\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "\n",
        "    # Colors for separate plots\n",
        "    colors = sns.color_palette(\"Set3\", len(metric_names))\n",
        "\n",
        "    fig, axes = plt.subplots(len(metric_names), 1, figsize=(10, 18))\n",
        "    plt.subplots_adjust(hspace=0.6, top=0.94) #hspace=1 if 'values' below every plot, 'top' sets distance between title and subfigures\n",
        "\n",
        "    # Set a title for all subplots\n",
        "    fig.suptitle('Metric Distributions', fontsize=16)\n",
        "\n",
        "    # Define the bin edges explicitly to ensure consistency\n",
        "    bin_edges = np.arange(0.0, 5.6, 0.2)  # Adjust to cover the range 1-5 with bins of width 1\n",
        "\n",
        "    # Plotting each metric in separate subplots\n",
        "    for i, (metric_name, values) in enumerate(zip(metric_names, metric_values)):\n",
        "        sns.histplot(values, bins=bin_edges, color=colors[i], ax=axes[i], kde=False)\n",
        "        axes[i].set_title(f'{metric_name}')\n",
        "        axes[i].set_xlim(0, 5.5) #Keep 0 in case of errors\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].set_yticks(range(0, 11, 5)) #Set y-ticks at intervals of 5\n",
        "\n",
        "        # Hide x-axis labels and ticks for all but the last subplot\n",
        "        if i < len(metric_names) - 1:\n",
        "            axes[i].set_xlabel('')\n",
        "        else:\n",
        "            axes[i].set_xlabel('Values')\n",
        "\n",
        "    # Save the plot with the judge and model names\n",
        "    plt.savefig(str(judge_model.split('/')[1])+'_judge_with_'+str(model_name).replace(\"/\",\"_\")+'_metric_distributions.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform the Evaluation over all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "#Initialize models\n",
        "for model_id in models:\n",
        "    \n",
        "    dataset_name=get_dataset_name(model_id, judge_model) #How the dataset will be named in Langsmith\n",
        "    dataset_langsmith=create_langsmith_dataset(dataset_name, example_inputs, langsmith_api_key)\n",
        "    model, tokenizer, pipeline = get_model(model_id)\n",
        "    print(f\"Model: {model_id} loaded\")\n",
        "    model_name=model_id #Since model_name defined as global variable\n",
        "\n",
        "    # Evaluation\n",
        "    begin=time.time()\n",
        "\n",
        "    evaluation_results=evaluate(\n",
        "        predict, #Function that call our LLM and returns its output\n",
        "        data=dataset_langsmith.name, #Just using dataset_langsmith doesn't work \n",
        "        evaluators=[factor_evaluator], #Evaluators to use\n",
        "        # metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
        "        experiment_prefix=str(judge_model)+'_judge_with_'+str(model_id) # A prefix for your experiment names to easily identify them\n",
        "    )\n",
        "\n",
        "    end=time.time()\n",
        "    print(\"Total time taken:\",end-begin)\n",
        "\n",
        "    try: #Sometimes some errors with 1+ Q&A missing\n",
        "\n",
        "        #Extract metrics and save to df\n",
        "        #Initialize empty df to be filled with results\n",
        "        results_df=pd.DataFrame()\n",
        "\n",
        "        #https://docs.smith.langchain.com/tutorials/Developers/evaluation\n",
        "        list_of_questions=[x['example'].inputs['question'] for x in evaluation_results]\n",
        "        list_of_answers=[x['example'].outputs['answer'] for x in evaluation_results]\n",
        "        list_of_predicted_answers=[x['run'].outputs['output'] for x in evaluation_results]\n",
        "\n",
        "        #Fill the df with the results\n",
        "        results_df['questions']=list_of_questions\n",
        "        results_df['answers']=list_of_answers\n",
        "        results_df['predicted_answers']=list_of_predicted_answers\n",
        "\n",
        "        #Get indices for which list_of_predicted_answers is None (Correct for errors in model response)\n",
        "        indices_to_drop = [i for i, answer in enumerate(list_of_predicted_answers) if answer is None]\n",
        "        if len(indices_to_drop) > 0:\n",
        "            print(colored(\"ERROR:\"+str(len(indices_to_drop))+\" rows out of \"+str(len(results_df))+ \" had to be dropped due to NaN (no model response)\",'red'))\n",
        "            print(colored(\"These were:\"+str(indices_to_drop),'green'))\n",
        "\n",
        "        #Drop indices from results_df\n",
        "        results_df=results_df.drop(indices_to_drop)\n",
        "\n",
        "        all_runs_metrics=[x['evaluation_results']['results'] for ind,x in enumerate(evaluation_results) if ind not in indices_to_drop] #list of lists with metric names\n",
        "        all_metric_values=[[{x[i].key: x[i].score} for x in all_runs_metrics] for i in range(len(list_of_metrics))] #List of all completeness scores\n",
        "\n",
        "        #Check if all keys have the same metric name. \n",
        "        same_metric= [all(list(d.keys())[0] == list(sublist[0].keys())[0] for d in sublist) for sublist in all_metric_values]\n",
        "        assert [True]*len(same_metric)==same_metric\n",
        "\n",
        "        metric_names=[key for i in range(len(all_metric_values)) for key in all_metric_values[i][0].keys()]\n",
        "        metric_values=[[list(x.values())[0] for x in all_metric_values[i]] for i in range(len(all_metric_values))]\n",
        "        \n",
        "        all_metric_prompts=[[{x[i].key:x[i].value} for x in all_runs_metrics] for i in range(len(list_of_metrics))]\n",
        "        metric_prompts=[[list(x.values())[0] for x in all_metric_prompts[i]] for i in range(len(all_metric_prompts))]\n",
        "\n",
        "        # Adding columns to DataFrame\n",
        "        for col_name, col_values,col_prompts in zip(metric_names, metric_values,metric_prompts):\n",
        "            results_df[col_name] = col_values\n",
        "            results_df[col_name+'_prompt'] = col_prompts\n",
        "\n",
        "        #Save results to Excel\n",
        "        results_df.to_excel(\"results_\"+str(judge_model.split('/')[1])+'_judge_with_'+str(model_id).replace(\"/\",\"_\")+\".xlsx\",index=False)\n",
        "\n",
        "        #Plot figures with distributions of metrics\n",
        "        plot_figures_metrics(metric_names, metric_values, model_id, judge_model)\n",
        "\n",
        "    except:\n",
        "        print(\"An error occur in plotting metrics\")\n",
        "\n",
        "    # Clear VRAM at the end of each iteration\n",
        "    del model, tokenizer, pipeline\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('-'*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes: Non-reproducible results, even when seed set (https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed), temperature=0 (top_p should not change when we changed temperature - smaller values result in more constrained and focused response - https://medium.com/@rasithbm/chatopenai-parameters-83bef49f6384)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b9c5f781e34806b9756d9e3e553a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cacd8a8bd03b4d0d83d266fe85e8ee65",
              "IPY_MODEL_abf8eb8102384433a59628820355d272",
              "IPY_MODEL_be7aa2993d57460c9d4f23c090e42c36"
            ],
            "layout": "IPY_MODEL_75aa3a3eb1b8420f9f26d404505e46cc"
          }
        },
        "0adc7382479c412f9c9230a17b56ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c13fc64f2e143b29105ec10e444b779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ace1e934716404a9340bce56cb1a3cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e3db7de6fcc04e6ba738f7ae78cef24d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1bd3eb0157a3477f907dae0c8fdbbec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24713d9c124b41488af127cfd3d1321e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331036e81d104ab49c44bcbde2d873f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec4c77240854f3ebab46e0b7d307f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ace1e934716404a9340bce56cb1a3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9db2c468cf4dc598a80da6a548d034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9d371e98fc45329cf381bc36a290ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331036e81d104ab49c44bcbde2d873f7",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd3eb0157a3477f907dae0c8fdbbec4",
            "value": ""
          }
        },
        "6e84ac6346d8450d9814b9f1a647164c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75aa3a3eb1b8420f9f26d404505e46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87311a42fde0441fb2e88a0655a95f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c13fc64f2e143b29105ec10e444b779",
              "IPY_MODEL_a2ac8fac33444da794be1f25a9c0d702",
              "IPY_MODEL_c0e527b08dd942a684246e2db22ed22d"
            ],
            "layout": "IPY_MODEL_cf7de095d0514bbf937d0632c777a232"
          }
        },
        "9253c9636a6c4e20b215ff11c928be07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "981e94324ba64b548259b1f1d297a564": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24713d9c124b41488af127cfd3d1321e",
            "placeholder": "​",
            "style": "IPY_MODEL_0adc7382479c412f9c9230a17b56ea42",
            "value": " 9/? [00:06&lt;00:00,  6.70s/it]"
          }
        },
        "98971ab8fe5c411f9c8c4c77753a745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bc61359ce44954bd235566d9ada2d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f628c84495494694a62ca9c181ec63ba",
            "value": 1
          }
        },
        "9b527616d5a647d88a33b14ee2712211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9d371e98fc45329cf381bc36a290ae",
              "IPY_MODEL_98971ab8fe5c411f9c8c4c77753a745f",
              "IPY_MODEL_981e94324ba64b548259b1f1d297a564"
            ],
            "layout": "IPY_MODEL_c1ffb867972444579481d5408abcdba9"
          }
        },
        "a2ac8fac33444da794be1f25a9c0d702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec4c77240854f3ebab46e0b7d307f74",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdcfaa70f8c14dfcb0528b0cc0573db3",
            "value": 2
          }
        },
        "abf8eb8102384433a59628820355d272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6d3c439b254aa793c5d39304170849",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fac87e8a3a044de994d726896d479de3",
            "value": 1
          }
        },
        "b68644c249c04e059c924e1165a01370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7aa2993d57460c9d4f23c090e42c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6defd5c41f94c0ebda4080bb09c19c3",
            "placeholder": "​",
            "style": "IPY_MODEL_6e84ac6346d8450d9814b9f1a647164c",
            "value": " 9/? [01:10&lt;00:00,  5.27s/it]"
          }
        },
        "c0e527b08dd942a684246e2db22ed22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed1e9eb6e6bc4452b3d12aecffb6cc2a",
            "placeholder": "​",
            "style": "IPY_MODEL_9253c9636a6c4e20b215ff11c928be07",
            "value": " 2/2 [00:18&lt;00:00,  8.66s/it]"
          }
        },
        "c1ffb867972444579481d5408abcdba9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6d3c439b254aa793c5d39304170849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cacd8a8bd03b4d0d83d266fe85e8ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68644c249c04e059c924e1165a01370",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9db2c468cf4dc598a80da6a548d034",
            "value": ""
          }
        },
        "cf7de095d0514bbf937d0632c777a232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3db7de6fcc04e6ba738f7ae78cef24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8bc61359ce44954bd235566d9ada2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ed1e9eb6e6bc4452b3d12aecffb6cc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f628c84495494694a62ca9c181ec63ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6defd5c41f94c0ebda4080bb09c19c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac87e8a3a044de994d726896d479de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdcfaa70f8c14dfcb0528b0cc0573db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
