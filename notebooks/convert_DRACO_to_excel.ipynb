{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76af58c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soyrl\\Desktop\\json\\merged.json\n",
      "inside\n",
      "Excel file created at: C:\\Users\\soyrl\\Desktop\\DRACO_conference.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "def fix_json_content(content):\n",
    "    \"\"\"Fix common JSON formatting issues.\"\"\"\n",
    "    content = re.sub(r'([{,])\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*:', r'\\1\"\\2\":', content)\n",
    "    content = re.sub(r'}\\s*{', '},{', content)\n",
    "    content = content.replace(',]', ']')\n",
    "    content = content.replace(',}', '}')\n",
    "    content = content.replace('\\\\\"', '\"')\n",
    "    content = content.replace(',\"https\"://', ', https://').replace('\\',\"https\"://', ', https://')\n",
    "    content = content.replace('\"https\"://', 'https://')\n",
    "    return content\n",
    "\n",
    "def process_json_files(base_path):\n",
    "    data = []\n",
    "    if not os.path.exists(base_path) or not os.path.isdir(base_path):\n",
    "        print(f\"Directory {base_path} does not exist.\")\n",
    "        return data\n",
    "\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".json\") and \".\" in file:\n",
    "                print(f\"Skipping {file} because it is not a JSON file.\")\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            print(file_path)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    print('inside')\n",
    "                    json_data = json.load(f)                    \n",
    "\n",
    "                    #Activate below to combine multiple json files into one - Deals with errors in json files\n",
    "                    # try:\n",
    "                    #     # json_data = json.loads(fixed_content)\n",
    "                    #     json_data = json.load(f)\n",
    "                    # except json.JSONDecodeError as e:\n",
    "                    #     print(f\"First JSON parse attempt failed: {str(e)}\")\n",
    "                    #     # Show the problematic text around the error position\n",
    "                    #     if hasattr(e, 'pos') and e.pos is not None:\n",
    "                    #         start_pos = max(0, e.pos - 50)\n",
    "                    #         end_pos = min(len(fixed_content), e.pos + 50)\n",
    "                    #         problematic_text = fixed_content[start_pos:end_pos]\n",
    "                    #         print(f\"Problematic text around position {e.pos}: '{problematic_text}'\")\n",
    "                    #         if e.pos < len(fixed_content):\n",
    "                    #             print(f\"Character at error position: '{fixed_content[e.pos]}' (ASCII: {ord(fixed_content[e.pos])})\")\n",
    "                        \n",
    "                    #     try:\n",
    "                    #         # Remove control characters more aggressively\n",
    "                    #         cleaned_content = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', fixed_content)\n",
    "                    #         json_data = json.loads(cleaned_content)\n",
    "                    #     except json.JSONDecodeError as e2:\n",
    "                    #         print(f\"Second JSON parse attempt failed: {str(e2)}\")\n",
    "                    #         # Show the problematic text around the error position for second attempt\n",
    "                    #         if hasattr(e2, 'pos') and e2.pos is not None:\n",
    "                    #             start_pos = max(0, e2.pos - 50)\n",
    "                    #             end_pos = min(len(cleaned_content), e2.pos + 50)\n",
    "                    #             problematic_text = cleaned_content[start_pos:end_pos]\n",
    "                    #             print(f\"Problematic text around position {e2.pos}: '{problematic_text}'\")\n",
    "                    #             if e2.pos < len(cleaned_content):\n",
    "                    #                 print(f\"Character at error position: '{cleaned_content[e2.pos]}' (ASCII: {ord(cleaned_content[e2.pos])})\")\n",
    "                            \n",
    "                    #         try:\n",
    "                    #             # Try to fix common JSON issues\n",
    "                    #             corrected_string = re.sub(r'{([^{}]*)}', r'\"\\1\"', cleaned_content)\n",
    "                    #             json_data = json.loads(corrected_string)\n",
    "                    #         except json.JSONDecodeError as e3:\n",
    "                    #             print(f\"All JSON parse attempts failed for {file_path}\")\n",
    "                    #             print(f\"Final error: {str(e3)}\")\n",
    "                    #             # Show the problematic text around the error position for final attempt\n",
    "                    #             if hasattr(e3, 'pos') and e3.pos is not None:\n",
    "                    #                 start_pos = max(0, e3.pos - 50)\n",
    "                    #                 end_pos = min(len(corrected_string), e3.pos + 50)\n",
    "                    #                 problematic_text = corrected_string[start_pos:end_pos]\n",
    "                    #                 print(f\"Final problematic text around position {e3.pos}: '{problematic_text}'\")\n",
    "                    #                 if e3.pos < len(corrected_string):\n",
    "                    #                     print(f\"Final character at error position: '{corrected_string[e3.pos]}' (ASCII: {ord(corrected_string[e3.pos])})\")\n",
    "                    #             continue  # Skip this file and move to the next one\n",
    "                \n",
    "                # if not isinstance(json_data, list):\n",
    "                #     json_data = [json_data]\n",
    "                \n",
    "                for entry in json_data:\n",
    "                    if not isinstance(entry, dict):\n",
    "                        continue\n",
    "                    # if entry.get(\"answer\", \"\"):\n",
    "                    #     answer_text = entry.get(\"answer\", \"\")\n",
    "                    # else:\n",
    "                    #     answer_text = entry.get(\"response\", \"\")\n",
    "                    data.append([\n",
    "                        # root.split(\"peer-review\")[1]+'/'+file,\n",
    "                        entry.get(\"id\", \"\"),\n",
    "                        entry.get(\"question\", \"\"),\n",
    "                        # answer_text,\n",
    "                        entry.get(\"answer\", \"\"),\n",
    "                        # f\"{entry.get('topic/subtopic', '')}\",\n",
    "                        # entry.get(\"reference\", \"\"),\n",
    "                        entry.get(\"origin_file\", \"\")\n",
    "                    ])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                if \"Expecting property name\" in str(e) or \"Expecting ',' delimiter\" in str(e):\n",
    "                    print(\"This appears to be a JSON formatting error. Please check the file for proper JSON syntax.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_to_excel(data, output_path):\n",
    "    if not data:\n",
    "        print(\"No JSON data found to write to Excel.\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=[\"id\", \"question\", \"answer\", \"origin_file\"])\n",
    "        df.to_excel(output_path, index=False)\n",
    "        print(f\"Excel file created at: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Excel file: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "    trial01_path = os.path.join(desktop_path, \"json\")\n",
    "    output_path = os.path.join(desktop_path, \"DRACO_conference.xlsx\")\n",
    "    \n",
    "    data = process_json_files(trial01_path)\n",
    "    save_to_excel(data, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
